Handbook of Knowledge Representation
Edited by
Frank van Harmelen Vrije Universiteit Amsterdam
The Netherlands Vladimir Lifschitz University of Texas at Austin
USA Bruce Porter University of Texas at Austin
USA
AMSTERDAM–BOSTON–HEIDELBERG–LONDON–NEW YORK–OXFORD PARIS–SAN DIEGO–SAN FRANCISCO–SINGAPORE–SYDNEY–TOKYO

Elsevier Radarweg 29, PO Box 211, 1000 AE Amsterdam, The Netherlands The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, UK
First edition 2008 Copyright © 2008 Elsevier B.V. All rights reserved No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means electronic, mechanical, photocopying, recording or otherwise without the prior written permission of the publisher Permissions may be sought directly from Elsevier’s Science & Technology Rights Department in Oxford, UK: phone (+44) (0) 1865 843830; fax (+44) (0) 1865 853333; email: permissions@elsevier.com. Alternatively you can submit your request online by visiting the Elsevier web site at http://elsevier.com/locate/ permissions, and selecting Obtaining permission to use Elsevier material Notice No responsibility is assumed by the publisher for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or from any use or operation of any methods, products, instructions or ideas contained in the material herein. Because of rapid advances in the medical sciences, in particular, independent veriﬁcation of diagnoses and drug dosages should be made
Library of Congress Cataloging-in-Publication Data A catalog record for this book is available from the Library of Congress
British Library Cataloguing in Publication Data A catalogue record for this book is available from the British Library ISBN: 978-0-444-52211-5
For information on all Elsevier publications visit our website at books.elsevier.com
Printed and bound in United Kingdom 08 09 10 11 12 10 9 8 7 6 5 4 3 2 1

We dedicate this book to the memory of Ray Reiter (1939–2002)

This page intentionally left blank

Preface
Knowledge Representation and Reasoning is at the heart of the great challenge of Artiﬁcial Intelligence: to understand the nature of intelligence and cognition so well that computers can be made to exhibit human-like abilities. As early as 1958, John McCarthy contemplated Artiﬁcial Intelligence systems that could exercise common sense. From this and other early work, researchers gained the conviction that (artiﬁcial) intelligence could be formalized as symbolic reasoning with explicit representations of knowledge, and that the core research challenge is to ﬁgure out how to represent knowledge in computers and to use it algorithmically to solve problems.
Fifty years later, this book surveys the substantial body of scientiﬁc and engineering insights that constitute the ﬁeld of Knowledge Representation and Reasoning. Advances have been made on three fronts. First, researchers have explored general methods of knowledge representation and reasoning, addressing fundamental issues that cut across application domains. Second, researchers have developed specialized methods of knowledge representation and reasoning to handle core domains, such as time, space, causation and action. Third, researchers have tackled important applications of knowledge representation and reasoning, including query answering, planning and the Semantic Web. Accordingly, the book is divided into three sections to cover these themes.
Part I focuses on general methods for representing knowledge in Artiﬁcial Intelligence systems. It begins with background on classical logic and theorem proving, then turns to new approaches that extend classical logic—for example, to handle qualitative or uncertain information—and to improve its computational tractability.
• Chapter 1 provides background for many of the subsequent chapters by surveying classical logic and methods of automated reasoning.
• Chapter 2 describes the remarkable success of satisﬁability (SAT) solvers. Researchers have found that this type of automated reasoning can be used for an ever increasing set of practical applications and that it can be made surprisingly efﬁcient.
• Chapter 3 reviews research in Description Logics, which provides methods for representing and reasoning with terminological knowledge. Description logics are the core of the representation language of the Semantic Web.
• Chapter 4 describes constraint programming, a powerful paradigm for solving combinatorial search problems. This style of knowledge representation and reasoning draws together a wide range of techniques from artiﬁcial intelligence, operations research, algorithms and graph theory.
vii

viii

Preface

• Chapter 5 reviews the inﬂuential work on Conceptual Graphs. This structured representation provides an expressive language and powerful reasoning methods that are essential for applications such as Natural Language Understanding.
• Chapter 6 introduces nonmonotonic logics, which deal with complications related to handling exceptions to general rules. These logics are called “nonmonotonic” because they describe the retraction of information from a knowledge base when additional exceptions are taken into account.
• Chapter 7 builds on the previous one by describing Answer Set logic, which neatly handles default rules and exceptions, along with the nonmonotonic reasoning that they engender. This form of logic also supports reasoning about the causal effects of actions—another key feature of common sense.
• Chapter 8 continues this theme with a survey of techniques for Belief Revision, that is, how an agent changes its knowledge base in light of new information that contradicts a previous belief.
• Chapter 9 explains the role of qualitative models of continuous systems. These models enable another key feature of common sense: reasoning with incomplete information. This form of reasoning can compute, for example, the possible future states of a system, which is important for numerous tasks, such as diagnosis and tutoring.
• Chapter 10 demonstrates that these theories and techniques establish the basis for problem solvers that exploit an explicit model of the behavior of systems for tasks such as design, testing, and diagnosis. Being based on ﬁrst principles knowledge and inference engines with a formal logical foundation, rather than experience tied to speciﬁc instances and situations, such model-based problem solvers achieve the competence and robustness needed for industrial applications of knowledge representation and reasoning techniques.
• Chapter 11 confronts the unavoidable problem of uncertainty in real world domains, and surveys the extensive research on Bayesian networks as a method for modeling and reasoning with uncertain beliefs.
Part II delves into the special challenges of representing and reasoning with some core domains of knowledge, including time, space, causation and action. These challenges are ubiquitous across application areas, so solutions must be general and composable.
• Chapter 12 discusses ways to represent the temporal aspects of an ever-changing world. In a theme that recurs throughout this section, this raises a variety of interesting ontological issues—such as whether time should be modeled with points or intervals, and at what level of granularity—along with the pragmatic consequences of these decisions.
• Chapter 13 surveys qualitative representations of space—including topology, orientation, shape, size and distance—as well as reasoning methods appropri-

Preface

ix

ate to each. Although no single theory covers these topics comprehensively, researchers have produced a powerful tool kit.
• Chapter 14 builds on the two previous chapters, and also on research on qualitative modeling, to tackle the general problem of physical reasoning. Two important domain theories are developed (for liquids and solid objects), and the key issue of shifting between alternative models is explored.
• Chapter 15 surveys representations of an agent’s knowledge and beliefs, including propositions about the knowledge state of other agents (e.g., “Tom believes that Mary knows. . .”). This work nicely extends to handle common knowledge and distributed knowledge within a community of agents.
• Chapter 16 surveys the long history of the “situation calculus”—a knowledge representation designed to handle dynamic worlds. As ﬁrst deﬁned by McCarthy and Hayes, a situation is “a complete state of the universe at an instance of time”. Because situations are ﬁrst-order objects that can be quantiﬁed over, this framework has proven to be a strong foundation for reasoning about change.
• Chapter 17 describes the Event Calculus as an alternative to the Situation Calculus with some additional nice features. In particular, the event calculus facilitates representing continuous events, nondeterministic effects, events with duration, triggered events, and more.
• Chapter 18 continues the development of representation languages designed for dynamic worlds by introducing Temporal Action Logics. This family of languages is especially well suited for reasoning about persistence, i.e., features of the world that carry forward through time, unchanged, until an action affects them. It facilitates the representation of nondeterministic actions, actions with duration, concurrent actions and delayed effects of actions, partly due to its use of explicit time, and it tightly couples an automated planner to the formalism.
• Chapter 19 focuses on Nonmonotonic Causal Logic, which handles dynamic worlds using a strong solution to the frame problem. This logic starts with assumption that everything has a cause: either a previous action or inertia (persistence). This results in nice formalizations for key issues such as ramiﬁcations, implied action preconditions, and concurrent interacting effects of actions.
Part III surveys important applications of knowledge representation and reasoning. The application areas span the breadth of Artiﬁcial Intelligence to include question answering, the Semantic Web, planning, robotics and multi-agent systems. Each application draws extensively on the research results described in Parts I and II.
• Chapter 20 surveys research in question answering systems. These systems answer questions given a corpus of relevant documents and, in some cases, a knowledge base of common sense information. The system’s challenge is to select relevant passages of text (an information retrieval task), interpret them (a natural language understanding task) and infer an answer to the question (a reasoning task).

x

Preface

• Chapter 21 reviews progress on the Semantic Web: an extension of the World Wide Web in which content is expressed in a formal language to enable software agents to ﬁnd, integrate and reason with it. This raises numerous challenges, including scaling knowledge representation methods to the size of the Web.
• Chapter 22 surveys advances in automated planning, which make these systems considerably more powerful than “classical planners” from the early years of Artiﬁcial Intelligence. The new framework supports, for example, nondeterministic actions and partial observability, which are important attributes of real-world domains.
• Chapter 23 extends knowledge representation and reasoning in a new direction: cognitive robotics. The challenge in this application is that the robots’ world is dynamic and incompletely known, which requires re-thinking traditional approaches to AI tasks, such as planning, as well as coupling high-level reasoning with low-level perception.
• Chapter 24 surveys research on multi-agent systems, in which it is important that each agent represent and reason about the other agents in the environment. This is especially challenging when the agents have different, or worse— conﬂicting—goals.
• Chapter 25 describes tools and techniques for knowledge engineering: how to acquire the knowledge that can be expressed in the formalisms described in the other chapters.
Together, these 25 chapters, organized in the three sections “General Methods”, “Specialized Representations” and “Applications”, provide a unique survey of the best that Knowledge Representation has achieved, written by researchers who have helped to shape the ﬁeld. We hope that students, researchers and practitioners in all areas of Artiﬁcial Intelligence and Cognitive Science will ﬁnd this book to be a useful resource.

Acknowledgement
Early drafts of each chapter were reviewed by authors of other chapters, and by these research colleagues: Krzysztof Apt, Paolo Ferraris, Enrico Giunchiglia, Joohyung Lee, Antonis Kakas, Benjamin Kuipers, David Poole, and Mary-Anne Williams. We are grateful to them all.

Frank Harmelen Vrije Universiteit Amsterdam
Vladimir Lifschitz University of Texas at Austin
Bruce Porter University of Texas at Austin
July 2007

Editors
Frank van Harmelen Vrije Universiteit Amsterdam The Netherlands Vladimir Lifschitz University of Texas at Austin USA Bruce Porter University of Texas at Austin USA
xi

This page intentionally left blank

Contributors

Franz Baader Technische Universität Dresden Germany
Marcello Balduccini Texas Tech University USA
Chitta Baral Arizona State University USA
Gerhard Brewka University of Leipzig Germany
Alessandro Cimatti ITC/IRST Italy
Anthony G. Cohn University of Leeds UK
Adnan Darwiche University of California, Los Angeles USA
Ernest Davis New York University USA
Patrick Doherty Linköping University Sweden
Michael Fisher University of Liverpool UK
Kenneth D. Forbus Northwestern University USA

Michael Gelfond Texas Tech University USA
Carla P. Gomes Cornell University USA
Jim Hendler Rensselaer Polytechnic Institute USA
Ian Horrocks University of Oxford UK
Henry Kautz University of Rochester USA
Jonas Kvarnström Linköping University Sweden
Gerhard Lakemeyer RWTH Aachen University Germany
Hector Levesque University of Toronto Canada
Yuliya Lierler University of Texas at Austin USA
Vladimir Lifschitz University of Texas at Austin USA
Fangzhen Lin Hong Kong University of Science and Technology Hong Kong

xiii

xiv

Contributors

Leora Morgenstern IBM Thomas J. Watson Research Center USA
Yoram Moses Technion, Israel Institute of Technology Israel
Erik T. Mueller IBM Thomas J. Watson Research Center USA
Ilkka Niemelä Helsinki University of Technology Finland
Pavlos Peppas University of Patras Greece
Marco Pistore Università di Trento Italy
David Plaisted University of North Carolina at Chapel Hill USA
Jochen Renz The Australian National University Australia
Francesca Rossi University of Padova Italy
Ashish Sabharwal Cornell University USA
Ulrike Sattler University of Manchester UK

Guus Schreiber Vrije Universiteit Amsterdam The Netherlands
Bart Selman Cornell University USA
John F. Sowa VivoMind Intelligence, Inc. USA
Peter Struss Technische Universität München Germany
Paolo Traverso ITC/IRST Italy
Mirosław Truszczyn´ski University of Kentucky USA
Hudson Turner University of Minnesota, Duluth USA
Peter van Beek University of Waterloo Canada
Frank van Harmelen Vrije Universiteit Amsterdam The Netherlands
Wiebe van der Hoek University of Liverpool UK
Toby Walsh University of New South Wales Australia
Michael Wooldridge University of Liverpool UK

Contents

Dedication

v

Preface

vii

Editors

xi

Contributors

xiii

Contents

xv

I General Methods in Knowledge Representation and

Reasoning

1

1 Knowledge Representation and Classical Logic

3

Vladimir Lifschitz, Leora Morgenstern and David Plaisted

1.1 Knowledge Representation and Classical Logic . . . . . . . . . . . . 3

1.2 Syntax, Semantics and Natural Deduction . . . . . . . . . . . . . . . 4

1.2.1 Propositional Logic . . . . . . . . . . . . . . . . . . . . . . . 4

1.2.2 First-Order Logic . . . . . . . . . . . . . . . . . . . . . . . . 8

1.2.3 Second-Order Logic . . . . . . . . . . . . . . . . . . . . . . . 16

1.3 Automated Theorem Proving . . . . . . . . . . . . . . . . . . . . . . 18

1.3.1 Resolution in the Propositional Calculus . . . . . . . . . . . . 22

1.3.2 First-Order Proof Systems . . . . . . . . . . . . . . . . . . . 25

1.3.3 Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

1.3.4 Term Rewriting Systems . . . . . . . . . . . . . . . . . . . . 43

1.3.5 Conﬂuence and Termination Properties . . . . . . . . . . . . 46

1.3.6 Equational Rewriting . . . . . . . . . . . . . . . . . . . . . . 50

1.3.7 Other Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

1.4 Applications of Automated Theorem Provers . . . . . . . . . . . . . 58

1.4.1 Applications Involving Human Intervention . . . . . . . . . . 59

1.4.2 Non-Interactive KR Applications of Automated Theorem

Provers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

1.4.3 Exploiting Structure . . . . . . . . . . . . . . . . . . . . . . . 64

1.4.4 Prolog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

1.5 Suitability of Logic for Knowledge Representation . . . . . . . . . . 67

1.5.1 Anti-logicist Arguments and Responses . . . . . . . . . . . . 67

xv

xvi

Contents

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

2 Satisﬁability Solvers

89

Carla P. Gomes, Henry Kautz, Ashish Sabharwal and Bart Selman

2.1 Deﬁnitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . 91

2.2 SAT Solver Technology—Complete Methods . . . . . . . . . . . . . 92

2.2.1 The DPLL Procedure . . . . . . . . . . . . . . . . . . . . . . 92

2.2.2 Key Features of Modern DPLL-Based SAT Solvers . . . . . 93

2.2.3 Clause Learning and Iterative DPLL . . . . . . . . . . . . . . 95

2.2.4 A Proof Complexity Perspective . . . . . . . . . . . . . . . . 100

2.2.5 Symmetry Breaking . . . . . . . . . . . . . . . . . . . . . . . 104

2.3 SAT Solver Technology—Incomplete Methods . . . . . . . . . . . . 107

2.3.1 The Phase Transition Phenomenon in Random k-SAT . . . . 109

2.3.2 A New Technique for Random k-SAT: Survey Propagation . 111

2.4 Runtime Variance and Problem Structure . . . . . . . . . . . . . . . 112

2.4.1 Fat and Heavy Tailed Behavior . . . . . . . . . . . . . . . . . 113

2.4.2 Backdoors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113

2.4.3 Restarts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

2.5 Beyond SAT: Quantiﬁed Boolean Formulas and Model Counting . . 117

2.5.1 QBF Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 117

2.5.2 Model Counting . . . . . . . . . . . . . . . . . . . . . . . . . 120

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

3 Description Logics

135

Franz Baader, Ian Horrocks and Ulrike Sattler

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

3.2 A Basic DL and its Extensions . . . . . . . . . . . . . . . . . . . . . 139

3.2.1 Syntax and Semantics of ALC . . . . . . . . . . . . . . . . . 140

3.2.2 Important Inference Problems . . . . . . . . . . . . . . . . . 141

3.2.3 Important Extensions to ALC . . . . . . . . . . . . . . . . . 142

3.3 Relationships with other Formalisms . . . . . . . . . . . . . . . . . . 144

3.3.1 DLs and Predicate Logic . . . . . . . . . . . . . . . . . . . . 144

3.3.2 DLs and Modal Logic . . . . . . . . . . . . . . . . . . . . . . 145

3.4 Tableau Based Reasoning Techniques . . . . . . . . . . . . . . . . . 146

3.4.1 A Tableau Algorithm for ALC . . . . . . . . . . . . . . . . . 146

3.4.2 Implementation and Optimization Techniques . . . . . . . . 150

3.5 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151

3.5.1 ALC ABox Consistency is PSpace-complete . . . . . . . . . 151

3.5.2 Adding General TBoxes Results in ExpTime-Hardness . . . 154

3.5.3 The Effect of other Constructors . . . . . . . . . . . . . . . . 154

3.6 Other Reasoning Techniques . . . . . . . . . . . . . . . . . . . . . . 155

3.6.1 The Automata Based Approach . . . . . . . . . . . . . . . . 156

3.6.2 Structural Approaches . . . . . . . . . . . . . . . . . . . . . . 161

3.7 DLs in Ontology Language Applications . . . . . . . . . . . . . . . 166

3.7.1 The OWL Ontology Language . . . . . . . . . . . . . . . . . 166

3.7.2 OWL Tools and Applications . . . . . . . . . . . . . . . . . . 167

Contents

xvii

3.8 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

4 Constraint Programming

181

Francesca Rossi, Peter van Beek and Toby Walsh

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

4.2 Constraint Propagation . . . . . . . . . . . . . . . . . . . . . . . . . 182

4.2.1 Local Consistency . . . . . . . . . . . . . . . . . . . . . . . . 183

4.2.2 Global Constraints . . . . . . . . . . . . . . . . . . . . . . . . 183

4.3 Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184

4.3.1 Backtracking Search . . . . . . . . . . . . . . . . . . . . . . 184

4.3.2 Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

4.3.3 Hybrid Methods . . . . . . . . . . . . . . . . . . . . . . . . . 188

4.4 Tractability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

4.4.1 Tractable Constraint Languages . . . . . . . . . . . . . . . . 189

4.4.2 Tractable Constraint Graphs . . . . . . . . . . . . . . . . . . 191

4.5 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

4.5.1 CP ∨ ¬ CP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

4.5.2 Viewpoints . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

4.5.3 Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

4.6 Soft Constraints and Optimization . . . . . . . . . . . . . . . . . . . 193

4.6.1 Modeling Soft Constraints . . . . . . . . . . . . . . . . . . . 194

4.6.2 Searching for the Best Solution . . . . . . . . . . . . . . . . . 195

4.6.3 Inference in Soft Constraints . . . . . . . . . . . . . . . . . . 195

4.7 Constraint Logic Programming . . . . . . . . . . . . . . . . . . . . . 197

4.7.1 Logic Programs . . . . . . . . . . . . . . . . . . . . . . . . . 197

4.7.2 Constraint Logic Programs . . . . . . . . . . . . . . . . . . . 198

4.7.3 LP and CLP Languages . . . . . . . . . . . . . . . . . . . . . 198

4.7.4 Other Programming Paradigms . . . . . . . . . . . . . . . . . 199

4.8 Beyond Finite Domains . . . . . . . . . . . . . . . . . . . . . . . . . 199

4.8.1 Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199

4.8.2 Temporal Problems . . . . . . . . . . . . . . . . . . . . . . . 200

4.8.3 Sets and other Datatypes . . . . . . . . . . . . . . . . . . . . 200

4.9 Distributed Constraint Programming . . . . . . . . . . . . . . . . . . 201

4.10 Application Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202

4.11 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

5 Conceptual Graphs

213

John F. Sowa

5.1 From Existential Graphs to Conceptual Graphs . . . . . . . . . . . . 213

5.2 Common Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217

5.3 Reasoning with Graphs . . . . . . . . . . . . . . . . . . . . . . . . . 223

5.4 Propositions, Situations, and Metalanguage . . . . . . . . . . . . . . 230

5.5 Research Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . 233

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235

xviii

Contents

6 Nonmonotonic Reasoning

239

Gerhard Brewka, Ilkka Niemelä and Mirosław Truszczyn´ ski

6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239

Rules with exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . 240

The frame problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 240

About this chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241

6.2 Default Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242

6.2.1 Basic Deﬁnitions and Properties . . . . . . . . . . . . . . . . 242

6.2.2 Computational Properties . . . . . . . . . . . . . . . . . . . . 246

6.2.3 Normal Default Theories . . . . . . . . . . . . . . . . . . . . 249

6.2.4 Closed-World Assumption and Normal Defaults . . . . . . . 250

6.2.5 Variants of Default Logic . . . . . . . . . . . . . . . . . . . . 252

6.3 Autoepistemic Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . 252

6.3.1 Preliminaries, Intuitions and Basic Results . . . . . . . . . . 253

6.3.2 Computational Properties . . . . . . . . . . . . . . . . . . . . 258

6.4 Circumscription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260

6.4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260

6.4.2 Deﬁning Circumscription . . . . . . . . . . . . . . . . . . . . 261

6.4.3 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263

6.4.4 Computational Properties . . . . . . . . . . . . . . . . . . . . 264

6.4.5 Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

6.5 Nonmonotonic Inference Relations . . . . . . . . . . . . . . . . . . . 267

6.5.1 Semantic Speciﬁcation of Inference Relations . . . . . . . . . 268

6.5.2 Default Conditionals . . . . . . . . . . . . . . . . . . . . . . 270

6.5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272

6.6 Further Issues and Conclusion . . . . . . . . . . . . . . . . . . . . . 272

6.6.1 Relating Default and Autoepistemic Logics . . . . . . . . . . 273

6.6.2 Relating Default Logic and Circumscription . . . . . . . . . 275

6.6.3 Further Approaches . . . . . . . . . . . . . . . . . . . . . . . 276

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

7 Answer Sets

285

Michael Gelfond

7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285

7.2 Syntax and Semantics of Answer Set Prolog . . . . . . . . . . . . . . 286

7.3 Properties of Logic Programs . . . . . . . . . . . . . . . . . . . . . . 292

7.3.1 Consistency of Logic Programs . . . . . . . . . . . . . . . . 292

7.3.2 Reasoning Methods for Answer Set Prolog . . . . . . . . . . 295

7.3.3 Properties of Entailment . . . . . . . . . . . . . . . . . . . . 297

7.3.4 Relations between Programs . . . . . . . . . . . . . . . . . . 298

7.4 A Simple Knowledge Base . . . . . . . . . . . . . . . . . . . . . . . 300

7.5 Reasoning in Dynamic Domains . . . . . . . . . . . . . . . . . . . . 302

7.6 Extensions of Answer Set Prolog . . . . . . . . . . . . . . . . . . . . 307

7.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310

Contents

xix

8 Belief Revision

317

Pavlos Peppas

8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317

8.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318

8.3 The AGM Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . 318

8.3.1 The AGM Postulates for Belief Revision . . . . . . . . . . . 319

8.3.2 The AGM Postulates for Belief Contraction . . . . . . . . . . 320

8.3.3 Selection Functions . . . . . . . . . . . . . . . . . . . . . . . 323

8.3.4 Epistemic Entrenchment . . . . . . . . . . . . . . . . . . . . 325

8.3.5 System of Spheres . . . . . . . . . . . . . . . . . . . . . . . . 327

8.4 Belief Base Change . . . . . . . . . . . . . . . . . . . . . . . . . . . 329

8.4.1 Belief Base Change Operations . . . . . . . . . . . . . . . . . 331

8.4.2 Belief Base Change Schemes . . . . . . . . . . . . . . . . . . 332

8.5 Multiple Belief Change . . . . . . . . . . . . . . . . . . . . . . . . . 335

8.5.1 Multiple Revision . . . . . . . . . . . . . . . . . . . . . . . . 336

8.5.2 Multiple Contraction . . . . . . . . . . . . . . . . . . . . . . 338

8.6 Iterated Revision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340

8.6.1 Iterated Revision with Enriched Epistemic Input . . . . . . . 340

8.6.2 Iterated Revision with Simple Epistemic Input . . . . . . . . 343

8.7 Non-Prioritized Revision . . . . . . . . . . . . . . . . . . . . . . . . 346

8.8 Belief Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349

8.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353

9 Qualitative Modeling

361

Kenneth D. Forbus

9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361

9.1.1 Key Principles . . . . . . . . . . . . . . . . . . . . . . . . . . 362

9.1.2 Overview of Basic Qualitative Reasoning . . . . . . . . . . . 363

9.2 Qualitative Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . 365

9.2.1 Quantities . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365

9.2.2 Functions and Relationships . . . . . . . . . . . . . . . . . . 369

9.3 Ontology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371

9.3.1 Component Ontologies . . . . . . . . . . . . . . . . . . . . . 372

9.3.2 Process Ontologies . . . . . . . . . . . . . . . . . . . . . . . 373

9.3.3 Field Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . 374

9.4 Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374

9.5 Compositional Modeling . . . . . . . . . . . . . . . . . . . . . . . . 376

9.5.1 Model Formulation Algorithms . . . . . . . . . . . . . . . . . 378

9.6 Qualitative States and Qualitative Simulation . . . . . . . . . . . . . 379

9.7 Qualitative Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . 381

9.7.1 Topological Representations . . . . . . . . . . . . . . . . . . 381

9.7.2 Shape, Location, and Orientation Representations . . . . . . 382

9.7.3 Diagrammatic Reasoning . . . . . . . . . . . . . . . . . . . . 382

9.8 Qualitative Modeling Applications . . . . . . . . . . . . . . . . . . . 383

xx

Contents

9.8.1 Automating or Assisting Professional Reasoning . . . . . . . 383 9.8.2 Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384 9.8.3 Cognitive Modeling . . . . . . . . . . . . . . . . . . . . . . . 386 9.9 Frontiers and Resources . . . . . . . . . . . . . . . . . . . . . . . . . 387 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387

10 Model-based Problem Solving

395

Peter Struss

10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395

10.2 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398

10.2.1 Situation Assessment/Diagnosis . . . . . . . . . . . . . . 398

10.2.2 Test Generation, Measurement Proposal, Diagnosability

Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 399

10.2.3 Design and Failure-Modes-and-Effects Analysis . . . . . 401

10.2.4 Proposal of Remedial Actions (Repair, Reconﬁguration,

Recovery, Therapy) . . . . . . . . . . . . . . . . . . . . . 402

10.2.5 Ingredients of Model-based Problem Solving . . . . . . . 402

10.3 Requirements on Modeling . . . . . . . . . . . . . . . . . . . . . . 403

10.3.1 Behavior Prediction and Consistency Check . . . . . . . 404

10.3.2 Validity of Behavior Modeling . . . . . . . . . . . . . . . 405

10.3.3 Conceptual Modeling . . . . . . . . . . . . . . . . . . . . 405

10.3.4 (Automated) Model Composition . . . . . . . . . . . . . 406

10.3.5 Genericity . . . . . . . . . . . . . . . . . . . . . . . . . . 406

10.3.6 Appropriate Granularity . . . . . . . . . . . . . . . . . . 407

10.4 Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407

10.4.1 Consistency-based Diagnosis with Component-oriented

Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408

10.4.2 Computation of Diagnoses . . . . . . . . . . . . . . . . . 418

10.4.3 Solution Scope and Limitations of Component-Oriented

Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . 422

10.4.4 Diagnosis across Time . . . . . . . . . . . . . . . . . . . 423

10.4.5 Abductive Diagnosis . . . . . . . . . . . . . . . . . . . . 431

10.4.6 Process-Oriented Diagnosis . . . . . . . . . . . . . . . . 434

10.4.7 Model-based Diagnosis in Control Engineering . . . . . . 438

10.5 Test and Measurement Proposal, Diagnosability Analysis . . . . . 438

10.5.1 Test Generation . . . . . . . . . . . . . . . . . . . . . . . 439

10.5.2 Entropy-based Test Selection . . . . . . . . . . . . . . . . 444

10.5.3 Probe Selection . . . . . . . . . . . . . . . . . . . . . . . 445

10.5.4 Diagnosability Analysis . . . . . . . . . . . . . . . . . . . 446

10.6 Remedy Proposal . . . . . . . . . . . . . . . . . . . . . . . . . . . 446

10.6.1 Integration of Diagnosis and Remedy Actions . . . . . . 448

10.6.2 Component-oriented Reconﬁguration . . . . . . . . . . . 450

10.6.3 Process-oriented Therapy Proposal . . . . . . . . . . . . 453

10.7 Other Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454

10.7.1 Conﬁguration and Design . . . . . . . . . . . . . . . . . . 454

10.7.2 Failure-Modes-and-Effects Analysis . . . . . . . . . . . . 456

10.7.3 Debugging and Testing of Software . . . . . . . . . . . . 456

Contents

xxi

10.8 State and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 458 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460

11 Bayesian Networks

467

Adnan Darwiche

11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467

11.2 Syntax and Semantics of Bayesian Networks . . . . . . . . . . . . 468

11.2.1 Notational Conventions . . . . . . . . . . . . . . . . . . . 468

11.2.2 Probabilistic Beliefs . . . . . . . . . . . . . . . . . . . . . 469

11.2.3 Bayesian Networks . . . . . . . . . . . . . . . . . . . . . 470

11.2.4 Structured Representations of CPTs . . . . . . . . . . . . 471

11.2.5 Reasoning about Independence . . . . . . . . . . . . . . . 471

11.2.6 Dynamic Bayesian Networks . . . . . . . . . . . . . . . . 472

11.3 Exact Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473

11.3.1 Structure-Based Algorithms . . . . . . . . . . . . . . . . 474

11.3.2 Inference with Local (Parametric) Structure . . . . . . . . 479

11.3.3 Solving MAP and MPE by Search . . . . . . . . . . . . . 480

11.3.4 Compiling Bayesian Networks . . . . . . . . . . . . . . . 481

11.3.5 Inference by Reduction to Logic . . . . . . . . . . . . . . 482

11.3.6 Additional Inference Techniques . . . . . . . . . . . . . . 484

11.4 Approximate Inference . . . . . . . . . . . . . . . . . . . . . . . . 485

11.4.1 Inference by Stochastic Sampling . . . . . . . . . . . . . 485

11.4.2 Inference as Optimization . . . . . . . . . . . . . . . . . 486

11.5 Constructing Bayesian Networks . . . . . . . . . . . . . . . . . . 489

11.5.1 Knowledge Engineering . . . . . . . . . . . . . . . . . . 489

11.5.2 High-Level Speciﬁcations . . . . . . . . . . . . . . . . . 490

11.5.3 Learning Bayesian Networks . . . . . . . . . . . . . . . . 493

11.6 Causality and Intervention . . . . . . . . . . . . . . . . . . . . . . 497

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499

II Classes of Knowledge and Specialized Representations 511

12 Temporal Representation and Reasoning

513

Michael Fisher

12.1 Temporal Structures . . . . . . . . . . . . . . . . . . . . . . . . . . 514

12.1.1 Instants and Durations . . . . . . . . . . . . . . . . . . . 514

12.1.2 From Discreteness to Density . . . . . . . . . . . . . . . 515

12.1.3 Granularity Hierarchies . . . . . . . . . . . . . . . . . . . 516

12.1.4 Temporal Organisation . . . . . . . . . . . . . . . . . . . 517

12.1.5 Moving in Real Time . . . . . . . . . . . . . . . . . . . . 517

12.1.6 Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . 518

12.2 Temporal Language . . . . . . . . . . . . . . . . . . . . . . . . . . 520

12.2.1 Modal Temporal Logic . . . . . . . . . . . . . . . . . . . 520

12.2.2 Back to the Future . . . . . . . . . . . . . . . . . . . . . . 521

12.2.3 Temporal Arguments and Reiﬁed Temporal Logics . . . . 521

xxii

Contents

12.2.4 Operators over Non-discrete Models . . . . . . . . . . . . 522 12.2.5 Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . 523 12.2.6 Real-Time and Hybrid Temporal Languages . . . . . . . 524 12.2.7 Quantiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . 525 12.2.8 Hybrid Temporal Logic and the Concept of “now” . . . . 528 12.3 Temporal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 528 12.3.1 Proof Systems . . . . . . . . . . . . . . . . . . . . . . . . 529 12.3.2 Automated Deduction . . . . . . . . . . . . . . . . . . . . 529 12.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530 12.4.1 Natural Language . . . . . . . . . . . . . . . . . . . . . . 530 12.4.2 Reactive System Speciﬁcation . . . . . . . . . . . . . . . 531 12.4.3 Theorem-Proving . . . . . . . . . . . . . . . . . . . . . . 532 12.4.4 Model Checking . . . . . . . . . . . . . . . . . . . . . . . 532 12.4.5 PSL/Sugar . . . . . . . . . . . . . . . . . . . . . . . . . . 534 12.4.6 Temporal Description Logics . . . . . . . . . . . . . . . . 534 12.5 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . 535 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535

13 Qualitative Spatial Representation and Reasoning

551

Anthony G. Cohn and Jochen Renz

13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551

13.1.1 What is Qualitative Spatial Reasoning? . . . . . . . . . . 551

13.1.2 Applications of Qualitative Spatial Reasoning . . . . . . 553

13.2 Aspects of Qualitative Spatial Representation . . . . . . . . . . . . 554

13.2.1 Ontology . . . . . . . . . . . . . . . . . . . . . . . . . . . 554

13.2.2 Spatial Relations . . . . . . . . . . . . . . . . . . . . . . 556

13.2.3 Mereology . . . . . . . . . . . . . . . . . . . . . . . . . . 557

13.2.4 Mereotopology . . . . . . . . . . . . . . . . . . . . . . . 557

13.2.5 Between Mereotopology and Fully Metric Spatial Repre-

sentation . . . . . . . . . . . . . . . . . . . . . . . . . . . 566

13.2.6 Mereogeometry . . . . . . . . . . . . . . . . . . . . . . . 570

13.2.7 Spatial Vagueness . . . . . . . . . . . . . . . . . . . . . . 571

13.3 Spatial Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 572

13.3.1 Deduction . . . . . . . . . . . . . . . . . . . . . . . . . . 574

13.3.2 Composition . . . . . . . . . . . . . . . . . . . . . . . . . 575

13.3.3 Constraint-based Spatial Reasoning . . . . . . . . . . . . 576

13.3.4 Finding Efﬁcient Reasoning Algorithms . . . . . . . . . . 578

13.3.5 Planar Realizability . . . . . . . . . . . . . . . . . . . . . 581

13.4 Reasoning about Spatial Change . . . . . . . . . . . . . . . . . . . 581

13.5 Cognitive Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . 582

13.6 Final Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584

Contents

xxiii

14 Physical Reasoning

597

Ernest Davis

14.1 Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600

14.1.1 Component Analysis . . . . . . . . . . . . . . . . . . . . 600

14.1.2 Process Model . . . . . . . . . . . . . . . . . . . . . . . . 601

14.2 Domain Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . 602

14.2.1 Rigid Object Kinematics . . . . . . . . . . . . . . . . . . 603

14.2.2 Rigid Object Dynamics . . . . . . . . . . . . . . . . . . . 605

14.2.3 Liquids . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608

14.3 Abstraction and Multiple Models . . . . . . . . . . . . . . . . . . 611

14.4 Historical and Bibliographical . . . . . . . . . . . . . . . . . . . . 614

14.4.1 Logic-based Representations . . . . . . . . . . . . . . . . 614

14.4.2 Solid Objects: Kinematics . . . . . . . . . . . . . . . . . 615

14.4.3 Solid Object Dynamics . . . . . . . . . . . . . . . . . . . 616

14.4.4 Abstraction and Multiple Models . . . . . . . . . . . . . 616

14.4.5 Other . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616

14.4.6 Books . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618

15 Reasoning about Knowledge and Belief

621

Yoram Moses

15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621

15.2 The Possible Worlds Model . . . . . . . . . . . . . . . . . . . . . 622

15.2.1 A Language for Knowledge and Belief . . . . . . . . . . 622

15.3 Properties of Knowledge . . . . . . . . . . . . . . . . . . . . . . . 626

15.4 The Knowledge of Groups . . . . . . . . . . . . . . . . . . . . . . 628

15.4.1 Common Knowledge . . . . . . . . . . . . . . . . . . . . 629

15.4.2 Distributed Knowledge . . . . . . . . . . . . . . . . . . . 632

15.5 Runs and Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 633

15.6 Adding Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635

15.6.1 Common Knowledge and Time . . . . . . . . . . . . . . 636

15.7 Knowledge-based Behaviors . . . . . . . . . . . . . . . . . . . . . 637

15.7.1 Contexts and Protocols . . . . . . . . . . . . . . . . . . . 637

15.7.2 Knowledge-based Programs . . . . . . . . . . . . . . . . 639

15.7.3 A Subtle kb Program . . . . . . . . . . . . . . . . . . . . 641

15.8 Beyond Square One . . . . . . . . . . . . . . . . . . . . . . . . . . 643

15.9 How to Reason about Knowledge and Belief . . . . . . . . . . . . 644

15.9.1 Concluding Remark . . . . . . . . . . . . . . . . . . . . . 645

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645

Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647

16 Situation Calculus

649

Fangzhen Lin

16.1 Axiomatizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650

16.2 The Frame, the Ramiﬁcation and the Qualiﬁcation Problems . . . 652

16.2.1 The Frame Problem—Reiter’s Solution . . . . . . . . . . 654

16.2.2 The Ramiﬁcation Problem and Lin’s Solution . . . . . . . 657

xxiv

Contents

16.2.3 The Qualiﬁcation Problem . . . . . . . . . . . . . . . . . 660 16.3 Reiter’s Foundational Axioms and Basic Action Theories . . . . . 661 16.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 665 16.5 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . 667 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667

17 Event Calculus

671

Erik T. Mueller

17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671

17.2 Versions of the Event Calculus . . . . . . . . . . . . . . . . . . . . 672

17.2.1 Original Event Calculus (OEC) . . . . . . . . . . . . . . 672

17.2.2 Simpliﬁed Event Calculus (SEC) . . . . . . . . . . . . . . 674

17.2.3 Basic Event Calculus (BEC) . . . . . . . . . . . . . . . . 676

17.2.4 Event Calculus (EC) . . . . . . . . . . . . . . . . . . . . 679

17.2.5 Discrete Event Calculus (DEC) . . . . . . . . . . . . . . 681

17.2.6 Equivalence of DEC and EC . . . . . . . . . . . . . . . . 683

17.2.7 Other Versions . . . . . . . . . . . . . . . . . . . . . . . . 683

17.3 Relationship to other Formalisms . . . . . . . . . . . . . . . . . . 684

17.4 Default Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 684

17.4.1 Circumscription . . . . . . . . . . . . . . . . . . . . . . . 684

17.4.2 Computing Circumscription . . . . . . . . . . . . . . . . 685

17.4.3 Historical Note . . . . . . . . . . . . . . . . . . . . . . . 686

17.4.4 Negation as Failure . . . . . . . . . . . . . . . . . . . . . 687

17.5 Event Calculus Knowledge Representation . . . . . . . . . . . . . 687

17.5.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 687

17.5.2 Event Effects . . . . . . . . . . . . . . . . . . . . . . . . 688

17.5.3 Preconditions . . . . . . . . . . . . . . . . . . . . . . . . 689

17.5.4 State Constraints . . . . . . . . . . . . . . . . . . . . . . 689

17.5.5 Concurrent Events . . . . . . . . . . . . . . . . . . . . . . 690

17.5.6 Triggered Events . . . . . . . . . . . . . . . . . . . . . . 691

17.5.7 Continuous Change . . . . . . . . . . . . . . . . . . . . . 692

17.5.8 Nondeterministic Effects . . . . . . . . . . . . . . . . . . 693

17.5.9 Indirect Effects . . . . . . . . . . . . . . . . . . . . . . . 694

17.5.10 Partially Ordered Events . . . . . . . . . . . . . . . . . . 696

17.6 Action Language E . . . . . . . . . . . . . . . . . . . . . . . . . . 697

17.7 Automated Event Calculus Reasoning . . . . . . . . . . . . . . . . 699

17.7.1 Prolog . . . . . . . . . . . . . . . . . . . . . . . . . . . . 699

17.7.2 Answer Set Programming . . . . . . . . . . . . . . . . . 700

17.7.3 Satisﬁability (SAT) Solving . . . . . . . . . . . . . . . . 700

17.7.4 First-Order Logic Automated Theorem Proving . . . . . 700

17.8 Applications of the Event Calculus . . . . . . . . . . . . . . . . . 700

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701

18 Temporal Action Logics

709

Patrick Doherty and Jonas Kvarnström

18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 709

Contents

xxv

18.1.1 PMON and TAL . . . . . . . . . . . . . . . . . . . . . . 710 18.1.2 Previous Work . . . . . . . . . . . . . . . . . . . . . . . 711 18.1.3 Chapter Structure . . . . . . . . . . . . . . . . . . . . . 713 18.2 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713 18.3 TAL Narratives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 716 18.3.1 The Russian Airplane Hijack Scenario . . . . . . . . . . 717 18.3.2 Narrative Background Speciﬁcation . . . . . . . . . . . 718 18.3.3 Narrative Speciﬁcation . . . . . . . . . . . . . . . . . . 723 18.4 The Relation Between the TAL Languages L(ND) and L(FL) . . 724 18.5 The TAL Surface Language L(ND) . . . . . . . . . . . . . . . . . 725 18.5.1 Sorts, Terms and Variables . . . . . . . . . . . . . . . . 725 18.5.2 Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . 726 18.5.3 Statements . . . . . . . . . . . . . . . . . . . . . . . . . 727 18.6 The TAL Base Language L(FL) . . . . . . . . . . . . . . . . . . . 728 18.6.1 Translation from L(ND) to L(FL) . . . . . . . . . . . . 728 18.7 Circumscription and TAL . . . . . . . . . . . . . . . . . . . . . . . 730 18.8 Representing Ramiﬁcations in TAL . . . . . . . . . . . . . . . . . 735 18.9 Representing Qualiﬁcations in TAL . . . . . . . . . . . . . . . . . 737 18.9.1 Enabling Fluents . . . . . . . . . . . . . . . . . . . . . . 738 18.9.2 Strong Qualiﬁcation . . . . . . . . . . . . . . . . . . . . 740 18.9.3 Weak Qualiﬁcation . . . . . . . . . . . . . . . . . . . . . 740 18.9.4 Qualiﬁcation: Not Only For Actions . . . . . . . . . . . 741 18.9.5 Ramiﬁcations as Qualiﬁcations . . . . . . . . . . . . . . 742 18.10 Action Expressivity in TAL . . . . . . . . . . . . . . . . . . . . . 742 18.11 Concurrent Actions in TAL . . . . . . . . . . . . . . . . . . . . . . 744 18.11.1 Independent Concurrent Actions . . . . . . . . . . . . . 744 18.11.2 Interacting Concurrent Actions . . . . . . . . . . . . . . 745 18.11.3 Laws of Interaction . . . . . . . . . . . . . . . . . . . . 745 18.12 An Application of TAL: TALplanner . . . . . . . . . . . . . . . . 747 18.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753

19 Nonmonotonic Causal Logic

759

Hudson Turner

19.1 Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 762

19.1.1 Finite Domain Propositional Logic . . . . . . . . . . . . 762

19.1.2 Causal Theories . . . . . . . . . . . . . . . . . . . . . . 763

19.2 Strong Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . 765

19.3 Completion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766

19.4 Expressiveness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768

19.4.1 Nondeterminism: Coin Tossing . . . . . . . . . . . . . . 768

19.4.2 Implied Action Preconditions: Moving an Object . . . . 768

19.4.3 Things that Change by Themselves: Falling Dominos . 769

19.4.4 Things that Tend to Change by Themselves: Pendulum . 769

19.5 High-Level Action Language C+ . . . . . . . . . . . . . . . . . . 770

19.6 Relationship to Default Logic . . . . . . . . . . . . . . . . . . . . 771

xxvi

Contents

19.7 Causal Theories in Higher-Order Classical Logic . . . . . . . . . . 772 19.8 A Logic of Universal Causation . . . . . . . . . . . . . . . . . . . 773 Acknowledgement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774

III Knowledge Representation in Applications

777

20 Knowledge Representation and Question Answering

779

Marcello Balduccini, Chitta Baral and Yuliya Lierler

20.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 779

20.1.1 Role of Knowledge Representation and Reasoning in QA 780

20.1.2 Architectural Overview of QA Systems Using Knowl-

edge Representation and Reasoning . . . . . . . . . . . 782

20.2 From English to Logical Theories . . . . . . . . . . . . . . . . . . 783

20.3 The COGEX Logic Prover of the LCC QA System . . . . . . . . 790

20.4 Extracting Relevant Facts from Logical Theories and its Use in the

DD QA System about Dynamic Domains and Trips . . . . . . . . 792

20.4.1 The Overall Architecture of the DD System . . . . . . . 793

20.4.2 From Logic Forms to QSR Facts: An Illustration . . . . 794

20.4.3 OSR: From QSR Relations to Domain Relations . . . . 796

20.4.4 An Early Travel Module of the DD System . . . . . . . 798

20.4.5 Other Enhancements to the Travel Module . . . . . . . . 802

20.5 From Natural Language to Relevant Facts in the ASU QA System 803

20.6 Nutcracker—System for Recognizing Textual Entailment . . . . . 806

20.7 Mueller’s Story Understanding System . . . . . . . . . . . . . . . 810

20.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 813

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 815

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 815

21 The Semantic Web: Webizing Knowledge Representation

821

Jim Hendler and Frank van Harmelen

21.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 821

21.2 The Semantic Web Today . . . . . . . . . . . . . . . . . . . . . . 823

21.3 Semantic Web KR Language Design . . . . . . . . . . . . . . . . 826

21.3.1 Web Infrastructure . . . . . . . . . . . . . . . . . . . . . 826

21.3.2 Webizing KR . . . . . . . . . . . . . . . . . . . . . . . . 827

21.3.3 Scalability and the Semantic Web . . . . . . . . . . . . 830

21.4 OWL—Deﬁning a Semantic Web KR Language . . . . . . . . . . 831

21.5 Semantic Web KR Challenges . . . . . . . . . . . . . . . . . . . . 836

21.6 Beyond OWL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836

21.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 838

22 Automated Planning

841

Alessandro Cimatti, Marco Pistore and Paolo Traverso

22.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 841

Contents

xxvii

22.2 The General Framework . . . . . . . . . . . . . . . . . . . . . . . 843 22.2.1 Domains . . . . . . . . . . . . . . . . . . . . . . . . . . 843 22.2.2 Plans and Plan Executions . . . . . . . . . . . . . . . . . 844 22.2.3 Goals and Problems . . . . . . . . . . . . . . . . . . . . 845
22.3 Strong Planning under Full Observability . . . . . . . . . . . . . . 845 22.4 Strong Cyclic Planning under Full Observability . . . . . . . . . . 847 22.5 Planning for Temporally Extended Goals under Full Observability 850 22.6 Conformant Planning . . . . . . . . . . . . . . . . . . . . . . . . . 857 22.7 Strong Planning under Partial Observability . . . . . . . . . . . . 859 22.8 A Technological Overview . . . . . . . . . . . . . . . . . . . . . . 860 22.9 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 864

23 Cognitive Robotics

869

Hector Levesque and Gerhard Lakemeyer

23.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869

23.2 Knowledge Representation for Cognitive Robots . . . . . . . . . . 870

23.2.1 Varieties of Actions . . . . . . . . . . . . . . . . . . . . 871

23.2.2 Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . 871

23.2.3 Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . 872

23.3 Reasoning for Cognitive Robots . . . . . . . . . . . . . . . . . . . 873

23.3.1 Projection via Progression and Regression . . . . . . . . 873

23.3.2 Reasoning in Closed and Open Worlds . . . . . . . . . . 875

23.4 High-Level Control for Cognitive Robots . . . . . . . . . . . . . . 876

23.4.1 Classical Planning . . . . . . . . . . . . . . . . . . . . . 876

23.4.2 High-Level Ofﬂine Robot Programming . . . . . . . . . 877

23.4.3 High-Level Online Robot Programming . . . . . . . . . 879

23.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 881

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882

24 Multi-Agent Systems

887

Wiebe van der Hoek and Michael Wooldridge

24.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 887

24.2 Representing Rational Cognitive States . . . . . . . . . . . . . . . 888

24.2.1 A Logical Toolkit . . . . . . . . . . . . . . . . . . . . . 890

24.2.2 Dynamic Epistemic Logic . . . . . . . . . . . . . . . . . 891

24.2.3 Cohen and Levesque’s Intention Logic . . . . . . . . . . 893

24.2.4 Rao and Georgeff’s BDI Logics . . . . . . . . . . . . . . 896

24.2.5 The KARO Framework . . . . . . . . . . . . . . . . . . 899

24.2.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . 903

24.2.7 Cognitive Agent Logics in Practice . . . . . . . . . . . . 903

24.3 Representing the Strategic Structure of a System . . . . . . . . . . 909

24.3.1 Coalition Logic . . . . . . . . . . . . . . . . . . . . . . 910

24.3.2 Strategic Temporal Logic: ATL . . . . . . . . . . . . . . 913

24.3.3 Knowledge in Strategic Temporal Logics: ATEL . . . . . 916

24.3.4 CL-PC . . . . . . . . . . . . . . . . . . . . . . . . . . . 919

24.3.5 Applications of Strategic Cooperation Logics . . . . . . 920

xxviii

Contents

24.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920

25 Knowledge Engineering

929

Guus Schreiber

25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929

25.2 Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929

25.3 Tasks and Problem-Solving Methods . . . . . . . . . . . . . . . . 930

25.3.1 Two Sample Problem-Solving Methods . . . . . . . . . 930

25.3.2 The Notion of “Knowledge Role” . . . . . . . . . . . . 934

25.3.3 Speciﬁcation Languages . . . . . . . . . . . . . . . . . . 935

25.4 Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 936

25.4.1 Ontology Speciﬁcation Languages . . . . . . . . . . . . 937

25.4.2 Types of Ontologies . . . . . . . . . . . . . . . . . . . . 938

25.4.3 Ontology Engineering . . . . . . . . . . . . . . . . . . . 940

25.4.4 Ontologies and Data Models . . . . . . . . . . . . . . . 941

25.5 Knowledge Elicitation Techniques1 . . . . . . . . . . . . . . . . . 941

Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 943

Author Index

947

Subject Index

987

Part I
General Methods in Knowledge Representation and Reasoning

This page intentionally left blank

Handbook of Knowledge Representation

3

Edited by F. van Harmelen, V. Lifschitz and B. Porter

© 2008 Elsevier B.V. All rights reserved

DOI: 10.1016/S1574-6526(07)03001-5

Chapter 1
Knowledge Representation and
Classical Logic
Vladimir Lifschitz, Leora Morgenstern, David Plaisted
1.1 Knowledge Representation and Classical Logic
Mathematical logicians had developed the art of formalizing declarative knowledge long before the advent of the computer age. But they were interested primarily in formalizing mathematics. Because of the important role of nonmathematical knowledge in AI, their emphasis was too narrow from the perspective of knowledge representation, their formal languages were not sufﬁciently expressive. On the other hand, most logicians were not concerned about the possibility of automated reasoning; from the perspective of knowledge representation, they were often too generous in the choice of syntactic constructs. In spite of these differences, classical mathematical logic has exerted signiﬁcant inﬂuence on knowledge representation research, and it is appropriate to begin this Handbook with a discussion of the relationship between these ﬁelds.
The language of classical logic that is most widely used in the theory of knowledge representation is the language of ﬁrst-order (predicate) formulas. These are the formulas that John McCarthy proposed to use for representing declarative knowledge in his Advice Taker paper [171], and Alan Robinson proposed to prove automatically using resolution [230]. Propositional logic is, of course, the most important subset of ﬁrst-order logic; recent surge of interest in representing knowledge by propositional formulas is related to the creation of fast satisﬁability solvers for propositional logic (see Chapter 2). At the other end of the spectrum we ﬁnd higher-order languages of classical logic. Second-order formulas are particularly important for the theory of knowledge representation, among other reasons, because they are sufﬁciently expressive for deﬁning transitive closure and related concepts, and because they are used in the deﬁnition of circumscription (see Section 6.4).
Now a few words about the logical languages that are not considered “classical”. Formulas containing modal operators, such as operators representing knowledge and belief (Chapter 15), are not classical. Languages with a classical syntax but a nonclas-

4

1. Knowledge Representation and Classical Logic

sical semantics, such as intuitionistic logic and the superintuitionistic logic of strong equivalence (see Section 7.3.3), are not discussed in this chapter either. Nonmonotonic logics (Chapters 6 and 19) are nonclassical as well.
This chapter contains an introduction to the syntax and semantics of classical logic and to natural deduction; a survey of automated theorem proving; a concise overview of selected implementations and applications of theorem proving; and a brief discussion of the suitability of classical logic for knowledge representation, a debate as old as the ﬁeld itself.

1.2 Syntax, Semantics and Natural Deduction
Early versions of modern logical notation were introduced at the end of the 19th century in two short books. One was written by Gottlob Frege [89]; his intention was “to express a content through written signs in a more precise and clear way than it is possible to do through words” [261, p. 2]. The second, by Giuseppe Peano [204], introduces notation in which “every proposition assumes the form and the precision that equations have in algebra” [261, p. 85]. Two other logicians who have contributed to the creation of ﬁrst-order logic are Charles Sanders Peirce and Alfred Tarski.
The description of the syntax of logical formulas in this section is rather brief. A more detailed discussion of syntactic questions can be found in Chapter 2 of the Handbook of Logic in Artiﬁcial Intelligence and Logic Programming [68], or in introductory sections of any logic textbook.
1.2.1 Propositional Logic
Propositional logic was carved out of a more expressive formal language by Emil Post [216].
Syntax and semantics
A propositional signature is a nonempty set of symbols called atoms. (Some authors say “vocabulary” instead of “signature”, and “variable” instead of “atom”.) Formulas of a propositional signature σ are formed from atoms and the 0-place connectives ⊥ and using the unary connective ¬ and the binary connectives ∧, ∨, → and ↔. (Some authors write & for ∧, ⊃ for →, and ≡ for ↔.)1
The symbols FALSE and TRUE are called truth values. An interpretation of a propositional signature σ (or an assignment) is a function from σ into {FALSE, TRUE}. The semantics of propositional formulas deﬁnes which truth value is assigned to a formula F by an interpretation I . It refers to the following truth-valued functions, associated with the propositional connectives:

x
FALSE TRUE

¬(x)
TRUE FALSE

1Note that ⊥ and are not atoms, according to this deﬁnition. They do not belong to the signature, and the semantics of propositional logic, deﬁned below, treats them in a special way.

V. Lifschitz, L. Morgenstern, D. Plaisted

5

x
FALSE FALSE TRUE TRUE

y
FALSE TRUE FALSE TRUE

∧(x, y)
FALSE FALSE FALSE TRUE

∨(x, y)
FALSE TRUE TRUE TRUE

→(x, y)
TRUE TRUE FALSE TRUE

↔(x, y)
TRUE FALSE FALSE TRUE

For any formula F and any interpretation I , the truth value F I that is assigned to F by I is deﬁned recursively, as follows:
• for any atom F , F I = I (F ),

• ⊥I = FALSE, I = TRUE,

• (¬F )I = ¬(F I ),

• (F G)I = (F I , GI ) for every binary connective .

If the underlying signature is ﬁnite then the set of interpretations is ﬁnite also, and the values of F I for all interpretations I can be represented by a ﬁnite table, called the truth table of F .
If F I = TRUE then we say that the interpretation I satisﬁes F , or is a model of F (symbolically, I |= F ).
A formula F is a tautology if every interpretation satisﬁes F . Two formulas, or sets of formulas, are equivalent to each other if they are satisﬁed by the same interpretations. It is clear that F is equivalent to G if and only if F ↔ G is a tautology.
A set Γ of formulas is satisﬁable if there exists an interpretation satisfying all formulas in Γ . We say that Γ entails a formula F (symbolically, Γ |= F ) if every interpretation satisfying Γ satisﬁes F .2
To represent knowledge by propositional formulas, we choose a propositional signature σ such that interpretations of σ correspond to states of the system that we want to describe. Then any formula of σ represents a condition on states; a set of formulas can be viewed as a knowledge base; if a formula F is entailed by a knowledge base Γ then the condition expressed by F follows from the knowledge included in Γ .
Imagine, for instance, that Paul, Quentin and Robert share an ofﬁce. Let us agree to use the atom p to express that Paul is in the ofﬁce, and similarly q for Quentin and r for Robert. The knowledge base {p, q} entails neither r nor ¬r. (The semantics of propositional logic does not incorporate the closed world assumption, discussed below in Section 6.2.4.) But if we add to the knowledge base the formula

¬p ∨ ¬q ∨ ¬r,

(1.1)

expressing that at least one person is away, then the formula ¬r (Robert is away) will be entailed.

Explicit deﬁnitions
Let Γ be a set of formulas of a propositional signature σ . To extend Γ by an explicit deﬁnition means to add to σ a new atom d, and to add to Γ a formula of the form

2Thus the relation symbol |= is understood either as “satisﬁes” or as “entails” depending on whether its ﬁrst operand is an interpretation or a set of formulas.

6

1. Knowledge Representation and Classical Logic

d ↔ F , where F is a formula of the signature σ . For instance, if
σ = {p, q, r}, Γ = {p, q},
as in the example above, then we can introduce an explicit deﬁnition that makes d an abbreviation for the formula q ∧ r (“both Quentin and Robert are in”):
σ = {p, q, r, d}, Γ = {p, q, d ↔ (q ∧ r)}.
Adding an explicit deﬁnition to a knowledge base Γ is, in a sense, a trivial modiﬁcation. For instance, there is a simple one-to-one correspondence between the set of models of Γ and the set of models of such an extension: a model of the extended set of formulas can be turned into the corresponding model of Γ by restricting it to σ . It follows that the extended set of formulas is satisﬁable if and only if Γ is satisﬁable. It follows also that adding an explicit deﬁnition produces a “conservative extension”: a formula that does not contain the new atom d is entailed by the extended set of formulas if and only if it is entailed by Γ .
It is not true, however, that the extended knowledge base is equivalent to Γ . For instance, in the example above {p, q} does not entail d ↔ (q ∧ r), of course. This observation is related to the difference between two ways to convert a propositional formula to conjunctive normal form (that is, to turn it into a set of clauses): the more obvious method based on equivalent transformations on the one hand, and Tseitin’s procedure, reviewed in Section 2.2 below, on the other. The latter can be thought of as a sequence of steps that add explicit deﬁnitions to the current set of formulas, interspersed with equivalent transformations that make formulas smaller and turn them into clauses. Tseitin’s procedure is more efﬁcient, but it does not produce a CNF equivalent to the input formula; it only gives us a conservative extension.

Natural deduction in propositional logic
Natural deduction, invented by Gerhard Gentzen [96], formalizes the process of introducing and discharging assumptions, common in informal mathematical proofs.
In the natural deduction system for propositional system described below, derivable objects are sequents of the form Γ ⇒ F , where F is a formula, and Γ is a ﬁnite set of formulas (“F under assumptions Γ ”). For simplicity we only consider formulas that contain neither nor ↔; these connectives can be viewed as abbreviations. It is notationally convenient to write sets of assumptions as lists, and understand, for instance, A1, A2 ⇒ F as shorthand for {A1, A2} ⇒ F , and Γ, A ⇒ F as shorthand for Γ ∪ {A} ⇒ F .
The axiom schemas of this system are
F ⇒F
and
⇒ F ∨ ¬F.
The inference rules are shown in Fig. 1.1. Most of the rules can be can be divided into two groups—introduction rules (the left column) and elimination rules (the right column). Each of the introduction rules tells us how to derive a formula of some syntactic form. For instance, the conjunction introduction rule (∧I ) shows that we can derive

V. Lifschitz, L. Morgenstern, D. Plaisted

7

(∧I )

Γ ⇒F ⇒G Γ, ⇒F ∧G

(∨I )

Γ ⇒F Γ ⇒F ∨G

Γ ⇒G Γ ⇒F ∨G

(→I )

Γ,F ⇒G Γ ⇒F →G

(¬I )

Γ,F ⇒⊥ Γ ⇒¬F

(∧E)

Γ ⇒F ∧G Γ ⇒F

Γ ⇒F ∧G Γ ⇒G

(∨E)

Γ ⇒F ∨G

Γ,

1,F ⇒H 1, 2⇒H

(→E)

Γ ⇒F Γ,

⇒F →G ⇒G

(¬E)

Γ ⇒F Γ,

⇒¬F ⇒⊥

2,G⇒H

(C)

Γ ⇒⊥ Γ ⇒F

(W )

Γ ⇒Σ Γ, ⇒Σ

Figure 1.1: Inference rules of propositional logic.

a conjunction if we derive both conjunctive terms; the disjunction introduction rules (∨I ) show that we can derive a disjunction if we derive one of the disjunctive terms. Each of the elimination rules tells us how we can use a formula of some syntactic form. For instance, the conjunction elimination rules (∧E) show that a conjunction can be used to derive any of its conjunctive terms; the disjunction elimination rules (∨E) shows that a disjunction can be used to justify reasoning by cases.
Besides introduction and elimination rules, the deductive system includes the contradiction rule (C) and the weakening rule (W ).
In most inference rules, the set of assumptions in the conclusion is simply the union of the sets of assumptions of all the premises. The rules (→I ), (¬I ) and (∨E) are exceptions; when one of these rule is applied, some of the assumptions from the premises are “discharged”.
An example of a proof in this system is shown in Fig. 1.2. This proof can be informally summarized as follows. Assume ¬p, q → r and p ∨ q. We will prove r by cases.
Case 1: p. This contradicts the assumption ¬p, so that r follows. Case 2: q. In view of the assumption q → r, r follows also. Consequently, from the assumptions ¬p and q → r we have derived (p ∨ q) → r. The deductive system described above is sound and complete: a sequent Γ ⇒ F is provable in it if and only if Γ |= F . The ﬁrst proof of a completeness theorem for propositional logic (involving a different deductive system) is due to Post [216].
Meta-level and object-level proofs
When we want to establish that a formula F is entailed by a knowledge base Γ , the straightforward approach is to use the deﬁnition of entailment, that is, to reason about interpretations of the underlying signature. For instance, to check that the formulas ¬p and q → r entail (p ∨ q) → r we can argue that no interpretation of the signature {p, q, r} can satisfy both ¬p and q → r unless it satisﬁes (p ∨ q) → r as well.
A sound deductive system provides an “object-level” alternative to this meta-level approach. Once we proved the sequent Γ ⇒ F in the deductive system described above, we have established that Γ entails F . For instance, the claim that the formulas ¬p and q → r entail (p ∨ q) → r is justiﬁed by Fig. 1.2. As a matter of convenience, informal summaries, as in the example above, can be used instead of formal proofs.

8

1. Knowledge Representation and Classical Logic

1.

¬p ⇒ ¬p

— axiom

2.

q→r ⇒ q→r

— axiom

3.

p∨q ⇒ p∨q

— axiom

4.

p⇒p

— axiom

5.

p, ¬p ⇒ ⊥

— by (¬E) from 4, 1

6.

p, ¬p ⇒ r

— by (C) from 5

7.

q ⇒q

— axiom

8.

q,q → r ⇒ r

— by (→E) from 7, 2

9. p ∨ q, ¬p, q → r ⇒ r

— by (∨E) from 3, 6, 8

10.

¬p, q → r ⇒ (p ∨ q) → r — by (→I ) from 9

Figure 1.2: A proof in propositional logic.

Since the system is not only sound but also complete, the object-level approach to establishing entailment is, in principle, always applicable.
Object-level proofs can be used also to establish general properties of entailment. Consider, for instance, the following fact: for any formulas F1, . . . , Fn, the implications Fi → Fi+1 (i = 1, . . . , n − 1) entail F1 → Fn. We can justify it by saying that if we assume F1 then F2, . . . , Fn will consecutively follow using the given implications. By saying this, we have outlined a method for constructing a proof of the sequent
F1 → F2, . . . , Fn−1 → Fn ⇒ F1 → Fn
that consists of n−1 implication eliminations followed by an implication introduction.

1.2.2 First-Order Logic
Syntax
In ﬁrst-order logic, a signature is a set of symbols of two kinds—function constants and predicate constants—with a nonnegative integer, called the arity, assigned to each symbol. Function constants of arity 0 are called object constants; predicate constants of arity 0 are called propositional constants.
Object variables are elements of some ﬁxed inﬁnite sequence of symbols, for instance, x, y, z, x1, y1, z1, . . . . Terms of a signature σ are formed from object variables and from function constants of σ . An atomic formula of σ is an expression of the form P (t1, . . . , tn) or t1 = t2, where P is a predicate constant of arity n, and each ti is a term of σ .3 Formulas are formed from atomic formulas using propositional connectives and the quantiﬁers ∀, ∃.
An occurrence of a variable v in a formula F is bound if it belongs to a subformula of F that has the form ∀vG or ∃vG; otherwise it is free. If at least one occurrence of v in F is free then we say that v is a free variable of F . Note that a formula can contain both free and bound occurrences of the same variable, as in

P (x) ∧ ∃xQ(x).

(1.2)

3Note that equality is not a predicate constant, according to this deﬁnition. Although syntactically it is similar to binary predicate constants, it does not belong to the signature, and the semantics of ﬁrst-order logic, deﬁned below, treats equality in a special way.

V. Lifschitz, L. Morgenstern, D. Plaisted

9

We can avoid such cases by renaming bound occurrences of variables:

P (x) ∧ ∃x1Q(x1).

(1.3)

Both formulas have the same meaning: x has the property P , and there exists an object with the property Q.
A closed formula, or a sentence, is a formula without free variables. The universal closure of a formula F is the sentence ∀v1 . . . vnF , where v1, . . . , vn are the free variables of F .
The result of the substitution of a term t for a variable v in a formula F is the formula obtained from F by simultaneously replacing each free occurrence of v by t. When we intend to consider substitutions for v in a formula, it is convenient to denote this formula by an expression like F (v); then we can denote the result of substituting a term t for v in this formula by F (t).
By ∃!vF (v) (“there exists a unique v such that F (v)”) we denote the formula

∃v∀w(F (w) ↔ v = w),
where w is the ﬁrst variable that does not occur in F (v). A term t is substitutable for a variable v in a formula F if, for each variable w
occurring in t, no subformula of F that has the form ∀wG or ∃wG contains an occurrence of v which is free in F . (Some authors say in this case that t is free for x in F .) This condition is important because when it is violated, the formula obtained by substituting t for v in F does not usually convey the intended meaning. For instance, the formula ∃x(f (x) = y) expresses that y belongs to the range of f . If we substitute, say, the term g(a, z) for y in this formula then we will get the formula ∃x(f (x) = g(a, z)), which expresses that g(a, z) belongs to the range of f —as one would expect. If, however, we substitute the term g(a, x) instead, the result ∃x(f (x) = g(a, x)) will not express that g(a, x) belongs to the range of f . This is related to the fact that the term g(a, x) is not substitutable for y in ∃x(f (x) = y); the occurrence of x resulting from this substitution is “captured” by the quantiﬁer at the beginning of the formula. To express that g(a, x) belongs to the range of f , we should ﬁrst rename x in the formula ∃x(f (x) = y) using, say, the variable x1. The substitution will produce then the formula ∃x1(f (x1) = g(a, x)).

Semantics An interpretation (or structure) of a signature σ consists of
• a nonempty set |I |, called the universe (or domain) of I ,

• for every object constant c of σ , an element cI of |I |,

• for every function constant f of σ of arity n > 0, a function f I from |I |n to |I |,
• for every propositional constant P of σ , an element P I of {FALSE, TRUE},
• for every predicate constant R of σ of arity n > 0, a function RI from |I |n to {FALSE, TRUE}.
The semantics of ﬁrst-order logic deﬁnes, for any sentence F and any interpretation I of a signature σ , the truth value F I that is assigned to F by I . Note that the

10

1. Knowledge Representation and Classical Logic

deﬁnition does not apply to formulas with free variables. (Whether ∃x(f (x) = y) is true or false, for instance, is not completely determined by the universe and by the function representing f ; the answer depends also on the value of y within the universe.) For this reason, stating correctly the clauses for quantiﬁers in the recursive deﬁnition of F I is a little tricky. One possibility is to extend the signature σ by “names” for all elements of the universe, as follows.
Consider an interpretation I of a signature σ . For any element ξ of its universe |I |, select a new symbol ξ ∗, called the name of ξ . By σ I we denote the signature obtained from σ by adding all names ξ ∗ as object constants. The interpretation I can be extended to the new signature σ I by deﬁning (ξ ∗)I = ξ for all ξ ∈ |I |.
For any term t of the extended signature that does not contain variables, we will deﬁne recursively the element tI of the universe that is assigned to t by I . If t is an object constant then tI is part of the interpretation I . For other terms, tI is deﬁned by the equation
f (t1, . . . , tn)I = f I (t1I , . . . , tnI )
for all function constants f of arity n > 0. Now we are ready to deﬁne F I for every sentence F of the extended signature σ I .
For any propositional constant P , P I is part of the interpretation I . Otherwise, we deﬁne:
• R(t1, . . . , tn)I = RI (t1I , . . . , tnI ),
• ⊥I = FALSE, I = TRUE,
• (¬F )I = ¬(F I ),
• (F G)I = (F I , GI ) for every binary connective ,
• ∀wF (w)I = TRUE if F (ξ ∗)I = TRUE for all ξ ∈ |I |,
• ∃wF (w)I = TRUE if F (ξ ∗)I = TRUE for some ξ ∈ |I |.
We say that an interpretation I satisﬁes a sentence F , or is a model of F , and write I |= F , if F I = TRUE. A sentence F is logically valid if every interpretation satisﬁes F . Two sentences, or sets of sentences, are equivalent to each other if they are satisﬁed by the same interpretations. A formula with free variables is said to be logically valid if its universal closure is logically valid. Formulas F and G that may contain free variables are equivalent to each other if F ↔ G is logically valid.
A set Γ of sentences is satisﬁable if there exists an interpretation satisfying all sentences in Γ . A set Γ of sentences entails a formula F (symbolically, Γ |= F ) if every interpretation satisfying Γ satisﬁes the universal closure of F .
Sorts
Representing knowledge in ﬁrst-order languages can be often simpliﬁed by introducing sorts, which requires that the deﬁnitions of the syntax and semantics above be generalized.
Besides function constants and predicate constants, a many-sorted signature includes symbols called sorts. In addition to an arity n, we assign to every function

V. Lifschitz, L. Morgenstern, D. Plaisted

11

constant and every predicate constant its argument sorts s1, . . . , sn; to every function constant we assign also its value sort sn+1. For instance, in the situation calculus (Section 16.1), the symbols situation and action are sorts; do is a binary function sym-
bol with the argument sorts action and situation, and the value sort situation.
For every sort s, we assume a separate inﬁnite sequence of variables of that sort.
The recursive deﬁnition of a term assigns a sort to every term. Atomic formulas are
expressions of the form P (t1, . . . , tn), where the sorts of the terms t1, . . . , tn are the argument sorts of P , and also expressions t1 = t2 where t1 and t2 are terms of the same sort.
An interpretation, in the many-sorted setting, includes a separate nonempty universe |I |s for each sort s. Otherwise, extending the deﬁnition of the semantics to many-sorted languages is straightforward.
A further extension of the syntax and semantics of ﬁrst-order formulas allows one
sort to be a “subsort” of another. For instance, when we talk about the blocks world,
it may be convenient to treat the sort block as a subsort of the sort location. Let b1 and b2 be object constants of the sort block, let table be an object constant of the sort location, and let on be a binary function constant with the argument sorts block and
location. Not only on(b1, table) will be counted as a term, but also on(b1, b2), because the sort of b2 is a subsort of the second argument sort of on.
Generally, a subsort relation is an order (reﬂexive, transitive and anti-symmetric
relation) on the set of sorts. In the recursive deﬁnition of a term, f (t1, . . . , tn) is a term if the sort of each ti is a subsort of the ith argument sort of f . The condition on sorts in the deﬁnition of atomic formulas P (t1, . . . , tn) is similar. An expression t1 = t2 is considered an atomic formula if the sorts of t1 and t2 have a common supersort. In the deﬁnition of an interpretation, |I |s1 is required to be a subset of |I |s2 whenever s1 is a subsort of s2.
In the rest of this chapter we often assume for simplicity that the underlying signa-
ture is nonsorted.

Uniqueness of names
To talk about Paul, Quentin and Robert from Section 1.2.1 in a ﬁrst-order language, we can introduce the signature consisting of the object constants Paul, Quentin, Robert and the unary predicate constant in, and then use the atomic sentences

in(Paul), in(Quentin), in(Robert)

(1.4)

instead of the atoms p, q, r from the propositional representation. However some interpretations of this signature are unintuitive and do not corre-
spond to any of the 8 interpretations of the propositional signature {p, q, r}. Those are the intepretations that map two, or even all three, object constants to the same element of the universe. (The deﬁnition of an interpretation in ﬁrst-order logic does not require that c1I be different from c2I for distinct object constants c1, c2.) We can express that PaulI , QuentinI and RobertI are pairwise distinct by saying that I satisﬁes the “unique name conditions”

Paul = Quentin, Paul = Robert, Quentin = Robert.

(1.5)

12

1. Knowledge Representation and Classical Logic

Generally, the unique name assumption for a signature σ is expressed by the formulas

∀x1 . . . xmy1 . . . yn(f (x1, . . . , xm) = g(y1, . . . , yn))

(1.6)

for all pairs of distinct function constants f , g, and

∀x1 . . . xny1 . . . yn(f (x1, . . . , xn) = f (y1, . . . , yn)

→ (x1 = y1 ∧ · · · ∧ xn = yn))

(1.7)

for all function constants f of arity > 0. These formulas entail t1 = t2 for any distinct variable-free terms t1, t2.
The set of equality axioms that was introduced by Keith Clark [57] and is often
used in the theory of logic programming includes, in addition to (1.6) and (1.7), the axioms t = x, where t is a term containing x as a proper subterm.

Domain closure
Consider the ﬁrst-order counterpart of the propositional formula (1.1), expressing that at least one person is away:

¬in(Paul) ∨ ¬in(Quentin) ∨ ¬in(Robert).

(1.8)

The same idea can be also conveyed by the formula

∃x¬in(x).

(1.9)

But sentences (1.8) and (1.9) are not equivalent to each other: the former entails the latter, but not the other way around. Indeed, the deﬁnition of an interpretation in ﬁrstorder logic does not require that every element of the universe be equal to cI for some object constant c. Formula (1.9) interprets “at least one” as referring to a certain group that includes Paul, Quentin and Robert, and may also include others.
If we want to express that every element of the universe corresponds to one of the three explicitly named persons then this can be done by the formula

∀x(x = Paul ∨ x = Quentin ∨ x = Robert).

(1.10)

This “domain closure condition” entails the equivalence between (1.8) and (1.9); more generally, it entails the equivalences

∀xF (x) ↔ F (Paul) ∧ F (Quentin) ∧ F (Robert),
∃xF (x) ↔ F (Paul) ∨ F (Quentin) ∨ F (Robert)
for any formula F (x). These equivalences allow us to replace all quantiﬁers in an arbitrary formula with multiple conjunctions and disjunctions. Furthermore, under the unique name assumption (1.5) any equality between two object constants can be equivalently replaced by or ⊥, depending on whether the constants are equal to each other. The result of these transformations is a propositional combination of the atomic sentences (1.4).
Generally, consider a signature σ containing ﬁnitely many object constants c1, . . . , cn are no function constants of arity > 0. The domain closure assumption

V. Lifschitz, L. Morgenstern, D. Plaisted

13

for σ is the formula

∀x(x = c1 ∨ · · · ∨ x = cn).

(1.11)

The interpretations of σ that satisfy both the unique name assumption c1 = cj (1 i < j n) and the domain closure assumption (1.11) are essentially identical to the interpretations of the propositional signature that consists of all atomic sentences of σ other than equalities. Any sentence F of σ can be transformed into a formula F of this propositional signature such that the unique name and domain closure assumptions entail F ↔ F . In this sense, these assumptions turn ﬁrst-order sentences into abbreviations for propositional formulas.
The domain closure assumption in the presence of function constant of arity > 0 is discussed in Sections 1.2.2 and 1.2.3.

Reiﬁcation

The ﬁrst-order language introduced in Section 1.2.2 has variables for people, such as Paul and Quentin, but not for places, such as their ofﬁce. In this sense, people are “reiﬁed” in that language, and places are not. To reify places, we can add them to the signature as a second sort, add ofﬁce as an object constant of that sort, and turn in into a binary predicate constant with the argument sorts person and place. In the modiﬁed language, the formula in(Paul) will turn into in(Paul, ofﬁce).
Reiﬁcation makes the language more expressive. For instance, having reiﬁed places, we can say that every person has a unique location:

∀x∃!p in(x, p).

(1.12)

There is no way to express this idea in the language from Section 1.2.2. As another example illustrating the idea of reiﬁcation, compare two versions of the
situation calculus. We can express that block b1 is clear in the initial situation S0 by writing either

clear(b1, S0) or

(1.13)

Holds(clear(b1), S0).

(1.14)

In (1.13), clear is a binary predicate constant; in (1.14), clear is a unary function constant. Formula (1.14) is written in the version of the situation calculus in which (relational) ﬂuents are reiﬁed; ﬂuent is the ﬁrst argument sort of the predicate constant Holds. The version of the situation calculus introduced in Section 16.1 is the more expressive version, with reiﬁed ﬂuents. Expression (1.13) is viewed there as shorthand for (1.14).

Explicit deﬁnitions in ﬁrst-order logic
Let Γ be a set of sentences of a signature σ . To extend Γ by an explicit deﬁnition of a predicate constant means to add to σ a new predicate constant P of some arity n, and to add to Γ a sentence of the form
∀v1 . . . vn(P (v1, . . . , vn) ↔ F ),

14

1. Knowledge Representation and Classical Logic

where v1, . . . , vn are distinct variables and F is a formula of the signature σ . About the effect of such an extension we can say the same as about the effect of adding an explicit deﬁnition to a set of propositional formulas (Section 1.2.1): there is an obvious one-to-one correspondence between the models of the original knowledge base and the models of the extended knowledge base.
With function constants, the situation is a little more complex. To extend a set Γ of sentences of a signature σ by an explicit deﬁnition of a function constant means to add to σ a new function constant f , and to add to Γ a sentence of the form
∀v1 . . . vnv(f (v1, . . . , vn) = v ↔ F ),
where v1, . . . , vn, v are distinct variables and F is a formula of the signature σ such that Γ entails the sentence
∀v1 . . . vn∃!vF.
The last assumption is essential: if it does not hold then adding a function constant along with the corresponding axiom would eliminate some of the models of Γ .
For instance, if Γ entails (1.12) then we can extend Γ by the explicit deﬁnition of the function constant location:
∀xp(location(x) = p ↔ in(x, p)).

Natural deduction with quantiﬁers and equality

The natural deduction system for ﬁrst-order logic includes all axiom schemas and inference rules shown in Section 1.2.1 and a few additional postulates. First, we add the introduction and elimination rules for quantiﬁers:

(∀I )

Γ ⇒ F (v) Γ ⇒ ∀vF (v)

(∀E)

Γ ⇒ ∀vF (v) Γ ⇒ F (t)

where v is not a free variable where t is substitutable

of any formula in Γ

for v in F (v)

(∃I )

Γ ⇒ F (t) Γ ⇒ ∃vF (v)

(∃E) Γ ⇒ ∃vF (v) Γ,

, F (v) ⇒ G ⇒G

where t is substitutable for v in F (v)

where v is not a free variable of any formula in , G

Second, postulates for equality are added: the axiom schema expressing its reﬂexivity

⇒t =t

and the inference rules for replacing equals by equals:

(Repl)

Γ

⇒ t1 = t2 ⇒ F (t1) Γ, ⇒ F (t2)

Γ ⇒ t1 = t2 ⇒ F (t2) Γ, ⇒ F (t1)

where t1 and t2 are terms substitutable for v in F (v). This formal system is sound and complete: for any ﬁnite set Γ of sentences and any
formula F , the sequent Γ ⇒ F is provable if and only if Γ |= F . The completeness

of (a different formalization of) ﬁrst-order logic was proved by Gödel [100].

V. Lifschitz, L. Morgenstern, D. Plaisted

15

1.

(1.9) ⇒ (1.9)

2.

¬in(x) ⇒ ¬in(x)

3.

x=P ⇒ x=P

4.

x = P , ¬in(x) ⇒ ¬in(P )

5.

x = P , ¬in(x) ⇒ ¬in(P ) ∨ ¬in(Q)

6.

x = P , ¬in(x) ⇒ (1.8)

7.

x=Q ⇒ x=Q

8.

x = Q, ¬in(x) ⇒ ¬in(Q)

9.

x = Q, ¬in(x) ⇒ ¬in(P ) ∨ ¬in(Q)

10.

x = Q, ¬in(x) ⇒ (1.8)

11.

x =P ∨ x =Q ⇒ x =P ∨x =Q

12. x = P ∨ x = Q, ¬in(x) ⇒ (1.8)

13.

x=R ⇒ x=R

14.

x = R, ¬in(x) ⇒ ¬in(R)

15.

x = R, ¬in(x) ⇒ (1.8)

16.

(1.10) ⇒ (1.10)

17.

(1.10) ⇒ x = P ∨ x = Q

∨x =R

18.

(1.10), ¬in(x) ⇒ (1.8)

19.

(1.9), (1.10) ⇒ (1.8)

— axiom — axiom — axiom — by Repl from 3, 2 — by (∨I ) from 4 — by (∨I ) from 5 — axiom — by Repl from 7, 2 — by (∨I ) from 8 — by (∨I ) from 9 — axiom — by (∨E) from 11, 6, 10 — axiom — by Repl from 13, 2 — by (∨I ) from 14 — axiom
— by (∀E) from 16 — by (∨E) from 17, 12, 15 — by (∃E) from 1, 18

Figure 1.3: A proof in ﬁrst-order logic.

As in the propositional case (Section 1.2.1), the soundness theorem justiﬁes establishing entailment in ﬁrst-order logic by an object-level argument. For instance, we can prove the claim that (1.8) is entailed by (1.9) and (1.10) as follows: take x such that ¬in(x) and consider the three cases corresponding to the disjunctive terms of (1.10); in each case, one of the disjunctive terms of (1.8) follows. This argument is an informal summary of the proof shown in Fig. 1.3, with the names Paul, Quentin, Robert replaced by P , Q, R.
Since proofs in the deductive system described above can be effectively enumerated, from the soundness and completeness of the system we can conclude that the set of logically valid sentences is recursively enumerable. But it is not recursive [56], even if the underlying signature consists of a single binary predicate constant, and even if we disregard formulas containing equality [135].
As discussed in Section 3.3.1, most descriptions logics can be viewed as decidable fragments of ﬁrst-order logic.
Limitations of ﬁrst-order logic
The sentence
∀xy(Q(x, y) ↔ P (y, x))
expresses that Q is the inverse of P . Does there exist a ﬁrst-order sentence expressing that Q is the transitive closure of P ? To be more precise, does there exist a sentence F of the signature {P , Q} such that an interpretation I of this signature satisﬁes F if and only if QI is the transitive closure of P I ?
The answer to this question is no. From the perspective of knowledge representation, this is an essential limitation, because the concept of transitive closure is the

16

1. Knowledge Representation and Classical Logic

mathematical counterpart of the important commonsense idea of reachability. As discussed in Section 1.2.3 below, one way to overcome this limitation is to turn to second-order logic.
Another example illustrating the usefulness of second-order logic in knowledge representation is related to the idea of domain closure (Section 1.2.2). If the underlying signature contains the object constants c1, . . . , cn and no function constants of arity > 0 then sentence (1.11) expresses the domain closure assumption: an interpretation I satisﬁes (1.11) if and only if
|I | = {c1I , . . . , cnI }.
Consider now the signature consisting of the object constant c and the unary function constant f . Does there exist a ﬁrst-order sentence expressing the domain closure assumption for this signature? To be precise, we would like to ﬁnd a sentence F such that an interpretation I satisﬁes F if and only if
|I | = {cI , f (c)I , f (f (c))I , . . .}.
There is no ﬁrst-order sentence with this property. Similarly, ﬁrst-order languages do not allow us to state Reiter’s foundational axiom
expressing that each situation is the result of performing a sequence of actions in the initial situation ([225, Section 4.2.2]; see also Section 16.3 below).

1.2.3 Second-Order Logic
Syntax and semantics
In second-order logic, the deﬁnition of a signature remains the same (Section 1.2.2). But its syntax is richer, because, along with object variables, we assume now an inﬁnite sequence of function variables of arity n for each n > 0, and an inﬁnite sequence of predicate variables of arity n for each n 0. Object variables are viewed as function variables of arity 0.
Function variables can be used to form new terms in the same way as function constants. For instance, if α is a unary function variable and c is an object constant then α(c) is a term. Predicate variables can be used to form atomic formulas in the same way as predicate constants. In non-atomic formulas, function and predicate variables can be bound by quantiﬁers in the same way as object variables. For instance,
∀αβ∃γ ∀x(γ (x) = α(β(x)))
is a sentence expressing the possibility of composing any two functions. (When we say that a second-order formula is a sentence, we mean that all occurrences of all variables in it are bound, including function and predicate variables.)
Note that α = β is not an atomic formula, because unary function variables are not terms. But this expression can be viewed as shorthand for the formula
∀x(α(x) = β(x)).
Similarly, the expression p = q, where p and q are unary predicate variables, can be viewed as shorthand for
∀x(p(x) ↔ q(x)).

V. Lifschitz, L. Morgenstern, D. Plaisted

17

The condition “Q is the transitive closure of P ” can be expressed by the secondorder sentence

∀xy(Q(x, y) ↔ ∀q(F (q) → q(x, y))), where F (q) stands for

(1.15)

∀x1y1(P (x1, y1) → q(x1, y1))
∧ ∀x1y1z1((q(x1, y1) ∧ q(y1, z1)) → q(x1, z1))
(Q is the intersection of all transitive relations containing P ). The domain closure assumption for the signature {c, f } can be expressed by the
sentence

∀p(G(p) → ∀x p(x)), where G(p) stands for

(1.16)

p(c) ∧ ∀x(p(x) → p(f (x)))
(any set that contains c and is closed under f covers the whole universe). The deﬁnition of an interpretation remains the same (Section 1.2.2). The semantics
of second-order logic deﬁnes, for each sentence F and each interpretation I , the corresponding truth value F I . In the clauses for quantiﬁers, whenever a quantiﬁer binds a function variable, names of arbitrary functions from |I |n to I are substituted for it; when a quantiﬁer binds a predicate variable, names of arbitrary functions from |I |n to {FALSE, TRUE} are substituted.
Quantiﬁers binding a propositional variable p can be always eliminated: ∀pF (p) is equivalent to F (⊥) ∧ F ( ), and ∃pF (p) is equivalent to F (⊥) ∨ F ( ). In the special case when the underlying signature consists of propositional constants, secondorder formulas (in prenex form) are known as quantiﬁed Boolean formulas (see Section 2.5.1). The equivalences above allow us to rewrite any such formula in the syntax of propositional logic. But a sentence containing predicate variables of arity > 0 may not be equivalent to any ﬁrst-order sentence; (1.15) and (1.16) are examples of such “hard” cases.

Object-level proofs in second-order logic
In this section we consider a deductive system for second-order logic that contains all postulates from Sections 1.2.1 and 1.2.2; in rules (∀E) and (∃I ), if v is a function variable of arity > 0 then t is assumed to be a function variable of the same arity, and similarly for predicate variables. In addition, we include two axiom schemas asserting the existence of predicates and functions. One is the axiom schema of comprehension

⇒ ∃p∀v1 . . . vn(p(v1, . . . , vn) ↔ F ),
where v1, . . . , vn are distinct object variables, and p is not free in F . (Recall that ↔ is not allowed in sequents, but we treat F ↔ G as shorthand for (F → G) ∧ (G → F ).)

18

1. Knowledge Representation and Classical Logic

1.

F ⇒F

— axiom

2.

F ⇒ p(x) → p(y)

— by (∀E) from 1

3.

⇒ ∃p∀z(p(z) ↔ x = z) — axiom (comprehension)

4. ∀z(p(z) ↔ x = z) ⇒ ∀z(p(z) ↔ x = z) — axiom

5. ∀z(p(z) ↔ x = z) ⇒ p(x) ↔ x = x

— by (∀E) from 4

6. ∀z(p(z) ↔ x = z) ⇒ x = x → p(x)

— by (∧E) from 5

7.

⇒ x=x

— axiom

8. ∀z(p(z) ↔ x = z) ⇒ p(x)

— by (→ E) from 7, 6

9. F, ∀z(p(z) ↔ x = z) ⇒ p(y)

— by (→ E) from 8, 2

10. ∀z(p(z) ↔ x = z) ⇒ p(y) ↔ x = y

— by (∀E) from 4

11. ∀z(p(z) ↔ x = z) ⇒ p(y) → x = y

— by (∧E) from 10

12. F, ∀z(p(z) ↔ x = z) ⇒ x = y

— by (→ E) from 9, 11

13.

F ⇒ x=y

— by (∃E) from 1, 12

14.

⇒ F →x=y

— by (→ I ) from 13

Figure 1.4: A proof in second-order logic. F stands for ∀p(p(x) → p(y)).

The other is the axioms of choice
⇒ ∀v1 . . . vn∃vn+1p(v1, . . . , vn+1)
→ ∃α∀v1 . . . vn(p(v1, . . . , vn, α(v1, . . . , vn)),
where v1, . . . , vn+1 are distinct object variables. This deductive system is sound but incomplete. Adding any sound axioms or infer-
ence rules would not make it complete, because the set of logically valid second-order sentences is not recursively enumerable.
As in the case of ﬁrst-order logic, the availability of a sound deductive system allows us to establish second-order entailment by object-level reasoning. To illustrate this point, consider the formula
∀p(p(x) → p(y)) → x = y,
which can be thought of as a formalization of “Leibniz’s principle of equality”: two objects are equal if they share the same properties. Its logical validity can be justiﬁed as follows. Assume ∀p(p(x) → p(y)), and take p to be the property of being equal to x. Clearly x has this property; consequently y has this property as well, that is, x = y. This argument is an informal summary of the proof shown in Fig. 1.4.

1.3 Automated Theorem Proving
Automated theorem proving is the study of techniques for programming computers to search for proofs of formal assertions, either fully automatically or with varying degrees of human guidance. This area has potential applications to hardware and software veriﬁcation, expert systems, planning, mathematics research, and education.
Given a set A of axioms and a logical consequence B, a theorem proving program should, ideally, eventually construct a proof of B from A. If B is not a consequence of A, the program may run forever without coming to any deﬁnite conclusion. This is the best one can hope for, in general, in many logics, and indeed even this is not always possible. In principle, theorem proving programs can be written just by enumerating

V. Lifschitz, L. Morgenstern, D. Plaisted

19

all possible proofs and stopping when a proof of the desired statement is found, but this approach is so inefﬁcient as to be useless. Much more powerful methods have been developed.
History of theorem proving
Despite the potential advantages of machine theorem proving, it was difﬁcult initially to obtain any kind of respectable performance from machines on theorem proving problems. Some of the earliest automatic theorem proving methods, such as those of Gilmore [99], Prawitz [217], and Davis and Putnam [70] were based on Herbrand’s theorem, which gives an enumeration process for testing if a theorem of ﬁrst-order logic is true. Davis and Putnam used Skolem functions and conjunctive normal form clauses, and generated elements of the Herbrand universe exhaustively, while Prawitz showed how this enumeration could be guided to only generate terms likely to be useful for the proof, but did not use Skolem functions or clause form. Later Davis [66] showed how to realize this same idea in the context of clause form and Skolem functions. However, these approaches turned out to be too inefﬁcient. The resolution approach of Robinson [229, 230] was developed in about 1963, and led to a signiﬁcant advance in ﬁrst-order theorem provers. This approach, like that of Davis and Putnam [70], used clause form and Skolem functions, but made use of a uniﬁcation algorithm to ﬁnd the terms most likely to lead to a proof. Robinson also used the resolution inference rule which in itself is all that is needed for theorem proving in ﬁrst-order logic. The theorem proving group at Argonne, Illinois took the lead in implementing resolution theorem provers, with some initial success on group theory problems that had been intractable before. They were even able to solve some previously open problems using resolution theorem provers. For a discussion of the early history of mechanical theorem proving, see [67].
About the same time, Maslov [168] developed the inverse method which has been less widely known than resolution in the West. This method was originally deﬁned for classical ﬁrst-order logic without function symbols and equality, and for formulas having a quantiﬁer preﬁx followed by a disjunction of conjunctions of clauses. Later the method was extended to formulas with function symbols. This method was used not only for theorem proving but also to show the decidability of some classes of ﬁrstorder formulas. In the inverse method, substitutions were originally represented as sets of equations, and there appears to have been some analogue of most general uniﬁers. The method was implemented for classical ﬁrst-order logic by 1968. The inverse method is based on forward reasoning to derive a formula. In terms of implementation, it is competitive with resolution, and in fact can be simulated by resolution with the introduction of new predicate symbols to deﬁne subformulas of the original formula. For a readable exposition of the inverse method, see [159]. For many extensions of the method, see [71].
In the West, the initial successes of resolution led to a rush of enthusiasm, as resolution theorem provers were applied to question-answering problems, situation calculus problems, and many others. It was soon discovered that resolution had serious inefﬁciencies, and a long series of reﬁnements were developed to attempt to overcome them. These included the unit preference rule, the set of support strategy, hyper-resolution, paramodulation for equality, and a nearly innumerable list of other reﬁnements. The initial enthusiasm for resolution, and for automated deduction in general, soon wore

20

1. Knowledge Representation and Classical Logic

off. This reaction led, for example, to the development of specialized decision procedures for proving theorems in certain theories [190, 191] and the development of expert systems.
However, resolution and similar approaches continued to be developed. Data structures were developed permitting the resolution operation to be implemented much more efﬁciently, which were eventually greatly reﬁned [222] as in the Vampire prover [227]. One of the ﬁrst provers to employ such techniques was Stickel’s Prolog Technology Theorem Prover [252]. Techniques for parallel implementations of provers were also eventually considered [34]. Other strategies besides resolution were developed, such as model elimination [162], which led eventually to logic programming and Prolog, the matings method for higher-order logic [3], and Bibel’s connection method [28]. Though these methods are not resolution based, they did preserve some of the key concepts of resolution, namely, the use of uniﬁcation and the combination of uniﬁcation with inference in clause form ﬁrst-order logic. Two other techniques used to improve the performance of provers, especially in competitions [253], are strategy selection and strategy scheduling. Strategy selection means that different theorem proving strategies and different settings of the coefﬁcients are used for different kinds of problems. Strategy scheduling means that even for a given kind of problem, many strategies are used, one after another, and a speciﬁed amount of time is allotted to each one. Between the two of these approaches, there is considerable freedom for imposing an outer level of control on the theorem prover to tailor its performance to a given problem set.
Some other provers dealt with higher-order logic, such as the TPS prover of Andrews and others [4, 5] and the interactive NqTHM and ACL2 provers of Boyer, Moore, and Kaufmann [142, 141] for proofs by mathematical induction. Today, a variety of approaches including formal methods and theorem proving seem to be accepted as part of the standard AI tool kit.
Despite early difﬁculties, the power of theorem provers has continued to increase. Notable in this respect is Otter [177], which is widely distributed, and coded in C with very efﬁcient data structures. Prover9 is a more recent prover of W. McCune in the same style, and is a successor of Otter. The increasing speed of hardware has also signiﬁcantly aided theorem provers. An impetus was given to theorem proving research by McCune’s solution of the Robbins problem [176] by a ﬁrst-order equational theorem prover derived from Otter. The Robbins problem is a ﬁrst-order theorem involving equality that had been known to mathematicians for decades but which no one was able to solve. McCune’s prover was able to ﬁnd a proof after about a week of computation. Many other proofs have also been found by McCune’s group on various provers; see for example the web page http://www.cs.unm.edu/~veroff/MEDIAN_ALGEBRA/. Now substantial theorems in mathematics whose correctness is in doubt can be checked by interactive theorem provers [196].
First-order theorem provers vary in their user interfaces, but most of them permit formulas to be entered in clause form in a reasonable syntax. Some provers also permit the user to enter ﬁrst-order formulas; these provers generally provide various ways of translating such formulas to clause form. Some provers require substantial user guidance, though most such provers have higher-order features, while other provers are designed to be more automatic. For automatic provers, there are often many different ﬂags that can be set to guide the search. For example, typical ﬁrst-order provers

V. Lifschitz, L. Morgenstern, D. Plaisted

21

allow the user to select from among a number of inference strategies for ﬁrst-order logic as well as strategies for equality. For equality, it may be possible to specify a termination ordering to guide the application of equations. Sometimes the user will select incomplete strategies, hoping that the desired proof will be found faster. It is also often possible to set a size bound so that all clauses or literals larger than a certain size are deleted. Of course one does not know in advance what bound to choose, so some experimentation is necessary. A sliding priority approach to setting the size bound automatically was presented in [211]. It is sometimes possible to assign various weights to various symbols or subterms or to variables to guide the proof search. Modern provers generally have term indexing [222] built in to speed up inference, and also have some equality strategy involving ordered paramodulation and rewriting. Many provers are based on resolution, but some are based on model elimination and some are based on propositional approaches. Provers can generate clauses rapidly; for example, Vampire [227] can often generate more than 40,000 clauses per second. Most provers rapidly ﬁll up memory with generated clauses, so that if a proof is not found in a few minutes it will not be found at all. However, equational proofs involve considerable simpliﬁcation and can sometimes run for a long time without exhausting memory. For example, the Robbins problem ran for 8 days on a SPARC 5 class UNIX computer with a size bound of 70 and required about 30 megabytes of memory, generating 49,548 equations, most of which were deleted by simpliﬁcation. Sometimes small problems can run for a long time without ﬁnding a proof, and sometimes problems with a hundred or more input clauses can result in proofs fairly quickly. Generally, simple problems will be proved by nearly any complete strategy on a modern prover, but hard problems may require ﬁne tuning. For an overview of a list of problems and information about how well various provers perform on them, see the web site at www.tptp.org, and for a sketch of some of the main ﬁrst-order provers in use today, see http://www.cs.miami.edu/~tptp/CASC/ as well as the journal articles devoted to the individual competitions such as [253, 254]. Current provers often do not have facilities for interacting with other reasoning programs, but work in this area is progressing.
In addition to developing ﬁrst-order provers, there has been work on other logics, too. The simplest logic typically considered is propositional logic, in which there are only predicate symbols (that is, Boolean variables) and logical connectives. Despite its simplicity, propositional logic has surprisingly many applications, such as in hardware veriﬁcation and constraint satisfaction problems. Propositional provers have even found applications in planning. The general validity (respectively, satisﬁability) problem of propositional logic is NP-hard, which means that it does not in all likelihood have an efﬁcient general solution. Nevertheless, there are propositional provers that are surprisingly efﬁcient, and becoming increasingly more so; see Chapter 2 of this Handbook for details.
Binary decision diagrams [43] are a particular form of propositional formulas for which efﬁcient provers exist. BDD’s are used in hardware veriﬁcation, and initiated a tremendous surge of interest by industry in formal veriﬁcation techniques. Also, the Davis–Putnam–Logemann–Loveland method [69] for propositional logic is heavily used in industry for hardware veriﬁcation.
Another restricted logic for which efﬁcient provers exist is that of temporal logic, the logic of time (see Chapter 12 of this Handbook). This has applications to con-

22

1. Knowledge Representation and Classical Logic

currency. The model-checking approach of Clarke and others [48] has proven to be particularly efﬁcient in this area, and has also stimulated considerable interest by industry.
Other logical systems for which provers have been developed are the theory of equational systems, for which term-rewriting techniques lead to remarkably efﬁcient theorem provers, mathematical induction, geometry theorem proving, constraints (Chapter 4 of this Handbook), higher-order logic, and set theory.
Not only proving theorems, but ﬁnding counterexamples, or building models, is of increasing importance. This permits one to detect when a theorem is not provable, and thus one need not waste time attempting to ﬁnd a proof. This is, of course, an activity which human mathematicians often engage in. These counterexamples are typically ﬁnite structures. For the so-called ﬁnitely controllable theories, running a theorem prover and a counterexample (model) ﬁnder together yields a decision procedure, which theoretically can have practical applications to such theories. Model ﬁnding has recently been extended to larger classes of theories [51].
Among the current applications of theorem provers one can list hardware veriﬁcation and program veriﬁcation. For a more detailed survey, see the excellent report by Loveland [164]. Among potential applications of theorem provers are planning problems, the situation calculus, and problems involving knowledge and belief.
There are a number of provers in prominence today, including Otter [177], the provers of Boyer, Moore, and Kaufmann [142, 141], Andrew’s matings prover [3], the HOL prover [101], Isabelle [203], Mizar [260], NuPrl [62], PVS [201], and many more. Many of these require substantial human guidance to ﬁnd proofs. The Omega system [240] is a higher order logic proof development system that attempts to overcome some of the shortcomings of traditional ﬁrst-order proof systems. In the past it has used a natural deduction calculus to develop proofs with human guidance, though the system is changing.
Provers can be evaluated on a number of grounds. One is completeness; can they, in principle, provide a proof of every true theorem? Another evaluation criterion is their performance on speciﬁc examples; in this regard, the TPTP problem set [255] is of particular value. Finally, one can attempt to provide an analytic estimate of the efﬁciency of a theorem prover on classes of problems [212]. This gives a measure which is to a large extent independent of particular problems or machines. The Handbook of Automated Reasoning [231] is a good source of information about many areas of theorem proving.
We next discuss resolution for the propositional calculus and then some of the many ﬁrst-order theorem proving methods, with particular attention to resolution. We also consider techniques for ﬁrst-order logic with equality. Finally, we brieﬂy discuss some other logics, and corresponding theorem proving techniques.

1.3.1 Resolution in the Propositional Calculus
The main problem for theorem proving purposes is given a formula A, to determine whether it is valid. Since A is valid iff ¬A is unsatisﬁable, it is possible to determine validity if one can determine satisﬁability. Many theorem provers test satisﬁability instead of validity.
The problem of determining whether a Boolean formula A is satisﬁable is one of the NP-complete problems. This means that the fastest algorithms known require an

V. Lifschitz, L. Morgenstern, D. Plaisted

23

amount of time that is asymptotically exponential in the size of A. Also, it is not likely
that faster algorithms will be found, although no one can prove that they do not exist.
Despite this negative result, there is a wide variety of methods in use for testing
if a formula is satisﬁable. One of the simplest is truth tables. For a formula A over {P1, P2, . . . , Pn}, this involves testing for each of the 2n valuations I over {P1, P2, . . . , Pn} whether I |= A. In general, this will require time at least proportional to 2n to show that A is valid, but may detect satisﬁability sooner.

Clause form
Many of the other satisﬁability checking algorithms depend on conversion of a formula A to clause form. This is deﬁned as follows: An atom is a proposition. A literal is an atom or an atom preceded by a negation sign. The two literals P and ¬P are said to be complementary to each other. A clause is a disjunction of literals. A formula is in clause form if it is a conjunction of clauses. Thus the formula

(P ∨ ¬R) ∧ (¬P ∨ Q ∨ R) ∧ (¬Q ∨ ¬R)
is in clause form. This is also known as conjunctive normal form. We represent clauses by sets of literals and clause form formulas by sets of clauses, so that the above formula would be represented by the following set of sets:

{{P , ¬R}, {¬P , Q, R}, {¬Q, ¬R}}.
A unit clause is a clause that contains only one literal. The empty clause { } is understood to represent FALSE.
It is straightforward to show that for every formula A there is an equivalent formula B in clause form. Furthermore, there are well-known algorithms for converting any formula A into such an equivalent formula B. These involve converting all connectives to ∧, ∨, and ¬, pushing ¬ to the bottom, and bringing ∧ to the top. Unfortunately, this process of conversion can take exponential time and can increase the length of the formula by an exponential amount.
The exponential increase in size in converting to clause form can be avoided by adding extra propositions representing subformulas of the given formula. For example, given the formula

(P1 ∧ Q1) ∨ (P2 ∧ Q2) ∨ (P3 ∧ Q3) ∨ · · · ∨ (Pn ∧ Qn)
a straightforward conversion to clause form creates 2n clauses of length n, for a formula of length at least n2n. However, by adding the new propositions Ri which are deﬁned as Pi ∧ Qi, one obtains the new formula
(R1 ∨ R2 ∨ · · · ∨ Rn) ∧ ((P1 ∧ Q1) ↔ R1) ∧ · · · ∧ ((Pn ∧ Qn) ↔ Rn).
When this formula is converted to clause form, a much smaller set of clauses results, and the exponential size increase does not occur. The same technique works for any Boolean formula. This transformation is satisﬁability preserving but not equivalence preserving, which is enough for theorem proving purposes.

24

1. Knowledge Representation and Classical Logic

Ground resolution
Many ﬁrst-order theorem provers are based on resolution, and there is a propositional analogue of resolution called ground resolution, which we now present as an introduction to ﬁrst-order resolution. Although resolution is reasonably efﬁcient for ﬁrst-order logic, it turns out that ground resolution is generally much less efﬁcient than Davis and Putnam-like procedures for propositional logic [70, 69], often referred to as DPLL procedures because the original Davis and Putnam procedure had some inefﬁciencies. These DPLL procedures are specialized to clause form and explore the set of possible interpretations of a propositional formula by depth-ﬁrst search and backtracking with some additional simpliﬁcation rules for unit clauses.
Ground resolution is a decision procedure for propositional formulas in clause form. If C1 and C2 are two clauses, and L1 ∈ C1 and L2 ∈ C2 are complementary literals, then
(C1 − {L1}) ∪ (C2 − {L2})
is called a resolvent of C1 and C2, where the set difference of two sets A and B is indicated by A − B, that is, {x: x ∈ A, x ∈/ B}. There may be more than one resolvent of two clauses, or maybe none. It is straightforward to show that a resolvent D of two clauses C1 and C2 is a logical consequence of C1 ∧ C2.
For example, if C1 is {¬P , Q} and C2 is {¬Q, R}, then one can choose L1 to be Q and L2 to be ¬Q. Then the resolvent is {¬P , R}. Note also that R is a resolvent of {Q} and {¬Q, R}, and { } (the empty clause) is a resolvent of {Q} and {¬Q}.
A resolution proof of a clause C from a set S of clauses is a sequence C1, C2, . . . , Cn of clauses in which each Ci is either a member of S or a resolvent of Cj and Ck, for j, k less than i, and Cn is C. Such a proof is called a (resolution) refutation if Cn is { }. Resolution is complete:
Theorem 1.3.1. Suppose S is a set of propositional clauses. Then S is unsatisﬁable iff there exists a resolution refutation from S.

As an example, let S be the set of clauses

{{P }, {¬P , Q}, {¬Q}}.

The following is a resolution refutation from S, listing with each resolvent the two clauses that are resolved together:

1. P

given

2. ¬P , Q given

3. ¬Q given

4. Q

1, 2, resolution

5. { }

3, 4, resolution

(Here set braces are omitted, except for the empty clause.) This is a resolution refuta-
tion from S, so S is unsatisﬁable. Deﬁne R(S) to be C1,C2∈S resolvents(C1, C2). Deﬁne R1(S) to be R(S) and
Ri+1(S) to be R(S ∪ Ri(S)), for i > 1. Typical resolution theorem provers essen-
tially generate all of the resolution proofs from S (with some improvements that will

V. Lifschitz, L. Morgenstern, D. Plaisted

25

be discussed later), looking for a proof of the empty clause. Formally, such provers generate R1(S), R2(S), R3(S), and so on, until for some i, Ri(S) = Ri+1(S), or the empty clause is generated. In the former case, S is satisﬁable. If the empty clause is generated, S is unsatisﬁable.
Even though DPLL essentially constructs a resolution proof, propositional resolution is much less efﬁcient than DPLL as a decision procedure for satisﬁability of formulas in the propositional calculus because the total number of resolutions performed by a propositional resolution prover in the search for a proof is typically much larger than for DPLL. Also, Haken [107] showed that there are unsatisﬁable sets S of propositional clauses for which the length of the shortest resolution refutation is exponential in the size (number of clauses) in S. Despite these inefﬁciencies, we introduced propositional resolution as a way to lead up to ﬁrst-order resolution, which has signiﬁcant advantages. In order to extend resolution to ﬁrst-order logic, it is necessary to add uniﬁcation to it.
1.3.2 First-Order Proof Systems
We now discuss methods for partially deciding validity. These construct proofs of ﬁrst-order formulas, and a formula is valid iff it can be proven in such a system. Thus there are complete proof systems for ﬁrst-order logic, and Gödel’s incompleteness theorem does not apply to ﬁrst-order logic. Since the set of proofs is countable, one can partially decide validity of a formula A by enumerating the set of proofs, and stopping whenever a proof of A is found. This already gives us a theorem prover, but provers constructed in this way are typically very inefﬁcient.
There are a number of classical proof systems for ﬁrst-order logic: Hilbert-style systems, Gentzen-style systems, natural deduction systems, semantic tableau systems, and others [87]. Since these generally have not found much application to automated deduction, except for semantic tableau systems, they are not discussed here. Typically they specify inference rules of the form
A1, A2, . . . , An
A which means that if one has already derived the formulas A1, A2, . . . , An, then one can also infer A. Using such rules, one builds up a proof as a sequence of formulas, and if a formula B appears in such a sequence, one has proved B.
We now discuss proof systems that have found application to automated deduction. In the following sections, the letters f, g, h, . . . will be used as function symbols, a, b, c, . . . as individual constants, x, y, z and possibly other letters as individual variables, and = as the equality symbol. Each function symbol has an arity, which is a non-negative integer telling how many arguments it takes. A term is either a variable, an individual constant, or an expression of the form f (t1, t2, . . . , tn) where f is a function symbol of arity n and the ti are terms. The letters r, s, t, . . . will denote terms.
Clause form
Many ﬁrst-order theorem provers convert a ﬁrst-order formula to clause form before attempting to prove it. The beauty of clause form is that it makes the syntax of ﬁrstorder logic, already quite simple, even simpler. Quantiﬁers are omitted, and Boolean

26

1. Knowledge Representation and Classical Logic

connectives as well. One has in the end just sets of sets of literals. It is amazing that the expressive power of ﬁrst-order logic can be reduced to such a simple form. This simplicity also makes clause form suitable for machine implementation of theorem provers. Not only that, but the validity problem is also simpliﬁed in a theoretical sense; one only needs to consider the Herbrand interpretations, so the question of validity becomes easier to analyze.
Any ﬁrst-order formula A can be transformed to a clause form formula B such that A is satisﬁable iff B is satisﬁable. The translation is not validity preserving. So in order to show that A is valid, one translates ¬A to clause form B and shows that B is unsatisﬁable. For convenience, assume that A is a sentence, that is, it has no free variables.
The translation of a ﬁrst-order sentence A to clause form has several steps:
• Push negations in.
• Replace existentially quantiﬁed variables by Skolem functions.
• Move universal quantiﬁers to the front.
• Convert the matrix of the formula to conjunctive normal form.
• Remove universal quantiﬁers and Boolean connectives.
This transformation will be presented as a set of rewrite rules. A rewrite rule X → Y means that a subformula of the form X is replaced by a subformula of the form Y .
The following rewrite rules push negations in:
(A ↔ B) → (A → B) ∧ (B → A),
(A → B) → ((¬A) ∨ B),
¬¬A → A,
¬(A ∧ B) → (¬A) ∨ (¬B),
¬(A ∨ B) → (¬A) ∧ (¬B),
¬∀xA → ∃x(¬A),
¬∃xA → ∀x(¬A).
After negations have been pushed in, we assume for simplicity that variables in the formula are renamed so that each variable appears in only one quantiﬁer. Existential quantiﬁers are then eliminated by replacing formulas of the form ∃xA[x] by A[f (x1, . . . , xn)], where x1, . . . , xn are all the universally quantiﬁed variables whose scope includes the formula A, and f is a new function symbol (that does not already appear in the formula), called a Skolem function.
The following rules then move quantiﬁers to the front:
(∀xA) ∨ B → ∀x(A ∨ B),
B ∨ (∀xA) → ∀x(B ∨ A),
(∀xA) ∧ B → ∀x(A ∧ B),
B ∧ (∀xA) → ∀x(B ∧ A).

V. Lifschitz, L. Morgenstern, D. Plaisted

27

Next, the matrix is converted to conjunctive normal form by the following rules:
(A ∨ (B ∧ C)) → (A ∨ B) ∧ (A ∨ C), ((B ∧ C) ∨ A) → (B ∨ A) ∧ (C ∨ A). Finally, universal quantiﬁers are removed from the front of the formula and a conjunctive normal form formula of the form
(A1 ∨ A2 ∨ · · · ∨ Ak) ∧ (B1 ∨ B2 ∨ · · · ∨ Bm) ∧ · · · ∧ (C1 ∨ C2 ∨ · · · ∨ Cn) is replaced by the set of sets of literals
{{A1, A2, . . . , Ak}, {B1, B2, . . . , Bm}, . . . , {C1, C2, . . . , Cn}}. This last formula is the clause form formula which is satisﬁable iff the original formula is.
As an example, consider the formula
¬∃x(P (x) → ∀yQ(x, y)). First, negation is pushed past the existential quantiﬁer:
∀x(¬(P (x) → ∀yQ(x, y))). Next, negation is further pushed in, which involves replacing → by its deﬁnition as follows:
∀x¬((¬P (x)) ∨ ∀yQ(x, y)). Then ¬ is moved in past ∨:
∀x((¬¬P (x)) ∧ ¬∀yQ(x, y)). Next the double negation is eliminated and ¬ is moved past the quantiﬁer:
∀x(P (x) ∧ ∃y¬Q(x, y)). Now, negations have been pushed in. Note that no variable appears in more than one quantiﬁer, so it is not necessary to rename variables. Next, the existential quantiﬁer is replaced by a Skolem function:
∀x(P (x) ∧ ¬Q(x, f (x))). There are no quantiﬁers to move to the front. Eliminating the universal quantiﬁer yields the formula
P (x) ∧ ¬Q(x, f (x)). The clause form is then
{{P (x)}, {¬Q(x, f (x))}}. Recall that if B is the clause form of A, then B is satisﬁable iff A is. As in propositional calculus, the clause form translation can increase the size of a formula by an exponential amount. This can be avoided as in the propositional calculus by

28

1. Knowledge Representation and Classical Logic

introducing new predicate symbols for sub-formulas. Suppose A is a formula with sub-formula B, denoted by A[B]. Let x1, x2, . . . , xn be the free variables in B. Let P be a new predicate symbol (that does not appear in A). Then A[B] is transformed to the formula A[P (x1, x2, . . . , xn)] ∧ ∀x1∀x2 . . . ∀xn(P (x1, x2, . . . , xn) ↔ B). Thus the occurrence of B in A is replaced by P (x1, x2, . . . , xn), and the equivalence of B with P (x1, x2, . . . , xn) is added on to the formula as well. This transformation can be applied to the new formula in turn, and again as many times as desired. The transformation is satisﬁability preserving, which means that the resulting formula is satisﬁable iff the original formula A was.
Free variables in a clause are assumed to be universally quantiﬁed. Thus the clause {¬P (x), Q(f (x))} represents the formula ∀x(¬P (x) ∨ Q(f (x))). A term, literal, or clause not containing any variables is said to be ground.
A set of clauses represents the conjunction of the clauses in the set. Thus the set {{¬P (x), Q(f (x))}, {¬Q(y), R(g(y))}, {P (a)}, {¬R(z)}} represents the formula (∀x(¬P (x) ∨ Q(f (x)))) ∧ (∀y(¬Q(y) ∨ R(g(y)))) ∧ P (a) ∧ ∀z¬R(z).
Herbrand interpretations
There is a special kind of interpretation that turns out to be signiﬁcant for mechanical theorem proving. This is called a Herbrand interpretation. Herbrand interpretations are deﬁned relative to a set S of clauses. The domain D of a Herbrand interpretation I consists of the set of terms constructed from function and constant symbols of S, with an extra constant symbol added if S has no constant symbols. The constant and function symbols are interpreted so that for any ﬁnite term t composed of these symbols, tI is the term t itself, which is an element of D. Thus if S has a unary function symbol f and a constant symbol c, then D = {c, f (c), f (f (c)), f (f (f (c))), . . .} and c is interpreted so that cI is the element c of D and f is interpreted so that f I applied to the term c yields the term f (c), f I applied to the term f (c) of D yields f (f (c)), and so on. Thus these interpretations are quite syntactic in nature. There is no restriction, however, on how a Herbrand interpretation I may interpret the predicate symbols of S.
The interest of Herbrand interpretations for theorem proving comes from the following result:

Theorem 1.3.2. If S is a set of clauses, then S is satisﬁable iff there is a Herbrand interpretation I such that I |= S.

What this theorem means is that for purposes of testing satisﬁability of clause sets, one only needs to consider Herbrand interpretations. This implicitly leads to a mechanical theorem proving procedure, which will be presented below. This procedure makes use of substitutions.
A substitution is a mapping from variables to terms which is the identity on all but ﬁnitely many variables. If L is a literal and α is a substitution, then Lα is the result of replacing all variables in L by their image under α. The application of substitutions to terms, clauses, and sets of clauses is deﬁned similarly. The expression {x1 → t1, x2 → t2, . . . , xn → tn} denotes the substitution mapping the variable xi to the term ti, for 1 i n.
For example, P (x, f (x)){x → g(y)} = P (g(y), f (g(y))).

V. Lifschitz, L. Morgenstern, D. Plaisted

29

If L is a literal and α is a substitution, then Lα is called an instance of L. Thus P (g(y), f (g(y))) is an instance of P (x, f (x)). Similar terminology applies to clauses and terms.
If S is a set of clauses, then a Herbrand set for S is an unsatisﬁable set T of ground clauses such that for every clause D in T there is a clause C in S such that D is an instance of C. If there is a Herbrand set for S, then S is unsatisﬁable.
For example, let S be the following clause set:
{{P (a)}, {¬P (x), P (f (x))}, {¬P (f (f (a)))}}.
For this set of clauses, the following is a Herbrand set:
{{P (a)}, {¬P (a), P (f (a))}, {¬P (f (a)), P (f (f (a)))}, {¬P (f (f (a)))}}.
The ground instantiation problem is the following: Given a set S of clauses, is there a Herbrand set for S?
The following result is known as Herbrand’s theorem, and follows from Theorem 1.3.2:

Theorem 1.3.3. A set S of clauses is unsatisﬁable iff there is a Herbrand set T for S.

It follows from this result that a set S of clauses is unsatisﬁable iff the ground instantiation problem for S is solvable. Thus the problem of ﬁrst-order validity has been reduced to the ground instantiation problem. This is actually quite an achievement, because the ground instantiation problem deals only with syntactic concepts such as replacing variables by terms, and with propositional unsatisﬁability, which is easily understood.
Herbrand’s theorem implies the completeness of the following theorem proving method:
Given a set S of clauses, let C1, C2, C3, . . . be an enumeration of all of the ground instances of clauses in S. This set of ground instances is countable, so it can be enumerated. Consider the following procedure Prover:
procedure Prover(S) for i = 1, 2, 3, . . . do if {C1, C2, . . . , Ci} is unsatisﬁable then return “unsatisﬁable” ﬁ od
end Prover
By Herbrand’s theorem, it follows that Prover(S) will eventually return “unsatisﬁable” iff S is unsatisﬁable. This is therefore a primitive theorem proving procedure. It is interesting that some of the earliest attempts to mechanize theorem proving [99] were based on this idea. The problem with this approach is that it enumerates many ground instances that could never appear in a proof. However, the efﬁciency of propositional decision procedures is an attractive feature of this procedure, and it may be possible to modify it to obtain an efﬁcient theorem proving procedure. And in fact, many of the theorem provers in use today are based implicitly on this procedure, and thereby on Herbrand’s theorem. The instance-based methods such as model evolution [23, 25], clause linking [153], the disconnection calculus [29, 245], and OSHL [213] are

30

1. Knowledge Representation and Classical Logic

based fairly directly on Herbrand’s theorem. These methods attempt to apply DPLLlike approaches [69] to ﬁrst-order theorem proving. Ganzinger and Korovin [93] also study the properties of instance-based methods and show how redundancy elimination and decidable fragments of ﬁrst-order logic can be incorporated into them. Korovin has continued this line of research with some later papers.

Uniﬁcation and resolution
Most mechanical theorem provers today are based on uniﬁcation, which guides the instantiation of clauses in an attempt to make the procedure Prover above more efﬁcient. The idea of uniﬁcation is to ﬁnd those instances which are in some sense the most general ones that could appear in a proof. This avoids a lot of work that results from the generation of irrelevant instances by Prover.
In the following discussion ≡ will refer to syntactic identity of terms, literals, etc. A substitution α is called a uniﬁer of literals L and M if Lα ≡ Mα. If such a substitution exists, L and M are said to be uniﬁable. A substitution α is a most general uniﬁer of L and M if for any other uniﬁer β of L and M, there is a substitution γ such that Lβ ≡ Lαγ and Mβ ≡ Mαγ .
It turns out that if two literals L and M are uniﬁable, then there is a most general uniﬁer of L and M, and such most general uniﬁers can be computed efﬁciently by a number of simple algorithms. The earliest in recent history was given by Robinson [230].
We present a simple uniﬁcation algorithm on terms which is similar to that presented by Robinson. This algorithm is worst-case exponential time, but often efﬁcient in practice. Algorithms that are more efﬁcient (and even linear time) on large terms have been devised since then [167, 202]. If s and t are two terms and α is a most general uniﬁer of s and t, then sα can be of size exponential in the sizes of s and t, so constructing sα is inherently exponential unless the proper encoding of terms is used; this entails representing repeated subterms only once. However, many symbolic computation systems still use Robinson’s original algorithm.
procedure Unify(r, s); [[ return the most general uniﬁer of terms r and s]] if r is a variable then if r ≡ s then return { } else ( if r occurs in s then return fail else return {r → s}) else if s is a variable then ( if s occurs in r then return fail else return {s → r}) else if the top-level function symbols of r and s differ or have different arities then return fail else suppose r is f (r1 . . . rn) and s is f (s1 . . . sn); return(Unify_lists([r1 . . . rn], [s1 . . . sn]))
end Unify;

V. Lifschitz, L. Morgenstern, D. Plaisted

31

procedure Unify_lists([r1 . . . rn], [s1 . . . sn]); if [r1 . . . rn] is empty then return {} else θ ← Unify(r1, t1); if θ ≡ fail then return fail ﬁ; α ← Unify_lists([r2 . . . rn]θ, [s2 . . . sn]θ ) if α ≡ fail then return fail ﬁ; return {θ ◦ α}
end Unify_lists;
For this last procedure, θ ◦ α is deﬁned as the composition of the substitutions θ and α, deﬁned by t (θ ◦ α) = (tθ )α. Note that the composition of two substitutions is a substitution. To extend the above algorithm to literals L and M, return fail if L and M have different signs or predicate symbols. Suppose L and M both have the same sign and predicate symbol P . Suppose L and M are P (r1, r2, . . . , rn) and P (s1, s2, . . . , sn), respectively, or their negations. Then return Unify_lists([r1 . . . rn], [s1 . . . sn]) as the most general uniﬁer of L and M.
As examples of uniﬁcation, a most general uniﬁer of the terms f (x, a) and f (b, y) is {x → b, y → a}. The terms f (x, g(x)) and f (y, y) are not uniﬁable. A most general uniﬁer of f (x, y, g(y)) and f (z, h(z), w) is {x → z, y → h(z), w → g(h(z))}.
One can also deﬁne uniﬁers and most general uniﬁers of sets of terms. A substitution α is said to be a uniﬁer of a set {t1, t2, . . . , tn} of terms if t1α ≡ t2α ≡ t3α · · · . If such a uniﬁer α exists, this set of terms is said to be uniﬁable. It turns out that if {t1, t2, . . . , tn} is a set of terms and has a uniﬁer, then it has a most general uniﬁer, and this uniﬁer can be computed as Unify(f (t1, t2, . . . , tn), f (t2, t3, . . . , tn, t1)) where f is a function symbol of arity n. In a similar way, one can deﬁne most general uniﬁers of sets of literals.
Finally, suppose C1 and C2 are two clauses and A1 and A2 are nonempty subsets of C1 and C2, respectively. Suppose for convenience that there are no common variables between C1 and C2. Suppose the set {L: L ∈ A1} ∪ {¬L: L ∈ A2} is uniﬁable, and let α be its most general uniﬁer. Deﬁne the resolvent of C1 and C2 on the subsets A1 and A2 to be the clause
(C1 − A1)α ∪ (C2 − A2)α.
A resolvent of C1 and C2 is deﬁned to be a resolvent of C1 and C2 on two such sets A1 and A2 of literals. A1 and A2 are called subsets of resolution. If C1 and C2 have common variables, it is assumed that the variables of one of these clauses are renamed before resolving to insure that there are no common variables. There may be more than one resolvent of two clauses, or there may not be any resolvents at all.
Most of the time, A1 and A2 consist of single literals. This considerably simpliﬁes the deﬁnition, and most of our examples will be of this special case. If A1 ≡ {L} and A2 ≡ {M}, then L and M are called literals of resolution. We call this kind of resolution single literal resolution. Often, one deﬁnes resolution in terms of factoring and single literal resolution. If C is a clause and θ is a most general uniﬁer of two distinct literals of C, then Cθ is called a factor of C. Deﬁning resolution in terms of factoring has some advantages, though it increases the number of clauses one must store.

32

1. Knowledge Representation and Classical Logic

Here are some examples. Suppose C1 is {P (a)} and C2 is {¬P (x), Q(f (x))}. Then a resolvent of these two clauses on the literals P (a) and ¬P (x) is {Q(f (a))}. This is because the most general uniﬁer of these two literals is {x → a}, and applying this substitution to {Q(f (x))} yields the clause {Q(f (a))}.
Suppose C1 is {¬P (a, x)} and C2 is {P (y, b)}. Then { } (the empty clause) is a resolvent of C1 and C2 on the literals ¬P (a, x) and P (y, b).
Suppose C1 is {¬P (x), Q(f (x))} and C2 is {¬Q(x), R(g(x))}. In this case, the variables of C2 are ﬁrst renamed before resolving, to eliminate common variables, yielding the clause {¬Q(y), R(g(y))}. Then a resolvent of C1 and C2 on the literals Q(f (x)) and ¬Q(y) is {¬P (x), R(g(f (x)))}.
Suppose C1 is {P (x), P (y)} and C2 is {¬P (z), Q(f (z))}. Then a resolvent of C1 and C2 on the sets {P (x), P (y)} and {¬P (z)} is {Q(f (z))}.
A resolution proof of a clause C from a set S of clauses is a sequence C1, C2, . . . , Cn of clauses in which Cn is C and in which for all i, either Ci is an element of S or there exist integers j, k < i such that Ci is a resolvent of Cj and Ck. Such a proof is called a (resolution) refutation from S if Cn is { } (the empty clause).
A theorem proving method is said to be complete if it is able to prove any valid
formula. For unsatisﬁability testing, a theorem proving method is said to be complete
if it can derive false, or the empty clause, from any unsatisﬁable set of clauses. It is
known that resolution is complete:

Theorem 1.3.4. A set S of ﬁrst-order clauses is unsatisﬁable iff there is a resolution refutation from S.

Therefore one can use resolution to test unsatisﬁability of clause sets, and hence validity of ﬁrst-order formulas. The advantage of resolution over the Prover procedure above is that resolution uses uniﬁcation to choose instances of the clauses that are more likely to appear in a proof. So in order to show that a ﬁrst-order formula A is valid, one can do the following:
• Convert ¬A to clause form S.
• Search for a proof of the empty clause from S.
As an example of this procedure, resolution can be applied to show that the ﬁrstorder formula
∀x∃y(P (x) → Q(x, y)) ∧ ∀x∀y∃z(Q(x, y) → R(x, z)) → ∀x∃z(P (x) → R(x, z))
is valid. Here → represents logical implication, as usual. In the refutational approach, one negates this formula to obtain
¬[∀x∃y(P (x) → Q(x, y)) ∧ ∀x∀y∃z(Q(x, y) → R(x, z)) → ∀x∃z(P (x) → R(x, z))],
and shows that this formula is unsatisﬁable. The procedure of Section 1.3.3 for translating formulas into clause form yields the following set S of clauses:
{{¬P (x), Q(x, f (x))}, {¬Q(x, y), R(x, g(x, y))}, {P (a)}, {¬R(a, z)}}.

V. Lifschitz, L. Morgenstern, D. Plaisted

33

The following is then a resolution refutation from this clause set:

1. P (a)

(input)

2. ¬P (x), Q(x, f (x))

(input)

3. Q(a, f (a))

(resolution, 1, 2)

4. ¬Q(x, y), R(x, g(x, y)) (input)

5. R(a, g(a, f (a)))

(3, 4, resolution)

6. ¬R(a, z)

(input)

7. false

(5, 6, resolution)

The designation “input” means that a clause is in S. Since false (the empty clause) has been derived from S by resolution, it follows that S is unsatisﬁable, and so the original ﬁrst-order formula is valid.
Even though resolution is much more efﬁcient than the Prover procedure, it is still not as efﬁcient as one would like. In the early days of resolution, a number of reﬁnements were added to resolution, mostly by the Argonne group, to make it more efﬁcient. These were the set of support strategy, unit preference, hyper-resolution, subsumption and tautology deletion, and demodulation. In addition, the Argonne group preferred using small clauses when searching for resolution proofs. Also, they employed some very efﬁcient data structures for storing and accessing clauses. We will describe most of these reﬁnements now.
A clause C is called a tautology if for some literal L, L ∈ C and ¬L ∈ C. It is known that if S is unsatisﬁable, there is a refutation from S that does not contain any tautologies. This means that tautologies can be deleted as soon as they are generated and need never be included in resolution proofs.
In general, given a set S of clauses, one searches for a refutation from S by performing a sequence of resolutions. To ensure completeness, this search should be fair, that is, if clauses C1 and C2 have been generated already, and it is possible to resolve these clauses, then this resolution must eventually be done. However, the order in which resolutions are performed is nonetheless very ﬂexible, and a good choice in this respect can help the prover a lot. One good idea is to prefer resolutions of clauses that are small, that is, that have small terms in them.
Another way to guide the choice of resolutions is based on subsumption, as follows: Clause C is said to subsume clause D if there is a substitution Θ such that CΘ ⊆ D. For example, the clause {Q(x)} subsumes the clause {¬P (a), Q(a)}. C is said to properly subsume D if C subsumes D and the number of literals in C is less than or equal to the number of literals in D. For example, the clause {Q(x), Q(y)} subsumes {Q(a)}, but does not properly subsume it. It is known that clauses properly subsumed by other clauses can be deleted when searching for resolution refutations from S. It is possible that these deleted clauses may still appear in the ﬁnal refutation, but once a clause C is generated that properly subsumes D, it is never necessary to use D in any further resolutions. Subsumption deletion can reduce the proof time tremendously, since long clauses tend to be subsumed by short ones. Of course, if two clauses properly subsume each other, one of them should be kept. The use of appropriate data structures [222, 226] can greatly speed up the subsumption test, and indeed term indexing data structures are essential for an efﬁcient theorem prover, both for quickly ﬁnding clauses to resolve and for performing the subsumption test. As an example [222], in a run of the Vampire prover on the problem LCL-129-1.p from the

34

1. Knowledge Representation and Classical Logic

TPTP library of www.tptp.org, in 270 seconds 8,272,207 clauses were generated of which 5,203,928 were deleted because their weights were too large, 3,060,226 were deleted because they were subsumed by existing clauses (forward subsumption), and only 8053 clauses were retained.
This can all be combined to obtain a program for searching for resolution proofs from S, as follows:
procedure Resolver(S) R ← S; while false ∈/ R do choose clauses C1, C2 ∈ R fairly, preferring small clauses; if no new pairs C1, C2 exist then return “satisﬁable” ﬁ; R ← {D: D is a resolvent of C1, C2 and D is not a tautology}; for D ∈ R do if no clause in R properly subsumes D then R ← {D} ∪ {C ∈ R: D does not properly subsume C} ﬁ; od od
end Resolver
In order to make precise what a “small clause” is, one deﬁnes C , the symbol size of clause C, as follows:
x = 1 for variables x c = 1 for constant symbols c f (t1, . . . , tn) = 1 + t1 + · · · + tn for terms f (t1, . . . , tn) P (t1, . . . , tn) = 1 + t1 + · · · + tn for atoms P (t1, . . . , tn) ¬A = A for atoms A {L1, L2, . . . , Ln} = L1 + · · · + Ln for clauses {L1, L2, . . . , Ln}
Small clauses, then, are those having a small symbol size. Another technique used by the Argonne group is the unit preference strategy, de-
ﬁned as follows: A unit clause is a clause that contains exactly one literal. A unit resolution is a resolution of clauses C1 and C2, where at least one of C1 and C2 is a unit clause. The unit preference strategy prefers unit resolutions, when searching for proofs. Unit preference has to be modiﬁed to permit non-unit resolutions to guarantee completeness. Thus non-unit resolutions are also performed, but not as early. The unit preference strategy helps because unit resolutions reduce the number of literals in a clause.

Reﬁnements of resolution
In an attempt to make resolution more efﬁcient, many, many reﬁnements were developed in the early days of theorem proving. We present a few of them, and mention a number of others. For a discussion of resolution and its reﬁnements, and theorem proving in general, see [53, 163, 45, 271, 87, 155]. It is hard to know which reﬁnements will help on any given example, but experience with a theorem prover can help to give one a better idea of which reﬁnements to try. In general, none of these reﬁnements help very much most of the time.

V. Lifschitz, L. Morgenstern, D. Plaisted

35

A literal is called positive if it is an atom, that is, has no negation sign. A literal
with a negation sign is called negative. A clause C is called positive if all of the literals
in C are positive. C is called negative if all of the literals in C are negative. A resolu-
tion of C1 and C2 is called positive if one of C1 and C2 is a positive clause. It is called negative if one of C1 and C2 is a negative clause. It turns out that positive resolution is complete, that is, if S is unsatisﬁable, then there is a refutation from S in which all
of the resolutions are positive. This reﬁnement of resolution is known as P1 deduction in the literature. Similarly, negative resolution is complete. Hyper-resolution is essen-
tially a modiﬁcation of positive resolution in which a series of positive resolvents is
done all at once. To be precise, suppose that C is a clause having at least one nega-
tive literal and D1, D2, . . . , Dn are positive clauses. Suppose C1 is a resolvent of C and D1, C2 is a resolvent of C1 and D2, . . . , and Cn is a resolvent of Cn−1 and Dn. Suppose that Cn is a positive clause but none of the clauses Ci are positive, for i < n. Then Cn is called a hyper-resolvent of C and D1, D2, . . . , Dn. Thus the inference steps in hyper-resolution are sequences of positive resolutions. In the hyper-resolution
strategy, the inference engine looks for a complete collection D1 . . . Dn of clauses to resolve with C and only performs the inference when the entire hyper-resolution can
be carried out. Hyper-resolution is sometimes useful because it reduces the number of
intermediate results that must be stored in the prover.
Typically, when proving a theorem, there is a general set A of axioms and a par-
ticular formula F that one wishes to prove. So one wishes to show that the formula A → F is valid. In the refutational approach, this is done by showing that ¬(A → F ) is unsatisﬁable. Now, ¬(A → F ) is transformed to A ∧ ¬F in the clause form translation. One then obtains a set SA of clauses from A and a set SF of clauses from ¬F . The set SA ∪ SF is unsatisﬁable iff A → F is valid. One typically tries to show SA ∪ SF unsatisﬁable by performing resolutions. Since one is attempting to prove F , one would expect that resolutions involving the clauses SF are more likely to be useful, since resolutions involving two clauses from SA are essentially combining general axioms. Thus one would like to only perform resolutions involving clauses in SF or clauses derived from them. This can be achieved by the set of support strategy, if the
set SF is properly chosen. The set of support strategy restricts all resolutions to involve a clause in the set
of support or a clause derived from it. To guarantee completeness, the set of support must be chosen to include the set of clauses C of S such that I |= C for some interpretation I . Sets A of axioms typically have standard models I , so that I |= A. Since translation to clause form is satisﬁability preserving, I |= SA as well, where I is obtained from I by a suitable interpretation of Skolem functions. If the set of support
is chosen as the clauses not satisﬁed by I , then this set of support will be a subset of
the set SF above and inferences are restricted to those that are relevant to the particular theorem. Of course, it is not necessary to test if I |= C for clauses C; if one knows that A is satisﬁable, one can choose SF as the set of support.
The semantic resolution strategy is like the set-of-support resolution, but requires
that when two clauses C1 and C2 resolve, at least one of them must not be satisﬁed by a speciﬁed interpretation I . Some interpretations permit the test I |= C to be carried out; this is possible, for example, if I has a ﬁnite domain. Using such a semantic
deﬁnition of the set of support strategy further restricts the set of possible resolutions
over the set of support strategy while retaining completeness.

36

1. Knowledge Representation and Classical Logic

Other reﬁnements of resolution include ordered resolution, which orders the literals of a clause, and requires that the subsets of resolution include a maximal literal in their respective clauses. Unit resolution requires all resolutions to be unit resolutions, and is not complete. Input resolution requires all resolutions to involve a clause from S, and this is not complete, either. Unit resulting (UR) resolution is like unit resolution, but has larger inference steps. This is also not complete, but works well surprisingly often. Locking resolution attaches indices to literals, and uses these to order the literals in a clause and decide which literals have to belong to the subsets of resolution. Ancestry-ﬁlter form resolution imposes a kind of linear format on resolution proofs. These strategies are both complete. Semantic resolution is compatible with some ordering reﬁnements, that is, the two strategies together are still complete.
It is interesting that resolution is complete for logical consequences, in the following sense: If S is a set of clauses, and C is a clause such that S |= C, that is, C is a logical consequence of S, then there is a clause D derivable by resolution such that D subsumes C.
Another resolution reﬁnement that is useful sometimes is splitting. If C is a clause and C ≡ C1 ∪ C2, where C1 and C2 have no common variables, then S ∪ {C} is unsatisﬁable iff S ∪ {C1} is unsatisﬁable and S ∪ {C2} is unsatisﬁable. The effect of this is to reduce the problem of testing unsatisﬁability of S ∪ {C} to two simpler problems. A typical example of such a clause C is a ground clause with two or more literals.
There is a special class of clauses called Horn clauses for which specialized theorem proving strategies are complete. A Horn clause is a clause that has at most one positive literal. Such clauses have found tremendous application in logic programming languages. If S is a set of Horn clauses, then unit resolution is complete, as is input resolution.

Other strategies
There are a number of other strategies which apply to sets S of clauses, but do not use resolution. One of the most notable is model elimination [162], which constructs chains of literals and has some similarities to the DPLL procedure. Model elimination also speciﬁes the order in which literals of a clause will “resolve away”. There are also a number of connection methods [28, 158], which operate by constructing links between complementary literals in different clauses, and creating structures containing more than one clause linked together. In addition, there are a number of instance-based strategies, which create a set T of ground instances of S and test T for unsatisﬁability using a DPLL-like procedure. Such instance-based methods can be much more efﬁcient than resolution on certain kinds of clause sets, namely, those that are highly non-Horn but do not involve deep term structure.
Furthermore, there are a number of strategies that do not use clause form at all. These include the semantic tableau methods, which work backwards from a formula and construct a tree of possibilities; Andrews’ matings method, which is suitable for higher order logic and has obtained some impressive proofs automatically; natural deduction methods; and sequent style systems. Tableau systems have found substantial application in automated deduction, and many of these are even adapted to formulas in clause form; for a survey see [106].

V. Lifschitz, L. Morgenstern, D. Plaisted

37

Evaluating strategies
In general, we feel that qualities that need to be considered when evaluating a strategy are not only completeness but also propositional efﬁciency, goal-sensitivity and use of semantics. By propositional efﬁciency is meant the degree to which the efﬁciency of the method on propositional problems compares with DPLL; most strategies do poorly in this respect. By goal-sensitivity is meant the degree to which the method permits one to concentrate on inferences related to the particular clauses coming from the negation of the theorem (the set SF discussed above). When there are many, many input clauses, goal sensitivity is crucial. By use of semantics is meant whether the method can take advantage of natural semantics that may be provided with the problem statement in its search for a proof. An early prover that did use semantics in this way was the geometry prover of Gelernter et al. [94]. Note that model elimination and set of support strategies are goal-sensitive but apparently not propositionally efﬁcient. Semantic resolution is goal-sensitive and can use natural semantics, but is not propositionally efﬁcient, either. Some instance-based strategies are goal-sensitive and use natural semantics and are propositionally efﬁcient, but may have to resort to exhaustive enumeration of ground terms instead of uniﬁcation in order to instantiate clauses. A further issue is to what extent various methods permit the incorporation of efﬁcient equality techniques, which varies a lot from method to method. Therefore there are some interesting problems involved in combining as many of these desirable features as possible. And for strategies involving extensive human interaction, the criteria for evaluation are considerably different.

1.3.3 Equality
When proving theorems involving equations, one obtains many irrelevant terms. For example, if one has the equations x + 0 = x and x ∗ 1 = x, and addition and multiplication are commutative and associative, then one obtains many terms identical to x, such as 1 ∗ x ∗ 1 ∗ 1 + 0. For products of two or three variables or constants, the situation becomes much worse. It is imperative to ﬁnd a way to get rid of all of these equivalent terms. For this purpose, specialized methods have been developed to handle equality.
As examples of mathematical structures where such equations arise, for groups and monoids the group operation is associative with an identity, and for abelian groups the group operation is associative and commutative. Rings and ﬁelds also have an associative and commutative addition operator with an identity and another multiplication operator that is typically associative. For Boolean algebras, the multiplication operation is also idempotent. For example, set union and intersection are associative, commutative, and idempotent. Lattices have similar properties. Such equations and structures typically arise when axiomatizing integers, reals, complex numbers, matrices, and other mathematical objects.
The most straightforward method of handling equality is to use a general ﬁrst-order resolution theorem prover together with the equality axioms, which are the following (assuming free variables are implicitly universally quantiﬁed):

38

1. Knowledge Representation and Classical Logic

x = x, x = y → y = x, x = y ∧ y = z → x = z, x1 = y1 ∧ x2 = y2 ∧ · · · ∧ xn = yn → f (x1 . . . xn) = f (y1 . . . yn)
for all function symbols f, x1 = y1 ∧ x2 = y2 ∧ · · · ∧ xn = yn ∧ P (x1 . . . xn) → P (y1 . . . yn)
for all predicate symbols P
Let Eq refer to this set of equality axioms. The approach of using Eq explicitly leads to many inefﬁciencies, as noted above, although in some cases it works reasonably well.
Another approach to equality is the modiﬁcation method of Brand [40, 19]. In this approach, a set S of clauses is transformed into another set S with the following property: S ∪ Eq is unsatisﬁable iff S ∪ {x = x} is unsatisﬁable. Thus this transformation avoids the need for the equality axioms, except for {x = x}. This approach often works a little better than using Eq explicitly.

Contexts
In order to discuss other inference rules for equality, some terminology is needed. A context is a term with occurrences of in it. For example, f ( , g(a, )) is a context. A by itself is also a context. One can also have literals and clauses with in them, and they are also called contexts. If n is an integer, then an n-context is a term with n occurrences of . If t is an n-context and m n, then t[t1, . . . , tm] represents t with the leftmost m occurrences of replaced by the terms t1, . . . , tm, respectively. Thus, for example, f ( , b, ) is a 2-context, and f ( , b, )[g(c)] is f (g(c), b, ). Also, f ( , b, )[g(c)][a] is f (g(c), b, a). In general, if r is an n-context and m n and the terms si are 0-contexts, then r[s1, . . . , sn] ≡ r[s1][s2] . . . [sn]. However, f ( , b, )[g( )] is f (g( ), b, ), so f ( , b, )[g( )][a] is f (g(a), b, ). In general, if r is a k-context for k 1 and s is an n-context for n 1, then r[s][t] ≡ r[s[t]], by a simple argument (both replace the leftmost in r[s] by t).

Termination orderings on terms
It is necessary to discuss partial orderings on terms in order to explain inference rules for equality. Partial orderings give a precise deﬁnition of the complexity of a term, so that s > t means that the term s is more complex than t in some sense, and replacing s by t makes a clause simpler. A partial ordering > is well-founded if there are no inﬁnite sequences xi of elements such that xi > xi+1 for all i 0. A termination ordering on terms is a partial ordering > which is well founded and satisﬁes the full invariance property, that is, if s > t and Θ is a substitution then sΘ > tΘ, and also satisﬁes the replacement property, that is, s > t implies r[s] > r[t] for all 1-contexts r.
Note that if s > t and > is a termination ordering, then all variables in t appear also in s. For example, if f (x) > g(x, y), then by full invariance f (x) > g(x, f (x)), and by replacement g(x, f (x)) > g(x, g(x, f (x))), etc., giving an inﬁnite descending sequence of terms.
The concept of a multiset is often useful to show termination. Informally, a multiset is a set in which an element can occur more than once. Formally, a multiset S is

V. Lifschitz, L. Morgenstern, D. Plaisted

39

a function from some underlying domain D to the non-negative integers. It is said to be ﬁnite if {x: S(x) > 0} is ﬁnite. One writes x ∈ S if S(x) > 0. S(x) is called the multiplicity of x in S; this represents the number of times x appears in S. If S and T are multisets then S ∪ T is deﬁned by (S ∪ T )(x) = S(x) + T (x) for all x. A partial ordering > on D can be extended to a partial ordering on multisets in the following way: One writes S T if there is some multiset V such that S = S ∪ V and T = T ∪ V and S is nonempty and for all t in T there is an s in S such that s > t. This relation can be computed reasonably fast by deleting common elements from S and T as long as possible, then testing if the speciﬁed relation between S and T holds. The idea is that a multiset becomes smaller if an element is replaced by any number of smaller elements. Thus {3, 4, 4} {2, 2, 2, 2, 1, 4, 4} since 3 has been replaced by 2, 2, 2, 2, 1. This operation can be repeated any number of times, still yielding a smaller multiset; in fact, the relation can be deﬁned in this way as the smallest transitive relation having this property [75]. One can show that if > is well founded, so is . For a comparison with other deﬁnitions of multiset ordering, see [131].
We now give some examples of termination orderings. The simplest kind of termination orderings are those that are based on size. Recall that s is the symbol size (number of symbol occurrences) of a term s. One can then deﬁne > so that s > t if for all Θ making sΘ and tΘ ground terms, sΘ > tΘ . For example, f (x, y) > g(y) in this ordering, but it is not true that h(x, a, b) > f (x, x) because x could be replaced by a large term. This termination ordering is computable; s > t iff s > t and no variable occurs more times in t than s.
More powerful techniques are needed to get some more interesting termination orderings. One of the most remarkable results in this area is a theorem of Dershowitz [75] about simpliﬁcation orderings, that gives a general technique for showing that an ordering is a termination ordering. Before his theorem, each ordering had to be shown well founded separately, and this was often difﬁcult. This theorem makes use of simpliﬁcation orderings.

Deﬁnition 1.3.5. A partial ordering > on terms is a simpliﬁcation ordering if it satisﬁes the replacement property, that is, for 1-contexts r, s > t implies r[s] > r[t], and has the subterm property, that is, s > t if t is a proper subterm of s. Also, if there are function symbols f with variable arity, it is required that f (. . . s . . .) > f (. . . . . .) for all such f .

Theorem 1.3.6. All simpliﬁcation orderings are well founded.

Proof. Based on Kruskal’s tree theorem [148], which says that in any inﬁnite sequence
t1, t2, t3, . . . of terms, there are natural numbers i and j with i < j such that ti is embedded in tj in a certain sense. It turns out that if ti is embedded in tj then tj ti for any simpliﬁcation ordering >.

The recursive path ordering is one of the simplest simpliﬁcation orderings. This ordering is deﬁned in terms of a precedence ordering on function symbols, which is a partial ordering on the function symbols. One writes f < g to indicate that f is less than g in the precedence relation on function symbols. The recursive path ordering will

40

1. Knowledge Representation and Classical Logic

be presented as a complete set of inference rules that may be used to construct proofs of s > t. That is, if s > t then there is a proof of this in the system. Also, by using the inference rules backwards in a goal-directed manner, it is possible to construct a reasonably efﬁcient decision procedure for statements of the form s > t. Recall that if > is an ordering, then is the extension of this ordering to multisets. The ordering we present is somewhat weaker than that usually given in the literature.

f = g {s1 . . . sm} {t1 . . . tn} f (s1 . . . sm) > g(t1 . . . tn) si t
f (s1 . . . sm) > t true ss

f > g f (s1 . . . sm) > ti all i f (s1 . . . sm) > g(t1 . . . tn)

For example, suppose ∗ > +. Then one can show that x ∗ (y + z) > x ∗ y + x ∗ z as follows:

true

true

yy

yy

y+z>y

y+z>z

{x, y + z} {x, y} {x, y + z} {x, z}

x ∗ (y + z) > x ∗ y x ∗ (y + z) > x ∗ z

x ∗ (y + z) > x ∗ y + x ∗ z

∗>+

For some purposes, it is necessary to modify this ordering so that subterms are considered lexicographically. In general, if > is an ordering, then the lexicographic extension >lex of > to tuples is deﬁned as follows:

s1 > t1 (s1 . . . sm) >lex (t1 . . . tn)
s1 = t1 (s2 . . . sm) >lex (t2 . . . tn) (s1 . . . sm) >lex (t1 . . . tn) true
(s1 . . . sm) >lex ( )
One can show that if > is well founded, then so is its extension >lex to bounded length tuples. This lexicographic treatment of subterms is the idea of the lexicographic path ordering of Kamin and Levy [136]. This ordering is deﬁned by the following inference rules:

f = g (s1 . . . sm) >lex (t1 . . . tn) f (s1 . . . sm) > tj , all j 2 f (s1 . . . sm) > g(t1 . . . tn)
si t f (s1 . . . sm) > t

V. Lifschitz, L. Morgenstern, D. Plaisted

41

true

ss f >g

f (s1 . . . sm) > ti all i

f (s1 . . . sm) > g(t1 . . . tn)

In the ﬁrst inference rule, it is not necessary to test f (s1 . . . sm) > t1 since (s1 . . . sm) >lex (t1 . . . tn) implies s1 t1 hence f (s1 . . . sm) > t1. One can show that this ordering is a simpliﬁcation ordering for systems having ﬁxed arity function symbols. This ordering has the useful property that f (f (x, y), z) >lex f (x, f (y, z)); informally, the reason for this is that the terms have the same size, but the ﬁrst subterm f (x, y) of f (f (x, y), z) is always larger than the ﬁrst subterm x of f (x, f (y, z)).
The ﬁrst orderings that could be classiﬁed as recursive path orderings were those of Plaisted [208, 207]. A large number of other similar orderings have been developed since the ones mentioned above, for example the dependency pair method [7] and its recent automatic versions [120, 98].

Paramodulation
Above, we saw that the equality axioms Eq can be used to prove theorems involving equality, and that Brand’s modiﬁcation method is another approach that avoids the need for the equality axioms. A better approach in most cases is to use the paramodulation rule [228, 193] deﬁned as follows:
C[t], r = s ∨ D, r and t are uniﬁable, t is not a variable, Unify(r, t) = θ
Cθ [sθ] ∨ Dθ
Here C[t] is a clause containing a subterm t, C is a context, and t is a non-variable term. Also, Cθ [sθ] is the clause (C[t])θ with sθ replacing the speciﬁed occurrence of tθ . Also, r = s ∨ D is another clause having a literal r = s whose predicate is equality and remaining literals D, which can be empty. To understand this rule, consider that rθ = sθ is an instance of r = s, and rθ and tθ are identical. If Dθ is false, then rθ = sθ must be true, so it is possible to replace rθ in (C[t])θ by sθ if Dθ is false. Thus Cθ [sθ] ∨ Dθ is inferred. It is assumed as usual that variables in C[t] or in r = s ∨ D are renamed if necessary to insure that these clauses have no common variables before performing paramodulation. The clause C[t] is said to be paramodulated into. It is also possible to paramodulate in the other direction, that is, the equation r = s can be used in either direction.
For example, the clause P (g(a)) ∨ Q(b) is a paramodulant of P (f (x)) and (f (a) = g(a)) ∨ Q(b). Brand [40] showed that if Eq is the set of equality axioms given above and S is a set of clauses, then S ∪ Eq is unsatisﬁable iff there is a proof of the empty clause from S ∪ {x = x} using resolution and paramodulation as inference rules. Thus, paramodulation allows us to dispense with all the equality axioms except x = x.
Some more recent proofs of the completeness of resolution and paramodulation [125] show the completeness of restricted versions of paramodulation which considerably reduce the search space. In particular, it is possible to restrict this rule so that it is not performed if sθ > rθ , where > is a termination ordering ﬁxed in advance. So if one has an equation r = s, and r > s, then this equation can only be used to replace instances of r by instances of s. If s > r, then this equation can only be used

42

1. Knowledge Representation and Classical Logic

in the reverse direction. The effect of this is to constrain paramodulation so that “big” terms are replaced by “smaller” ones, considerably improving its efﬁciency. It would be a disaster to allow paramodulation to replace x by x ∗ 1, for example. Another complete reﬁnement of ordered paramodulation is that paramodulation only needs to be done into the “large” side of an equation. If the subterm t of C[t] occurs in an equation u = v or v = u of C[t], and u > v, where > is the termination ordering being used, then the paramodulation need not be done if the speciﬁed occurrence of t is in v. Some early versions of paramodulation required the use of the functionally reﬂexive axioms of the form f (x1, . . . , xn) = f (x1, . . . , xn), but this is now known not to be necessary. When D is empty, paramodulation is similar to “narrowing”, which has been much studied in the context of logic programming and term rewriting. Recently, a more reﬁned approach to the completeness proof of resolution and paramodulation has been found [16, 17] which permits greater control over the equality strategy. This approach also permits one to devise resolution strategies that have a greater control over the order in which literals are resolved away.

Demodulation

Similar to paramodulation is the rewriting or “demodulation” rule, which is essentially a method of simpliﬁcation.

C[t], r = s, rθ ≡ t, rθ > sθ

C [s θ ]

.

Here C[t] is a clause (so C is a 1-context) containing a non-variable term t, r = s is a unit clause, and > is the termination ordering that is ﬁxed in advance. It is assumed that variables are renamed so that C[t] and r = s have no common variables before this rule is applied. The clause C[sθ] is called a demodulant of C[t] and r = s. Similarly, C[sθ] is a demodulant of C[t] and s = r, if rθ > sθ . Thus an equation can be used in either direction, if the ordering condition is satisﬁed.
As an example, given the equation x ∗ 1 = x and assuming x ∗ 1 > x and given a clause C[f (a) ∗ 1] having a subterm of the form f (a) ∗ 1, this clause can be simpliﬁed to C[f (a)], replacing the occurrence of f (a) ∗ 1 in C by f (a).
To justify the demodulation rule, the instance rθ = sθ of the equation r = s can be inferred because free variables are implicitly universally quantiﬁed. This makes it possible to replace rθ in C by sθ, and vice versa. But rθ is t, so t can be replaced by sθ.
Not only is the demodulant C[sθ] inferred, but the original clause C[t] is typically deleted. Thus, in contrast to resolution and paramodulation, demodulation replaces clauses by simpler clauses. This can be a considerable aid in reducing the number of generated clauses. This also makes mechanical theorem proving closer to human reasoning.
The reason for specifying that sθ is simpler than rθ is not only the intuitive desire to simplify clauses, but also to ensure that demodulation terminates. For example, there is no termination ordering in which x ∗ y > y ∗ x, since then the clause a ∗ b = c could demodulate using the equation x ∗ y = y ∗ x to b ∗ a = c and then to a ∗ b = c and so on indeﬁnitely. Such an ordering > could not be a termination ordering, since it

V. Lifschitz, L. Morgenstern, D. Plaisted

43

violates the well-foundedness condition. However, for many termination orderings >, x ∗ 1 > x, and thus the clauses P (x ∗ 1) and x ∗ 1 = x have P (x) as a demodulant if some such ordering is being used.
Resolution with ordered paramodulation and demodulation is still complete if paramodulation and demodulation are done with respect to the same simpliﬁcation ordering during the proof process [125]. Demodulation is essential in practice, for without it one can generate expressions like x ∗ 1 ∗ 1 ∗ 1 that clutter up the search space. Some complete reﬁnements of paramodulation also restrict which literals can be paramodulated into, which must be the “largest” literals in the clause in a sense. Such reﬁnements are typically used with resolution reﬁnements that also restrict subsets of resolution to contain “large” literals in a clause. Another recent development is basic paramodulation, which restricts the positions in a term into which paramodulation can be done [18, 194]; this reﬁnement was used in McCune’s proof of the Robbins problem [176].

1.3.4 Term Rewriting Systems
A beautiful theory of term-rewriting systems has been developed to handle proofs involving equational systems; these are theorems of the form E |= e where E is a collection of equations and e is an equation. For such systems, term-rewriting techniques often lead to very efﬁcient proofs. The Robbins problem was of this form, for example.
An equational system is a set of equations. Often one is interested in knowing if an equation follows logically from the given set. For example, given the equations x + y = y + x, (x + y) + z = x + (y + z), and −(−(x + y) + −(x + −y)) = x, one might want to know if the equation −(−x + y) + −(−x + −y) = x is a logical consequence. As another example, one might want to know whether x ∗ y = y ∗ x in a group in which x2 = e for all x. Such systems are of interest in theorem proving, programming languages, and other areas. Common data structures like lists and stacks can often be described by such sets of equations. In addition, a functional program is essentially a set of equations, typically with higher order functions, and the execution of a program is then a kind of equational reasoning. In fact, some programming languages based on term rewriting have been implemented, and can execute several tens of millions of rewrites per second [72]. Another language based on rewriting is MAUDE [119]. Rewriting techniques have also been used to detect ﬂaws in security protocols and prove properties of such protocols [129]. Systems for mechanising such proofs on a computer are becoming more and more powerful. The Waldmeister system [92] is particularly effective for proofs involving equations and rewriting. The area of rewriting was largely originated by the work of Knuth and Bendix [144]. For a discussion of term-rewriting techniques, see [76, 11, 77, 199, 256].
Syntax of equational systems
A term u is said to be a subterm of t if u is t or if t is f (t1, . . . , tn) and u is a subterm of ti for some i. An equation is an expression of the form s = t where s and t are terms. An equational system is a set of equations. We will generally consider only unsorted equational systems, for simplicity The letter E will be used to refer to equational systems.

44

1. Knowledge Representation and Classical Logic

We give a set of inference rules for deriving consequences of equations.
t =u tθ = uθ t =u u=t
t =u f (. . . t . . .) = f (. . . u . . .) t=u u=v
t =v true t =t The following result is due to Birkhoff [30]:

Theorem 1.3.7. If E is a set of equations then E |= r = s iff r = s is derivable from E using these rules.

This result can be stated in an equivalent way. Namely, E |= r = s iff there is a ﬁnite sequence u1, u2, . . . , un of terms such that r is u1 and s is un and for all i, ui+1 is obtained from ui by replacing a subterm t of ui by a term u, where the equation t = u or the equation u = t is an instance of an equation in E.
This gives a method for deriving logical consequences of sets of equations. However, it is inefﬁcient. Therefore it is of interest to ﬁnd restrictions of these inference rules that are still capable of deriving all equational consequences of an equational system. This is the motivation for the theory of term-rewriting systems.

Term rewriting
The idea of a term rewriting system is to orient an equation r = s into a rule r → s indicating that instances of r may be replaced by instances of s but not vice versa. Often this is done in such a way as to replace terms by simpler terms, where the deﬁnition of what is simple may be fairly subtle. However, as a ﬁrst approximation, smaller terms are typically simpler. The equation x + 0 = x then would typically be oriented into the rule x + 0 → x. This reduces the generation of terms like ((x + 0) + 0) + 0 which can appear in proofs if no such directionality is applied. The study of term rewriting systems is concerned with how to orient rules and what conditions guarantee that the resulting systems have the same computational power as the equational systems they came from.

Terminology
In this section, variables r, s, t, u refer to terms and → is a relation over terms. Thus the discussion is at a higher level than earlier.
A term-rewriting system R is a set of rules of the form r → s, where r and s are terms. It is common to require that any variable that appears in s must also appear in r. It is also common to require that r is not a variable. The rewrite relation →R is

V. Lifschitz, L. Morgenstern, D. Plaisted

45

deﬁned by the following inference rules:

r → s ρ a substitution

rρ → sρ

r →s

f (. . . r . . .) → f (. . . s . . .)

true

r →∗ r r →s

r →∗ s r →∗ s

s →∗ t

r →∗ t r →s

r ↔s s→r

r ↔s

true

r ↔∗ r r ↔s

r ↔∗ s r ↔∗ s

s ↔∗ t

r ↔∗ t

The notation r indicates derivability using these rules. The r subscript refers to

“rewriting” (not to the term r). A set R of rules may be thought of as a set of log-

ical axioms. Writing s → t is in R, indicates that s → t is such an axiom. Writing

R r s → t indicates that s → t may refer to a rewrite relation not included in R.

Often s →R t is used as an abbreviation for R r s → t, and sometimes the sub-

script R that the

is dropped. relation →∗R

Similarly, →R∗ is the reﬂexive

is deﬁned transitive

in terms of derivability closure of →R. Thus r

from R. →R∗ s if

Note there

is a sequence r1, r2, . . . , rn such that r1 is r, rn is s, and ri →R ri+1 for all i. Such

a sequence is called a rewrite sequence from r to s, or a derivation from r to s. Note

that r →∗R otherwise

r r

for all r and R is irreducible.

.A If r

term r →∗R

is reducible if there is s and s is irreducible

a term then s

s is

such that called a

r → s, normal

form of r.

For example, given the system R = {x + 0 → x, 0 + x → x}, the term 0 + (y + 0)

rewrites in two ways; 0 + (y + 0) → 0 + y and 0 + (y + 0) → y + 0. Applying rewriting again, one obtains 0 + (y + 0) →∗ y. In this case, y is a normal form of

0 + (y + 0), since y cannot be further rewritten. Computationally, rewriting a term s

proceeds by ﬁnding a subterm t of s, called a redex, such that t is an instance of the

left-hand side of some rule in R, and replacing t by the corresponding instance of the

right-hand side of the rule. For example, 0 + (y + 0) is an instance of the left-hand

side 0 + x of the rule 0 + x → x. The corresponding instance of the right-hand side x

of this rule is y + 0, so 0 + (y + 0) is replaced by y + 0. This approach assumes that

all variables on the right-hand side appear also on the left-hand side.

46

1. Knowledge Representation and Classical Logic

We now relate rewriting to equational theories. From the above rules, r ↔ s if

r → s or s → r, and ↔∗ is the reﬂexive transitive closure of ↔. Thus r ↔∗ s if

there is a sequence r1, r2, . . . , rn such that r1 is r, rn is s, and ri ↔ ri+1 for all i. Suppose R is a term rewriting system {r1 → s1, . . . , rn → sn}. Deﬁne R= to be

the associated equational system {r1 = s1, . . . , rn = sn}. Also, t =R u is deﬁned as R= |= t = u, that is, the equation t = u is a logical consequence of the associated

equational system. The relation =R is thus the smallest congruence relation generated by R, in algebraic terms. The relation =R is deﬁned semantically, and the relation →∗

is deﬁned syntactically. It is useful to ﬁnd relationships between these two concepts in

order to be able to compute properties of =R and to ﬁnd complete restrictions of the inference rules of Birkhoff’s theorem. Note that by Birkhoff’s theorem, R= |= t = u

iff t that

↔∗R u. This is already a rewriting can go in both

connection between the two concepts. directions in the derivation for t ↔∗R u

However, the fact is a disadvantage.

What we will show is that if R has certain properties, some of them decidable, then

t =R u iff any normal form of t is the same as any normal form of u. This permits us

to decide if t =R u by rewriting t and u to any normal form and checking if these are

identical.

1.3.5 Conﬂuence and Termination Properties
We now present some properties of term rewriting systems R. Equivalently, these can be thought of as properties of the rewrite relation →R. For terms s and t, s ↓ t means that there is a term u such that s →∗ u and t →∗ u. Also, s ↑ t means that there is a term r such that r →∗ s and r →∗ t. R is said to be conﬂuent if for all terms s and t, s ↑ t implies s ↓ t. The meaning of this is that any two rewrite sequences from a given term, can always be “brought together”. Sometimes one is also interested in ground conﬂuence. R is said to be ground conﬂuent if for all ground terms r, if r →∗ s and r →∗ t then s ↓ t. Most research in term rewriting systems concentrates on conﬂuent systems.
A term rewriting system R (alternatively, a rewrite relation →) has the Church– Rosser property if for all terms s and t, s ↔∗ t iff s ↓ t.

Theorem 1.3.8. (See [192].) A term rewriting system R has the Church–Rosser property iff R is conﬂuent.
Since s ↔∗ t iff s =R t, this theorem connects the equational theory of R with rewriting. In order to decide if s =R t for conﬂuent R it is only necessary to see if s and t rewrite to a common term.
Two term rewriting systems are said to be equivalent if their associated equational theories are equivalent (have the same logical consequences).

Deﬁnition 1.3.9. A term rewriting system is terminating (strongly normalizing) if it has no inﬁnite rewrite sequences. Informally, this means that the rewriting process, applied to a term, will eventually stop, no matter how the rewrite rules are applied.

One desires all rewrite sequences to stop in order to guarantee that no matter how the rewriting is done, it will eventually terminate. An example of a terminating system

V. Lifschitz, L. Morgenstern, D. Plaisted

47

is {g(x) → f (x), f (x) → x}. The ﬁrst rule changes g’s to f ’s and so can only be applied as many times as there are g’s. The second rule reduces the size and so it can only be applied as many times as the size of a term. An example of a nonterminating system is {x → f (x)}. It can be difﬁcult to determine if a system is terminating. The intuitive idea is that a system terminates if each rule makes a term simpler in some sense. However, the deﬁnition of simplicity is not always related to size. It can be that a term becomes simpler even if it becomes larger. In fact, it is not even partially decidable whether a term rewriting system is terminating [128]. Termination orderings are often used to prove that term rewriting systems are terminating. Recall the deﬁnition of termination ordering from Section 1.3.3.

Theorem 1.3.10. Suppose R is a term rewriting system and > is a termination ordering and for all rules r → s in R, r > s. Then R is terminating.

This result can be extended to quasi-orderings, which are relations that are reﬂexive and transitive, but the above result should be enough to give an idea of the proof methods used. Many termination orderings are known; some will be discussed in Section 1.3.5. The orderings of interest are computable orderings, that is, it is decidable whether r > s given terms r and s.
Note that if R is terminating, it is always possible to ﬁnd a normal form of a term by any rewrite sequence continued long enough. However there can be more than one normal form. If R is terminating and conﬂuent, there is exactly one normal form for every term. This gives a decision procedure for the equational theory, since for terms r and s, r =R s iff r ↔R∗ s (by Birkhoff’s theorem) iff r ↓ s (by conﬂuence) iff r and s have the same normal form (by termination). This gives us a directed form of theorem proving in such an equational theory. A term rewriting system which is both terminating and conﬂuent is called canonical. Some authors use the term convergent for such systems [76]. Many such systems are known. Systems that are not terminating may still be globally ﬁnite, which means that for every term s there are ﬁnitely many terms t such that s →∗ t. For a discussion of global ﬁniteness, see [105].
We have indicated how termination is shown; more will be presented in Section 1.3.5. However, we have not shown how to prove conﬂuence. As stated, this looks like a difﬁcult property. However, it turns out that if R is terminating, conﬂuence is decidable, from Newman’s lemma [192], given below. If R is not terminating, there are some methods that can still be used to prove conﬂuence. This is interesting, even though in that case one does not get a decision procedure by rewriting to normal form, since it allows some ﬂexibility in the rewriting procedure.

Deﬁnition 1.3.11. A term rewriting system is locally conﬂuent (weakly conﬂuent) if for all terms r, s, and t, if r → s and r → t then s ↓ t.

Theorem 1.3.12 (Newman’s lemma). If R is locally conﬂuent and terminating then R is conﬂuent.

It turns out that one can test whether R is locally conﬂuent using critical pairs [144], so that local conﬂuence is decidable for terminating systems. Also, if R is not locally conﬂuent, it can sometimes be made so by computing critical pairs between

48

1. Knowledge Representation and Classical Logic

rewrite rules in R and using these critical pairs to add new rewrite rules to R until the process stops. This process is known as completion and was introduced by Knuth and Bendix [144]. Completion can also be seen as adding equations to a set of rewrite rules by ordered paramodulation and demodulation, deleting new equations that are instances of existing ones or that are instances of x = x. These new equations are then oriented into rewrite rules and the process continues. This process may terminate with a ﬁnite canonical term rewriting system or it may continue forever. It may also fail by generating an equation that cannot be oriented into a rewrite rule. One can still use ordered rewriting on such equations so that they function much as a term rewriting system [61]. When completion does not terminate, and even if it fails, it is still possible to use a modiﬁed version of the completion procedure as a semidecision procedure for the associated equational theory using the so-called unfailing completion [14, 15] which in the limit produces a ground conﬂuent term rewriting system. In fact, Huet proved earlier [126] that if the original completion procedure does not fail, it provides a semidecision procedure for the associated equational theory.
Termination orderings
We give techniques to show that a term rewriting system is terminating. These all make use of well founded partial orderings on terms having the property that if s → t then s > t. If such an ordering exists, then a rewriting system is terminating since inﬁnite reduction sequences correspond to inﬁnite descending sequences of terms in the ordering. Recall from Section 1.3.3 that a termination ordering is a well-founded ordering that has the full invariance and replacement properties.
The termination ordering based on size was discussed in Section 1.3.3. Unfortunately, this ordering is too weak to handle many interesting systems such as those containing the rule x ∗ (y + z) → x ∗ y + x ∗ z, since the right-hand side is bigger than the left-hand side and has more occurrences of x. This ordering can be modiﬁed to weigh different symbols differently; the deﬁnition of s can be modiﬁed to be a weighted sum of the number of occurrences of the symbols. The ordering of Knuth and Bendix [144] is more reﬁned and is able to show that systems containing the rule (x ∗ y) ∗ z → x ∗ (y ∗ z) terminate.
Another class of termination orderings are the polynomial orderings suggested by Lankford [149, 150]. For these, each function and constant symbol is interpreted as a polynomial with integer coefﬁcients and terms are ordered by the functions associated with them.
The recursive path ordering was discussed in Section 1.3.3. In order to handle the associativity rule (x ∗ y) ∗ z → x ∗ (y ∗ z) it is necessary to modify the ordering so that subterms are considered lexicographically. This lexicographic treatment of subterms is the idea of the lexicographic path ordering of Kamin and Levy [136]. Using this ordering, one can prove the termination of Ackermann’s function. There are also many orderings intermediate between the recursive path ordering and the lexicographic path ordering; these are known as orderings with “status”. The idea of status is that for some function symbols, when f (s1 . . . sm) and f (t1 . . . tn) are compared, the subterms si and ti are compared using the multiset ordering. For other function symbols, the subterms are compared using the lexicographic ordering. For other function symbols, the subterms are compared using the lexicographic ordering in reverse, that is, from right to left; this is equivalent to reversing the lists and then applying

V. Lifschitz, L. Morgenstern, D. Plaisted

49

the lexicographic ordering. One can show that all such versions of the orderings are simpliﬁcation orderings, for function symbols of bounded arity.
There are also many other orderings known that are similar to the above ones, such as the recursive decomposition ordering [132] and others; for some surveys see [75, 244]. In practice, quasi-orderings are often used to prove termination. A relation is a quasi-ordering if it is reﬂexive and transitive. A quasi-ordering is often written as . Thus x x for all x, and if x y and y z then x z. It is possible that x y and y x even if x and y are distinct; then one writes x ≈ y indicating that such x and y are in some sense “equivalent” in the ordering. One writes x > y if x y but not y x, for a quasi-ordering . The relation > is called the strict part of the quasi-ordering . Note that the strict part of a quasi-ordering is a partial ordering. The multiset extension of a quasi-ordering is deﬁned in a manner similar to the multiset extension of a partial ordering [131, 75].

Deﬁnition 1.3.13. A quasi-ordering on terms satisﬁes the replacement property (is monotonic) if s t implies f (. . . s . . .) f (. . . t . . .). Note that it is possible to have s > t and f (. . . s . . .) ≈ f (. . . t . . .).

Deﬁnition 1.3.14. A quasi-ordering is a quasi-simpliﬁcation ordering if f (. . . t . . .) t for all terms and if f (. . . t . . .) f (. . . . . .) for all terms and all function symbols f of variable arity, and if the ordering satisﬁes the replacement property.

Deﬁnition 1.3.15. A quasi-ordering satisﬁes the full invariance property (see Section 1.3.5) if s > t implies sΘ > tΘ for all s, t, Θ.

Theorem 1.3.16. (See Dershowitz [74].) For terms over a ﬁnite set of function symbols, all quasi-simpliﬁcation orderings have strict parts which are well founded.

Proof. Using Kruskal’s tree theorem [148].

Theorem 1.3.17. Suppose R is a term rewriting system and is a quasi-simpliﬁcation ordering which satisﬁes the full invariance property. Suppose that for all rules l → r in R, l > r. Then R is terminating.

Actually, a version of the recursive path ordering adapted to quasi-orderings is known as the recursive path ordering in the literature. The idea is that terms that are identical up to permutation of arguments, are equivalent. There are a number of different orderings like the recursive path ordering.
Some decidability results about termination are known. In general, it is undecidable whether a system R is terminating [128]; however, for ground systems, that is, systems in which left and right-hand sides of rules are ground terms, termination is decidable [128]. For non-ground systems, termination of even one rule systems has been shown to be undecidable [63]. However, automatic tools have been developed that are very effective at either proving a system to be terminating or showing that it is not terminating, or ﬁnding an orientation of a set of equations that is terminating [120, 82, 145, 98]. In fact, one such system [145] from [91] was able to ﬁnd an automatic

50

1. Knowledge Representation and Classical Logic

proof of termination of a system for which the termination proof was the main result of a couple of published papers.
A number of relationships between termination orderings and large ordinals have been found; this is only natural since any well-founded ordering corresponds to some ordinal. It is interesting that the recursive path ordering and other orderings provide intuitive and useful descriptions of large ordinals. For a discussion of this, see [75] and [73].
There has also been some work on modular properties of termination; for example, if one knows that R1 and R2 terminate, what can be said about the termination of R1 ∪ R2 under certain conditions? For a few examples of works along this line, see [258, 259, 182].

1.3.6 Equational Rewriting
There are two motivations for equational rewriting. The ﬁrst is that some rules are nonterminating and cannot be used with a conventional term rewriting system. One example is the commutative axiom x + y = y + x which is nonterminating no matter how it is oriented into a rewrite rule. The second reason is that if an operator like + is associative and commutative then there are many equivalent ways to represent terms like a + b + c + d. This imposes a burden in storage and time on a theorem prover or term rewriting system. Equational rewriting permits us to treat some axioms, like x + y = y + x, in a special way, avoiding problems with termination. It also permits us to avoid explicitly representing many equivalent forms of a term. The cost is a more complicated rewriting relation, more difﬁcult termination proofs, and a more complicated completion procedure. Indeed, signiﬁcant developments are still occurring in these areas, to attempt to deal with the problems involved. In equational rewriting, some equations are converted into rewrite rules R and others are treated as equations E. Typically, rules that terminate are placed in R and rules for which termination is difﬁcult are placed in E, especially if E uniﬁcation algorithms are known.
The general idea is to consider E-equivalence classes of terms instead of single terms. The E-equivalence classes consist of terms that are provably equal under E. For example, if E includes associative and commutative axioms for +, then the terms (a + b) + c, a + (b + c), c + (b + a), etc., will all be in the same E-equivalence class. Recall that s =E t if E |= s = t, that is, t can be obtained from s by replacing subterms using E. Note that =E is an equivalence relation. Usually some representation of the whole equivalence class is used; thus it is not necessary to store all the different terms in the class. This is a considerable savings in storage and time for term rewriting and theorem proving systems.
It is necessary to deﬁne a rewriting relation on E-equivalence classes of terms. If s is a term, let [s]E be its E-equivalence class, that is, the set of terms E-equivalent to s. The simplest approach is to say that [s]E → [t]E if s → t. Retracting this back to individual terms, one writes u →R/E v if there are terms s and t such that u =E s and v =E t and s →R t. This system R/E is called a class rewriting system. However, R/E rewriting turns out to be difﬁcult to compute, since it requires searching through all terms E-equivalent to u. A computationally simpler idea is to say that u → v if u has a subterm s such that s =E s and s →R t and v is u with s replaced by t. In this case one writes that u →R,E v. This system R, E is called the extended rewrite

V. Lifschitz, L. Morgenstern, D. Plaisted

51

system for R modulo E. Note that rules with E-equivalent left-hand sides need not be kept. The R, E rewrite relation only requires using the equational theory on the chosen redex s instead of the whole term, to match s with the left-hand side of some rule. Such E-matching is often (but not always, see [116]) easy enough computationally to make R, E rewriting much more efﬁcient than R/E rewriting. Unfortunately, →R/E has better logical properties for deciding R ∪ E equivalence. So the theory of equational rewriting is largely concerned with ﬁnding connections between these two rewriting relations.
Consider the systems R/E and R, E where R is {a ∗ b → d} and E consists of the associative and commutative axioms for ∗. Suppose s is (a ∗ c) ∗ b and t is c ∗ d. Then s →R/E t since s is E-equivalent to c ∗ (a ∗ b). However, it is not true that s →R,E t since there is no subterm of s that is E-equivalent to a ∗ b. Suppose s is (b ∗ a) ∗ c. Then s →R,E d ∗ c since b ∗ a is E-equivalent to a ∗ b.
Note that if E equivalence classes are nontrivial then it is impossible for class rewriting to be conﬂuent in the traditional sense (since any term E-equivalent to a normal form will also be a normal form of a term). So it is necessary to modify the definition to allow E-equivalent normal forms. We want to capture the property that class rewriting is conﬂuent when considered as a rewrite relation on equivalence classes. More precisely, R/E is (class) conﬂuent if for any term t, if t →∗R/E u and t →R∗ /E v then there are E-equivalent terms u and v such that u →∗R/E u and v →R∗ /E v . This implies that R/E is conﬂuent and hence Church–Rosser, considered as a rewrite relation on E-equivalence classes. If R/E is class conﬂuent and terminating then a term may have more than one normal form, but all of them will be E-equivalent. Furthermore, if R/E is class conﬂuent and terminating, then any R= ∪ E equivalent terms can be reduced to E equivalent terms by rewriting. Then an E-equivalence procedure can be used to decide R= ∪ E equivalence, if there is one. Note that E-equivalent rules need not both be kept, for this method.
R is said to be Church–Rosser modulo E if any two R= ∪ E-equivalent terms can be R, E rewritten to E-equivalent terms. This is not the same as saying that R/E is Church–Rosser, considered as a rewrite system on E-equivalence classes; in fact, it is a stronger property. Note that R, E rewriting is a subset of R/E rewriting, so if R/E is terminating, so is R, E. If R/E is terminating and R is Church–Rosser modulo E then R, E rewriting is also terminating and R= ∪ E-equality is decidable if Eequality is. Also, the computationally simpler R, E rewriting can be used to decide the equational theory. But Church–Rosser modulo E is not a local property; in fact it is undecidable in general. Therefore one desires decidable sufﬁcient conditions for it. This is the contribution of Jouannaud and Kirchner [130], using conﬂuence and “coherence”. The idea of coherence is that there should be some similarity in the way all elements of an E-equivalence class rewrite. Their conditions involve critical pairs between rules and equations and E-uniﬁcation procedures.
Another approach is to add new rules to R to obtain a logically equivalent system R /E; that is, R= ∪ E and R = ∪ E have the same logical consequences (i.e., they are equivalent), but R , E rewriting is the same as R/E rewriting. Therefore it is possible to use the computationally simpler R , E rewriting to decide the equality theory of R/E. This is done for associative–commutative operators by Peterson and Stickel [205]. In this case, conﬂuence can be decided by methods simpler than those of Jouannaud and Kirchner. Termination for equational rewriting systems is tricky to

52

1. Knowledge Representation and Classical Logic

decide; this will be discussed later. Another topic is completion for equational rewriting, adding rules to convert an equational rewriting system into a logically equivalent equational rewriting system with desired conﬂuence properties. This is discussed by Peterson and Stickel [205] and also by Jouannaud and Kirchner [130]; for earlier work along this line see [151, 152].

AC rewriting
We now consider the special case of rewriting relative to the associative and commutative axioms E = {f (x, y) = f (y, x), f (f (x, y), z) = f (x, f (y, z))} for a function symbol f . Special efﬁcient methods exist for this case. One idea is to modify the term structure so that R, E rewriting can be used rather than R/E rewriting. This is done by ﬂattening, that is a term f (s1, f (s2, . . . , f (sn−1, sn) . . .)), where none of the si have f as a top-level function symbol, is represented as f (s1, s2, . . . , sn). Here f is a vary-adic symbol, which can take a variable number of arguments. Similarly, f (f (s1, s2), s3) is represented as f (s1, s2, s3). This represents all terms that are equivalent up to the associative equation f (f (x, y), z) = f (x, f (y, z)) by the same term. Also, terms that are equivalent up to permutation of arguments of f are also considered as identical. This means that each E-equivalence class is represented by a single term. This also means that all members of a given E-equivalence class have the same term structure, making R, E rewriting seem more of a possibility. Note however that the subterm structure has been changed; f (s1, s2) is a subterm of f (f (s1, s2), s3) but there is no corresponding subterm of f (s1, s2, s3). This means that R, E rewriting does not simulate R/E rewriting on the original system. For example, consider the systems R/E and R, E where R is {a ∗ b → d} and E consists of the associative and commutative axioms for ∗. Suppose s is (a ∗ b) ∗ c and t is d ∗ c. Then s →R/E t; in fact, s →R,E t. However, if one ﬂattens the terms, then s becomes ∗(a, b, c) and s no longer rewrites to t since the subterm a ∗ b has disappeared.
To overcome this, one adds extensions to rewrite rules to simulate their effect on ﬂattened terms. The extension of the rule {a ∗ b → d} is {∗(x, a, b) → ∗(x, d)}, where x is a new variable. With this extended rule, ∗(a, b, c) rewrites to d ∗ c. The general idea, then, is to ﬂatten terms, and extend R by adding extensions of rewrite rules to it. Then, extended rewriting on ﬂattened terms using the extended R is equivalent to class rewriting on the original R. Formally, suppose s and t are terms and s and t are their ﬂattened forms. Suppose R is a term rewriting system and R is R with the extensions added. Suppose E is associativity and commutativity. Then s →R/E t iff s →R ,E t . The extended R is obtained by adding, for each rule of the form f (r1, r2, . . . , rn) → s where f is associative and commutative, an extended rule of the form f (x, r1, r2, . . . , rn) → f (x, s), where x is a new variable. The original rule is also retained. This idea does not always work on other equational theories, however. Note that some kind of associative–commutative matching is needed for extended rewriting. This can be fairly expensive, since there are so many permutations to consider, but it is fairly straightforward to implement. Completion relative to associativity and commutativity can be done with the ﬂattened representation; a method for this is given in [205]. This method requires associative–commutative uniﬁcation (see Section 1.3.6).

V. Lifschitz, L. Morgenstern, D. Plaisted

53

Other sets of equations
The general topic of completion for other equational theories was addressed by Jouannaud and Kirchner in [130]. Earlier work along these lines was done by Lankford, as mentioned above. Such completion procedures may use E-uniﬁcation. Also, they may distinguish rules with linear left-hand sides from other rules. (A term is linear if no variable appears more than once.)
AC termination orderings
We now consider termination orderings for special equational theories E. The problem is that E-equivalent terms are identiﬁed when doing equational rewriting, so that all Eequivalent terms have to be considered the same by the ordering. Equational rewriting causes considerable problems for the recursive path ordering and similar orderings. For example, consider the associative–commutative equations E. One can represent E-equivalence classes by ﬂattened terms, as mentioned above. However, applying the recursive path ordering to such terms violates monotonicity. Suppose ∗ > + and ∗ is associative–commutative. Then x ∗(y +z) > x ∗y +x ∗z. By monotonicity, one should have u ∗ x ∗ (y + z) > u ∗ (x ∗ y + x ∗ z). In fact, this fails; the term on the right is larger in the recursive path ordering. A number of attempts have been made to overcome this. The ﬁrst was the associative path ordering of Dershowitz, Hsiang, Josephson, and Plaisted [78], developed by the last author. This ordering applied to transformed terms, in which big operators like ∗ were pushed inside small operators like +. The ordering was not originally extended to non-ground terms, but it seems that it would be fairly simple to do so using the fact that a variable is smaller than any term properly containing it. A simpler approach to extending this ordering to non-ground terms was given later by Plaisted [209], and then further developed in Bachmair and Plaisted [12], but this requires certain conditions on the precedence. This work was generalized by Bachmair and Dershowitz [13] using the idea of “commutation” between two term rewriting systems. Later, Kapur [139] devised a fully general associative termination ordering that applies to non-ground terms, but may be hard to compute. Work in this area has continued since that time [146]. Another issue is the incorporation of status in such orderings, such as left-to-right, right-to-left, or multiset, for various function symbols. E-termination orderings for other equational theories may be even more complicated than for associativity and commutativity.
Congruence closure
Suppose one wants to determine whether E |= s = t where E is a set (conjunction) of ground equations and s and t are ground terms. For example, one may want to decide whether {f 5(c) = c, f 3(c) = c} |= f (c) = c. This is a case in which rewriting techniques apply but another method is more efﬁcient. The method is called congruence closure [191]; for some efﬁcient implementations and data structures see [81]. The idea of congruence closure is essentially to use equality axioms, but restricted to terms that appear in E, including its subterms. For the above problem, the following is a derivation of f (c) = c, identifying equations u = v and v = u:
1. f 5(c) = c (given).
2. f 3(c) = c (given).

54

1. Knowledge Representation and Classical Logic

3. f 4(c) = f (c) (2, using equality replacement). 4. f 5(c) = f 2(c) (3, using equality replacement). 5. f 2(c) = c (1, 4, transitivity). 6. f 3(c) = f (c) (5, using equality replacement). 7. f (c) = c (2, 6, transitivity). One can show that this approach is complete.

E-uniﬁcation algorithms
When the set of axioms in a theorem to be proved includes a set E of equations, it is often better to use specialized methods than general theorem proving techniques. For example, if the binary inﬁx operator ∗ is associative and commutative, many equivalent terms x ∗ (y ∗ z), y ∗ (x ∗ z), y ∗ (z ∗ x), etc. may be generated. These cannot be eliminated by rewriting since none is simpler than the others. Even the idea of using unorderable equations as rewrite rules when the applied instance is orderable, will not help. One approach to this problem is to incorporate a general E-uniﬁcation algorithm into the theorem prover. Plotkin [214] ﬁrst discussed this general concept and showed its completeness in the context of theorem proving. With E uniﬁcation built into a prover, only one representative of each E-equivalence class need be kept, signiﬁcantly reducing the number of formulas retained. E-uniﬁcation is also known as semantic uniﬁcation, which may be a misnomer since no semantics (interpretation) is really involved. The general idea is that if E is a set of equations, an E-uniﬁer of two terms s and t is a substitution Θ such that E |= sΘ = tΘ, and a most general E-uniﬁer is an E-uniﬁer that is as general as possible in a certain technical sense relative to the theory E. Many uniﬁcation algorithms for various sets of equations have been developed [239, 9]. For some theories, there may be at most one most general Euniﬁer, and for others, there may be more than one, or even inﬁnitely many, most general E-uniﬁers.
An important special case, already mentioned above in the context of termrewriting, is associative–commutative (AC) uniﬁcation. In this case, if two terms are E-uniﬁable, then there are at most ﬁnitely many most general E-uniﬁers, and there are algorithms to ﬁnd them that are usually efﬁcient in practice. The well-known algorithm of [251] essentially involves solving Diophantine equations and ﬁnding a basis for the set of solutions and ﬁnding combinations of basis vectors in which all variables are present. This can sometimes be very time consuming; the time to perform AC-uniﬁcation can be double exponential in the sizes of the terms being uniﬁed [137]. Domenjoud [80] showed that the two terms x + x + x + x and y1 + y2 + y3 + y4 have more than 34 billion different AC uniﬁers. Perhaps AC uniﬁcation algorithm is artiﬁcially adding complexity to theorem proving, or perhaps the problem of theorem proving in the presence of AC axioms is really hard, and the difﬁculty of the AC uniﬁcation simply reveals that. There may be ways of reducing the work involved in AC uniﬁcation. For example, one might consider resource bounded AC uniﬁcation, that is, ﬁnding all uniﬁers within some size bound. This might reduce the number of uniﬁers in cases where many of them are very large. Another idea is to consider “optional

V. Lifschitz, L. Morgenstern, D. Plaisted

55

variables”, that is, variables that may or may not be present. If x is not present in the product x ∗ y then this product is equivalent to y. This is essentially equivalent to introducing a new identity operator, and greatly reduces the number of AC uniﬁers. This approach has been studied by Domenjoud [79]. This permits one to represent a large number of solutions compactly, but requires one to keep track of optionality conditions.

Rule-based uniﬁcation
Uniﬁcation can be viewed as equation solving, and therefore is part of theorem proving or possibly logic programming. This approach to uniﬁcation permits conceptual simplicity and also is convenient for theoretical investigations. For example, unifying two literals P (s1, s2, . . . , sn) and P (t1, t2, . . . , tn) can be viewed as solving the set of equations {s1 = t1, s2 = t2, . . . , sn = tn}. Uniﬁcation can be expressed as a collection of rules operating on such sets of equations to either obtain a most general uniﬁer or detect non-uniﬁability. For example, one rule replaces an equation f (u1, u2, . . . , un) = f (v1, v2, . . . , vn) by the set of equations {u1 = v1, u2 = v2, . . . , un = vn}. Another rule detects non-uniﬁability if there is an equation of the form f (. . .) = g(. . .) for distinct f and g. Another rule detects non-uniﬁability if there is an equation of the form x = t where t is a term properly containing x. With a few more such rules, one can obtain a simple uniﬁcation algorithm that will terminate with a set of equations representing a most general uniﬁer. For example, the set of equations {x = f (a), y = g(f (a))} would represent the substitution {x ← f (a), y ← g(f (a))}. This approach has also been extended to E-uniﬁcation for various equational theories E. For a survey of this approach, see [133].

1.3.7 Other Logics
Up to now, we have considered theorem proving in general ﬁrst-order logic. However, there are many more specialized logics for which more efﬁcient methods exist. Such logics ﬁx the domain of the interpretation, such as to the reals or integers, and also the interpretations of some of the symbols, such as “+” and “∗”. Examples of theories considered include Presburger arithmetic, the ﬁrst-order theory of natural numbers with addition [200], Euclidean and non-Euclidean geometry [272, 55], inequalities involving real polynomials (for which Tarski ﬁrst gave a decision procedure) [52], ground equalities and inequalities, for which congruence closure [191] is an efﬁcient decision procedure, modal logic, temporal logic, and many more specialized logics. Theorem proving for ground formulas of ﬁrst-order logic is also known as satisﬁability modulo theories (SMT) in the literature. Description logics [8], discussed in Chapter 3 of this Handbook, are sublanguages of ﬁrst-order logic, with extensions, that often have efﬁcient decision procedures and have applications to the semantic web. Specialized logics are often built into provers or logic programming systems using constraints [33]. The idea of using constraints in theorem proving has been around for some time [143]. Another specialized area is that of computing polynomial ideals, for which efﬁcient methods have been developed [44]. An approach to combining decision procedures was given in [190] and there has been continued interest in the combination of decision procedures since that time.

56

1. Knowledge Representation and Classical Logic

Higher-order logic
In addition to the logics mentioned above, there are more general logics to consider, including higher-order logics. Such logics permit quantiﬁcation over functions and predicates, as well as variables. The HOL prover [101] uses higher-order logic and permits users to give considerable guidance in the search for a proof. Andrews’ TPS prover is more automatic, and has obtained some impressive proofs fully automatically, including Cantor’s theorem that the powerset of a set has more elements than the set. The TPS prover was greatly aided by a breadth-ﬁrst method of instantiating matings described in [31]. In general, higher-order logic often permits a more natural formulation of a theorem than ﬁrst-order logic, and shorter proofs, in addition to being more expressive. But of course the price is that the theorem prover is more complicated; in particular, higher-order uniﬁcation is considerably more complex than ﬁrst-order uniﬁcation.

Mathematical induction

Without going to a full higher-order logic, one can still obtain a considerable increase in power by adding mathematical induction to a ﬁrst-order prover. The mathematical induction schema is the following one:

∀y[[∀x((x < y) → P (x))] → P (y)]

∀yP (y)

.

Here < is a well-founded ordering. Specializing this to the usual ordering on the integers, one obtains the following Peano induction schema:

P (0), ∀x(P (x) → P (x + 1))

∀xP (x)

.

With such inference rules, one can, for example, prove that addition and multiplication are associative and commutative, given their straightforward deﬁnitions. Both of these induction schemas are second-order, because the predicate P is implicitly universally quantiﬁed. The problem in using these schemas in an automatic theorem prover is in instantiating P . Once this is done, the induction schema can often be proved by ﬁrst-order techniques. One way to adapt a ﬁrst-order prover to perform mathematical induction, then, is simply to permit a human to instantiate P . The problem of instantiating P is similar to the problem of ﬁnding loop invariants for program veriﬁcation.
By instantiating P is meant replacing P (y) in the above formula by A[y] for some ﬁrst-order formula A containing the variable y. Equivalently, this means instantiating P to the function λz.A[z]. When this is done, the ﬁrst schema above becomes

∀y[[∀x((x < y) → A[x])] → A[y]] .
∀yA[y]

Note that the hypothesis and conclusion are now ﬁrst-order formulas. This instantiated
induction schema can then be given to a ﬁrst-order prover. One way to do this is to have the prover prove the formula ∀y[[∀x((x < y) → A[x])] → A[y]], and then conclude ∀yA[y]. Another approach is to add the ﬁrst-order formula {∀y[[∀x((x < y) → A[x])] → A[y]]} → {∀yA[y]} to the set of axioms. Both approaches are facilitated by using a structure-preserving translation of these formulas to clause form,

V. Lifschitz, L. Morgenstern, D. Plaisted

57

in which the formula A[y] is deﬁned to be equivalent to P (y) for a new predicate symbol P .
A number of semi-automatic techniques for ﬁnding such a formula A and choosing the ordering < have been developed. One of them is the following: To prove that for all ﬁnite ground terms t, A[t], ﬁrst prove A[c] for all constant symbols c, and then for each function symbol f of arity n prove that A[t1] ∧ A[t2] ∧ · · · ∧ A[tn] → A[f (t1, t2, . . . , tn)]. This is known as structural induction and is often reasonably effective.
A common case when an induction proof may be necessary is when the prover is not able to prove the formula ∀xA[x], but the formulas A[t] are separately provable for all ground terms t. Analogously, it may not be possible to prove that ∀x(natural_number(x) → A[x]), but one may be able to prove A[0], A[1], A[2], . . . individually. In such a case, it is reasonable to try to prove ∀xA[x] by induction, instantiating P (x) in the above schema to A[x]. However, this still does not specify which ordering < to use. For this, it can be useful to detect how long it takes to prove the A[t] individually. For example, if the time to prove A[n] for natural number n is proportional to n, then one may want to try the usual (size) ordering on natural numbers. If A[n] is easy to prove for all even n but for odd n, the time is proportional to n, then one may try to prove the even case directly without induction and the odd case by induction, using the usual ordering on natural numbers.
The Boyer–Moore prover NqTHM [38, 36] has mathematical induction techniques built in, and many difﬁcult proofs have been done on it, generally with substantial human guidance. For example, correctness of AMD Athlon’s elementary ﬂoating point operations, and parts of IBM Power 5 and other processors have been proved on it. ACL2 [142, 141] is a software system built on Common Lisp related to NqTHM that is intended to be an industrial strength version of NqTHM, mainly for the purpose of software and hardware veriﬁcation. Boyer, Kaufmann, and Moore won the ACM Software System Award in 2005 for these provers. A number of other provers also have automatic or semi-automatic induction proof techniques. Rippling [47] is a technique originally developed for mathematical induction but which also has applications to summing series and general equational reasoning. The ground reducibility property is also often used for induction proofs, and has applications to showing the completeness of algebraic speciﬁcations [134]. A term is ground reducible by a term rewriting system R if all its ground instances are reducible by R. This property was ﬁrst shown decidable in [210], with another proof soon after in [138]. It was shown to be exponential time complete by Comon and Jacquemard [60]. However, closely related versions of this problem are undecidable. Recently Kapur and Subramaniam [140] described a class of inductive theorems for which validity is decidable, and this work was extended by Giesl and Kapur [97]. Bundy has written an excellent survey of inductive theorem proving [46] and the same handbook also has a survey of the so-called inductionless induction technique, which is based on completion of term-rewriting systems [59]; see also [127].
Set theory
Since most of mathematics can be expressed in terms of set theory, it is logical to develop theorem proving methods that apply directly to theorems expressed in set theory. Second-order provers do this implicitly. First-order provers can be used for set

58

1. Knowledge Representation and Classical Logic

theory as well; Zermelo–Fraenkel set theory consists of an inﬁnite set of ﬁrst-order axioms, and so one again has the problem of instantiating the axiom schemas so that a ﬁrst-order prover can be used. There is another version of set theory known as von Neumann–Bernays–Gödel set theory [37] which is already expressed in ﬁrst-order logic. Quite a bit of work has been done on this version of set theory as applied to automated deduction problems. Unfortunately, this version of set theory is somewhat cumbersome for a human or for a machine. Still, some mathematicians have an interest in this approach. There are also a number of systems in which humans can construct proofs in set theory, such as Mizar [260] and others [26, 219]. In fact, there is an entire project (the QED project) devoted to computer-aided translation of mathematical proofs into completely formalized proofs [218].
It is interesting to note in this respect that many set theory proofs that are simple for a human are very hard for resolution and other clause-based theorem provers. This includes theorems about the associativity of union and intersection, for example. In this area, it seems worthwhile to incorporate more of the simple deﬁnitional replacement approach used by humans into clause-based theorem provers.
As an example of the problem, suppose that it is desired to prove that ∀x((x ∩ x) = x) from the axioms of set theory. A human would typically prove this by noting that (x ∩ x) = x is equivalent to ((x ∩ x) ⊆ x) ∧ (x ⊆ (x ∩ x)), then observe that A ⊆ B is equivalent to ∀y((y ∈ A) → (y ∈ B)), and ﬁnally observe that y ∈ (x ∩ x) is equivalent to (y ∈ x) ∧ (y ∈ x). After applying all of these equivalences to the original theorem, a human would observe that the result is a tautology, thus proving the theorem.
But for a resolution theorem prover, the situation is not so simple. The axioms needed for this proof are
(x = y) ↔ [(x ⊆ y) ∧ (y ⊆ x)],
(x ⊆ y) ↔ ∀z((z ∈ x) → (z ∈ y)),
(z ∈ (x ∩ y)) ↔ [(z ∈ x) ∧ (z ∈ y)].
When these are all translated into clause form and Skolemized, the intuition of replacing a formula by its deﬁnition gets lost in a mass of Skolem functions, and a resolution prover has a much harder time. This particular example may be easy enough for a resolution prover to obtain, but other examples that are easy for a human quickly become very difﬁcult for a resolution theorem prover using the standard approach.
The problem is more general than set theory, and has to do with how deﬁnitions are treated by resolution theorem provers. One possible method to deal with this problem is to use “replacement rules” as described in [154]. This gives a considerable improvement in efﬁciency on many problems of this kind. Andrews’ matings prover has a method of selectively instantiating deﬁnitions [32] that also helps on such problems in a higher-order context. The U-rules of OSHL also help signiﬁcantly [184].

1.4 Applications of Automated Theorem Provers
Among theorem proving applications, we can distinguish between those applications that are truly automated, and those requiring some level of human intervention; be-

V. Lifschitz, L. Morgenstern, D. Plaisted

59

tween KR and non-KR applications; and between applications using classical ﬁrstorder theorem provers and those that do not. In the latter category fall applications using theorem proving systems that do not support equality, or allow only restricted languages such as Horn clause logic, or supply inferential procedures beyond those of classical theorem proving.
These distinctions are not independent. In general, applications requiring human intervention have been only slightly used for KR; moreover, KR applications are more likely to use a restricted language, or to use special-purpose inferential procedures.
It should be noted that any theorem proving system that can solve the math problems that form a substantial part of the TPTP (Thousands of Problems for Theorem Provers) testbed [255] must be a classical ﬁrst-order theorem prover that supports equality.

1.4.1 Applications Involving Human Intervention
Because theorem proving is in general intractable, the majority of applications of automated theorem provers require direction from human users in order to work. The intervention required can be extensive, e.g., the user may be required to supply lemmas to the proofs on which the automated theorem prover is working [84]. In the worst case, a user may be required to supply every step of a proof to an automated theorem prover; in this case, the automated theorem prover is functioning simply as a proof checker.
The need for human intervention has often limited the applicability of automated theorem provers to applications where reasoning can be done ofﬂine; that is, where the reasoner is not used as part of a real-time application. Even given this restriction, automated theorem provers have proved very valuable in a number of domains, including software development and veriﬁcation of software and hardware.
Software development
An example of an application to software development is the Amphion system, which was developed by Stickel et al. [250] and uses the SNARK theorem prover [249]. It has been used by NASA to compose programs out of a library of FORTRAN-77 subroutines. The user of Amphion, who does not have to have any familiarity with either theorem proving or the library subroutines, gives a graphical speciﬁcation; this speciﬁcation is translated into a theorem of ﬁrst-order logic; and SNARK provides a constructive proof of this theorem. This constructive proof is then translated into the application program in FORTRAN-77.
The NORA/HAMMR system [86] similarly determines what software components can be reused during program development. Each software component is associated with a contract written in a formal language which captures the essentials of the component’s behavior. The system determines whether candidate components have compatible contracts and are thus potentially reusable; the proof of compatibility is carried out using an automated theorem prover, though with a fair amount of human guidance. Automated theorem provers used for NORA/HAMMR include Setheo [158], Spass [268, 269], and PROTEIN [24], a theorem prover based on Mark Stickel’s PTTP [246, 248].
In the area of algorithm design and program analysis and optimization, KIDS (Kestrel Interactive Development System) [241] is a program derivation system that

60

1. Knowledge Representation and Classical Logic

uses automated theorem proving technology to facilitate the derivation of programs from high-level program speciﬁcations. The program speciﬁcation is viewed as a goal, and rules of transformational development are viewed as axioms of the system. The system, guided by the user, searches to ﬁnd the appropriate transformational rules, the application of which leads to the ﬁnal program. Both Amphion and KIDS require relatively little intervention from the user once the initial speciﬁcation is made; KIDS, for example, requires active interaction only for the algorithm design tactic.
Hardware and software veriﬁcation
Formal veriﬁcation of both hardware and software has been a particularly fruitful application of automated theorem provers. The need for veriﬁcation of program correctness had been noted as far back as the early 1960s by McCarthy [172], who suggested approaching the problem by stating a theorem that a program had certain properties—and in particular, computed certain functions—and then using an automated theorem prover to prove this theorem. Veriﬁcation of cryptographic protocols is another important subﬁeld of this area.
The ﬁeld of hardware veriﬁcation can be traced back to the design of the ﬁrst hardware description languages, e.g., ISP [27], and became active in the 1970s and 1980s, with the advent of VLSI design. (See, e.g, [22].) It gained further prominence after the discovery in 1994 [108] of the Pentium FDIV bug, a bug in the ﬂoating point unit of Pentium processors. It was caused by missing lookup table entries and led to incorrect results for some ﬂoating point division operators. The error was widespread, well-publicized, and costly to Intel, Pentium’s manufacturer, since it was obliged to offer to replace all affected Pentium processors.
General-purpose automated theorem provers that have been commonly used for hardware and/or software veriﬁcation include the following:
• The Boyer–Moore theorem provers NqTHM and ACL2 [36, 142] were inspired by McCarthy’s ﬁrst papers on the topic of verifying program correctness. As mentioned in the previous section, these award-winning theorem provers have been used for many veriﬁcation applications.
• The Isabelle theorem prover [203, 197] can handle higher-order logics and temporal logics. Isabelle is thus especially well-suited for cases where program speciﬁcations are written in temporal or dynamic logic (as is frequently the case). It has also been used for veriﬁcation of cryptographic protocols [242], which are frequently written in higher order and/or epistemic logics [49].
• OTTER has been used for a system that analyzes and detects attacks on security APIs (application programming interfaces) [273].
Special-purpose veriﬁcation systems which build veriﬁcation techniques on top of a theorem prover include the following:
• The PVS system [201] has been used by NASA’s SPIDER (Scalable Processor-Independent Design for Enhanced Reliability) to verify SPIDER protocols [206].
• The KIV (Karlsruhe Interactive Veriﬁer) has been used for a range of software veriﬁcation applications, including validation of knowledge-based systems [84].

V. Lifschitz, L. Morgenstern, D. Plaisted

61

The underlying approach is similar to that of the KIDS and Amphion projects in that ﬁrst, the user is required to enter a speciﬁcation; second, the user is entering a speciﬁcation of a modularized system, and the interactions between the modules; and third, the user works with the system to construct a proof of validity. More interaction between the user and the theorem prover seems to be required in this case, perhaps due to the increased complexity of the problem. KIV offers a number of techniques to reduce the burden on the user, including reuse of proofs and the generation of counterexamples.

1.4.2 Non-Interactive KR Applications of Automated Theorem Provers
McCarthy argued [171] for an AI system consisting of a set of axioms and an automated theorem prover to reason with those axioms. The ﬁrst implementation of this vision came in the late 1960s with Cordell Green’s question-answering system QA3 and planning system [103, 104]. Given a set of facts and a question, Green’s questionanswering system worked by resolving the (negated) question against the set of facts. Green’s planning system used resolution theorem proving on a set of axioms representing facts about the world in order to make simple inferences about moving blocks in a simple blocks-world domain. In the late 1960s and early 1970s, SRI’s Shakey project [195] attempted to use the planning system STRIPS [85] for robot motion planning; automated theorem proving was used to determine applicability of operators and differences between states [232]. The difﬁculties posed by the intractability of theorem proving became evident. (Shakey also faced other problems, including dealing with noisy sensors and incomplete knowledge. Moreover, the Shakey project does not actually count as a non-interactive application of automated theorem proving, since people could obviously change Shakey’s environment while it acted. Nonetheless, projects like these underscored the importance of dealing effectively with theorem proving’s essential intractability.)
In fact, there are today many fewer non-interactive than interactive applications of theorem proving, due to its computational complexity. Moreover, non-interactive applications will generally use carefully crafted heuristics that are tailored and ﬁne-tuned to a particular domain or application. Without such heuristics, the theorem-proving program would not be able to handle the huge number of clauses generated. Finally, as mentioned above, non-interactive applications often use ATPs that are not general theorem provers with complete proof procedures. This is because completeness and generality often come at the price of efﬁciency.
Some of the most successful non-interactive ATP applications are based on two theorem provers developed by Mark Stickel at SRI, PTTP [246, 248] and SNARK [249]. PTTP attempts to retain as much as possible the efﬁciency of Prolog (see Section 1.4.4 below) while it remedies the ways in which Prolog fails as a general-purpose theorem prover, namely, its unsound uniﬁcation algorithm, its incomplete search strategy, and its incomplete inference system. PTTP was used in SRI’s TACITUS system [121, 124], a message understanding system for reports on equipment failure, naval operations, and terrorist activities. PTTP was used speciﬁcally to furnish minimal-cost abductive explanations. It is frequently necessary to perform abduction—that is, to posit a likely explanation—when processing text. For example, to understand the sentence “The Boston ofﬁce called”, one must understand that the construct of metonymy

62

1. Knowledge Representation and Classical Logic

(the use of a single characteristic to identify an entity of which it is an attribute) is being used, and that what is meant is a person in the ofﬁce called. Thus, to understand the sentence we must posit an explanation of a person being in the ofﬁce and making that call.
There are usually many possible explanations that can be posited for any particular phenomenon; thus, the problem arises of choosing the simplest non-trivial explanation. (One would not, for example, wish to posit an explanation consistent with an ofﬁce actually being able to make a call.) TACITUS considers explanations of the form P (a), where ∀xP (x) → Q(x) and Q(a) are in the theory, and chooses the explanation that has minimal cost [247]. Every conjunct in the logical form of a sentence is given an assumability cost; this cost is passed back to antecedents in the Horn clause. Because of the way costs are propagated, the cost may be partly dependent on the length of the proofs of the literals in the explanation.
PTTP was also used in a central component of Stanford’s Logic-Based Subsumption Architecture for robot control [1], which was used to program a Nomad-200 robot to travel to different rooms in a multi-story building. The system employed a multi-layered architecture; in each layer, PTTP was used to prove theorems from the given axioms. Goals were transmitted to layers below or to robot manipulators.
PTTP is fully automated; the user has no control over the search for solutions. In particular, each rule is used in its original form and in its contrapositive. In certain situations, such as stating principles about substituting equals, reasoning with a contrapositive form can lead to considerable inefﬁciency.
Stickel’s successor theorem prover to PTTP, SNARK [249], gives users this control. It is more closely patterned after Otter; difﬁcult theorems that are intractable for PTTP can be handled by SNARK. It was used as the reasoning component for SRI’s participation in DARPA’s High-Performance Knowledge Bases (HPKB) Project [58], which focused on constructing large knowledge bases in the domain of crisis management; and developing question-answering systems for querying these knowledge bases. SNARK was used primarily in SRI’s question-answering portion of that system. SNARK, in contrast to what would have been possible with PTTP, allowed users to ﬁne tune the question-answering system for HPKB, by crafting an ordering of predicates and clauses on which resolution would be performed. This ordering could be modiﬁed as the knowledge base was altered. Such strategies were necessary to get SNARK to work effectively given the large size of the HPKB knowledge base.
For its use in the HPKB project, SNARK had to be extended to handle temporal reasoning.
SNARK has also been used for consistency checking of semantic web ontologies [20].
Other general-purpose theorem provers have also been used for natural language applications, though on a smaller scale and for less mature applications. Otter has been used in PENG (Processable English) [236], a controlled natural language used for writing precise speciﬁcations. Speciﬁcations in PENG can be translated into ﬁrst-order logic; Otter is then used to draw conclusions. As discussed in detail in Chapter 20, Bos and Markert [35] have used Vampire (as well as the Paradox model ﬁnder) to determine whether a hypothesis is entailed by some text.
The Cyc artiﬁcial intelligence project [157, 156, 169] is another widespread application of non-interactive automated theorem proving. The ultimate goal of Cyc is

V. Lifschitz, L. Morgenstern, D. Plaisted

63

the development of a comprehensive, encyclopedic knowledge base of commonsense facts, along with inference mechanisms for reasoning with that knowledge. Cyc contains an ontology giving taxonomic information about commonsense concepts, as well as assertions about the concepts.
Cyc’s underlying language, CycL, allows expression of various constructs that go beyond ﬁrst-order logic. Examples include:
• The concept of contexts [50]: one can state that something is true in a particular context as opposed to absolutely. (E.g., the statement that vampires are afraid of garlic is true in a mythological context, though not in real life.)
• Higher-order concepts. (E.g., one can state that if a relation is reﬂexive, symmetric, and transitive, it is an equivalence relation.)
• Exceptions. (E.g., one can say that except for Taiwan, all Chinese provinces are part of the People’s Republic of China.)
The Cyc knowledge base is huge. Nevertheless, it has been successfully used in real-world applications, including HPKB. (Cyc currently has over 3 million assertions; at the time of its use in HPKB, it had over a million assertions.) Theorem proving in Cyc is incomplete but efﬁcient, partly due to various special-purpose mechanisms for reasoning with its higher-order constructs. For example, Cyc’s reasoner includes a special module for solving disjointWith queries that traverses the taxonomies in the knowledge base to determine whether two classes have an empty intersection.
Ramachandran et al. [221, 220] compared the performance of Cyc’s reasoner with standard theorem provers. First, most of ResearchCyc’s knowledge base4 was translated into ﬁrst-order logic. The translated sentences were then loaded into various theorem provers, namely, Vampire, E [235], Spass, and Otter. The installations of Vampire and Spass available to Ramachandran et al. did not have sufﬁcient memory to load all assertions, necessitating performing the comparison of Cyc with these theorem provers on just 10 percent of ResearchCyc’s knowledge base. On sample queries—e.g., “Babies can’t be doctors”, “If the U.S. bombs Iraq, someone is responsible”, –Cyc proved to be considerably more efﬁcient. For example, for the query about babies and doctors, Cyc took 0.04 seconds to answer the query, while Vampire took 847.6 seconds.
Ramachandran and his colleagues conjecture that the disparity in performance partly reﬂects the fact that Cyc’s reasoner and the standard theorem provers have been designed for different sets of problems. General automated theorem provers have been designed to perform deep inference on small sets of axioms. If one looks at the problems in the TPTP database, they often have just a few dozen and rarely have more than a few hundred axioms. Cyc’s reasoner, on the other hand, has been designed to perform relatively shallow inference on large sets of axioms.
It is also worth noting that the greatest disparity of inference time between Cyc and the other theorem provers occurred when Cyc was using a special purpose reasoning module. In that sense, of course, purists might argue that Cyc is not really doing

4ResearchCyc [169] contains the knowledge base open to the public for research; certain portions of Cyc itself are not open to the public. The knowledge base of ResearchCyc contains over a million assertions.

64

1. Knowledge Representation and Classical Logic

theorem proving faster than standard ATPs; rather, it is doing something that is functionally equivalent to theorem proving while ATPs are doing theorem proving, and it is doing that something much faster.

1.4.3 Exploiting Structure
Knowledge bases for real-world applications and commonsense reasoning often exhibit a modular-like structure, containing multiple sets of facts with relatively little connection to one another. For example, a knowledge base in the banking domain might contain sets of facts concerning loans, checking accounts, and investment instruments; moreover, these sets of facts might have little overlap with one another. In such a situation, reasoning would primarily take place within a module, rather than between modules. Reasoning between modules would take place—for example, one might want to reason about using automated payments from a checking account to pay off installments on a loan—but would be limited. One would expect that a theorem prover that takes advantage of this modularity would be more efﬁcient: most of the time, it would be doing searches in reduced spaces, and it would produce fewer irrelevant resolvents.
A recent trend in automated reasoning focuses on exploiting structure of a knowledge base to improve performance. This section presents a detailed example of such an approach. Amir and McIlraith [2] have studied the ways in which a knowledge base can be automatically partitioned into loosely coupled clusters of domain knowledge, forming a network of subtheories. The subtheories in the network are linked via the literals they share in common. Inference is carried out within a subtheory; if a literal is inferred within one subtheory that links to another subtheory, it may be passed from the ﬁrst to the second subtheory.
Consider, from [2], the following theory specifying the workings of an espresso machine, and the preparation of espresso and tea: (Note that while this example is propositional, the theory is ﬁrst-order.)
(1) ¬ okpump ∨¬ onpump ∨ water
(2) ¬ manﬁll ∨ water
(3) ¬ manﬁll ∨¬ onpump
(4) manﬁll ∨ onpump
(5) ¬ water ∨¬ okboiler ∨¬ onboiler ∨ steam
(6) water ∨¬ steam
(7) okboiler ∨¬ steam
(8) onboiler ∨¬ steam
(9) ¬ steam ∨¬ cofee ∨ hotdrink
(10) coffee ∨ teabag
(11) ¬ steam ∨¬ teabag ∨ hotdrink

V. Lifschitz, L. Morgenstern, D. Plaisted

65

Intuitively, this theory can be decomposed into three subtheories. The ﬁrst, A1, containing axioms 1 through 4, regards water in the machine; it speciﬁes the relations between manually ﬁlling the machine with water, having a working pump, and having water in the machine. The second, A2, containing axioms 5 through 8, regards getting steam; it speciﬁes the relations between having water, a working boiler, the boiler switch turned on, and steam. The third, A3, containing axioms 9 through 11, regards getting a hot drink; it speciﬁes the relation between having steam, having coffee, having a teabag, and having a hot drink.
In this partitioning, the literal water links A1 and A2; the literal steam links A2 and A3. One can reason with logical partitions using forward message-passing of linking literals. If one asserts okpump, and performs resolution on the clauses of A1, one obtains water. If one asserts okboiler and onboiler in A2, passes water from A1 to A2, and performs resolution in A2, one obtains steam. If one passes steam to A3 and performs resolution in A3, one obtains hotdrink.
In general, the complexity of this sort of reasoning depends on the number of partitions, the size of the partitions, the interconnectedness of the subtheory graph, and the number of literals linking subtheories. When partitioning the knowledge base, one wants to minimize these parameters to the extent possible. (Note that one cannot simultaneously minimize all parameters; as the number of partitions goes down, the size of at least some of the partitions goes up.)
McIlraith et al. [165] did some empirical studies on large parts of the Cyc database used for HPKB, comparing the results of the SNARK theorem prover with and without this partitioning strategy. SNARK plus (automatically-performed) partitioning performed considerably better than SNARK with no strategy, though it was comparable to SNARK plus set-of-support strategies. When partitioning was paired with another strategy like set-of-support, it outperformed combinations of strategies without partitioning.
Clustering to improve reasoning performance has also been explored by Hayes et al. [115]. In a similar spirit, there has been growing interest in modularization of ontologies from the Description Logic and Semantic Web communities [267, 223, 102]. Researchers have been investigating how such modularization affects the efﬁciency of reasoning (i.e., performing subsumption and classiﬁcation, and performing consistency checks) over the ontologies.

1.4.4 Prolog
In terms of its use in working applications, the logic programming paradigm [147] represents an important success in automated theorem proving. Its main advantage is its efﬁciency; this makes it suitable for real-world applications. The most popular language for logic programming is Prolog [41].
What makes Prolog work so efﬁciently is a combination of the restricted form of ﬁrst-order logic used, and the particular resolution and search strategies that are implemented. In the simplest case, a Prolog program consists of a set of Horn clauses; that is, either atomic formulas or implications of the form (P1 ∧ P2 ∧ · · ·) → P0, where the Pi’s are all atomic formulas. This translates into having at most one literal in the consequence of any implication. The resolution strategy used is linear-input resolution: that is, for each resolvent, one of the parents is either in the initial database

66

1. Knowledge Representation and Classical Logic

or is an ancestor of the other parent. The search strategy used is backward-chaining; the reasoner backchains from the query or goal, against the sentences in the logic program.
The following are also true in the logic programming paradigm: there is a form of negation that is interpreted as negation-as-failure: that is, not a will be taken to be true if a cannot be proven; and the result of a logic program can depend on the ordering of its clauses and subgoals. Prolog implementations provide additional control mechanisms, including the cut and fail operators; the result is that few programs in Prolog are pure realizations of the declarative paradigm. Prolog also has an incomplete mechanism for uniﬁcation, particularly of arithmetic expressions.
Prolog has been widely used in developing expert systems, especially in Europe and Japan, although languages such as Java and C++ have become more popular.
Examples of successful practical applications of logic programming include the HAPPS system for model house conﬁguration [83] and the Munich Rent Advisor [90], which calculates the estimated fair rent for an apartment. (This is a rather complex operation that can take days to do by hand.) There has been special interest in the last decade on world-wide web applications of logic programming (see Theory and Practice of Logic Programming, vol. 1, no. 3).
What are the drawbacks to Prolog? Why is there continued interest in the signiﬁcantly less efﬁcient general theorem provers?
First, the restriction to Horn clause form is rather severe; one may not be able to express knowledge crucial for one’s application. An implication whose conclusion is a disjunction is not expressible in Horn clause form. This means, for example, that one cannot represent a rule like
If you are diagnosed with high-blood pressure, you will either have to reduce your salt intake or take medication
because that is most naturally represented as an implication with a disjunction in the consequent.
Second, Prolog’s depth-ﬁrst-search strategy is incomplete. Third, because, in most current Prolog implementations, the results of a Prolog program depend crucially on the ordering of its clauses, and because it is difﬁcult to predict how the negation-as-failure mechanism will interact with one’s knowledge base and goal query, it may be difﬁcult to predict a program’s output. Fourth, since Prolog does not support inference with equality, it cannot be used for mathematical theorem proving. There has been interest in the logic programming community in addressing limitations or perceived drawbacks of Prolog. Disjunctive logic programming [6] allows clauses with a disjunction of literals in the consequent of a rule. Franconi et al. [88] discusses one application of disjunctive logic programming, the implementation of a clean-up procedure prior to processing census data. The fact that logic programs may have unclear or ambiguous semantics has concerned researchers for decades. This has led to the development of answer set programming, discussed in detail in Chapter 7, in which logic programs are interpreted with the stable model semantics. Answer set programming has been used for many applications, including question answering, computational biology, and system validation.

V. Lifschitz, L. Morgenstern, D. Plaisted

67

1.5 Suitability of Logic for Knowledge Representation
The central tenet of logicist AI5—that knowledge is best represented using formal logic—has been debated as long as the ﬁeld of knowledge representation has existed. Among logicist AI’s strong advocates are John McCarthy [171, 175], Patrick Hayes [112, 114, 111], and Robert Moore [186]; critics of the logicist approach have included Yehoshua Bar-Hillel [21], Marvin Minsky [185], Drew McDermott [180], and Rodney Brooks [42]. (McDermott can be counted in both the logicist and anti-logicist camps, having advocated for and contributed to logicist AI [178, 181, 179] before losing faith in the enterprise.)
The crux of the debate is simply this: Logicists believe that ﬁrst-order logic, along with its modiﬁcations, is a language particularly well suited to capture reasoning, due to its expressivity, its model-theoretic semantics, and its inferential power. Note [112] that it is not a particular syntax for which logicists argue; it is the notion of a formal, declarative semantics and methods of inference that are important. (See [95, 64, 233, 39] for examples of how AI logicism is used.) Anti-logicists have argued that the program, outside of textbook examples, is undesirable and infeasible. To paraphrase McDermott [180], You Don’t Want To Do It, and You Can’t Do It Anyway.
This handbook clearly approaches AI from a logicist point of view. It is nevertheless worthwhile examining the debate in detail. For it has not consisted merely of an ongoing sequence of arguments for and against a particular research approach. Rather, the arguments of the anti-logicists have proved quite beneﬁcial for the logicist agenda. The critiques have often been recognized as valid within the logicist community; researchers have applied themselves to solving the underlying difﬁculties; and in the process have frequently founded productive subﬁelds of logicist AI, such as nonmonotonic reasoning. Examining the debate puts into context the research in knowledge representation that is discussed in this Handbook.

1.5.1 Anti-logicist Arguments and Responses In the nearly ﬁfty years since McCarthy’s Advice Taker paper ﬁrst appeared [171], the criticisms against the logicist approach have been remarkably stable. Most of the arguments can be characterized under the following categories:
• Deductive reasoning is not enough.
• Deductive reasoning is too expensive.
• Writing down all the knowledge (the right way) is infeasible.
• Other approaches do it better and/or cheaper.

The argument: Deductive reasoning is not enough
McCarthy’s original logicist proposal called for the formalization of a set of commonsense facts in ﬁrst-order logic, along with an automated theorem prover to reason with

5The term logicism generally refers to the school of thought that mathematics can be reduced to logic [270], logicists to the proponents of logicism. Within the artiﬁcial intelligence community, however, a logicist refers to a proponent of logicist AI, as deﬁned in this section [257].

68

1. Knowledge Representation and Classical Logic

those facts. He gave as an example the reasoning task of planning to get to the airport. McCarthy argued that starting out from facts about ﬁrst, the location of oneself, one’s car, and the airport; second, how these locations relate to one another; third, the feasibility of certain actions, such as walking and driving; fourth, the effects that actions had; and ﬁfth, basic planning constructs, one could deduce that to get to the airport, one should walk to one’s car and drive the car to the airport. There were, all together, just 15 axioms in this draft formalization.
Bar-Hillel argued:
It sounds rather incredible that the machine could have arrived at its conclusion—which, in plain English, is “Walk from your desk to your car!”— by sound deduction! This conclusion surely could not possibly follow from the premise in any serious sense. Might it not be occasionally cheaper to call a taxi and have it take you over to the airport? Couldn’t you decide to cancel your ﬂight or to do a hundred other things?

The need for nonmonotonic reasoning
In part, Bar-Hillel was alluding to the many exceptions that could exist in any realistically complex situation. Indeed, it soon became apparent to AI researchers that exceptions exist for even simple situations and facts. The classic example is that of reasoning that a bird can ﬂy. Birds typically can ﬂy, although there are exceptions, such as penguins and birds whose wings are broken. If one wants to formalize a theory of bird ﬂying, one cannot simply write

∀x(Bird(x) → Flies(x))

(1.17)

because that would mean that all birds ﬂy. That would be wrong, because it does not take penguins and broken-winged birds into account. One could instead write

∀x(Bird(x) ∧ ¬Penguin(x) ∧ ¬Brokenwinged(x) → Flies(x))

(1.18)

which says that all birds ﬂy, as long as they are not penguins or broken-winged, or better yet, from the representational point of view, the following three formulas:

∀x(Bird(x) ∧ ¬Ab(x) → Flies(x)), ∀x(Penguin(x) → Ab(x)), ∀x(Brokenwinged(x) → Ab(x))

(1.19) (1.20) (1.21)

which say that birds ﬂy unless they are abnormal, and that penguins and brokenwinged birds are abnormal.
A formula in the style of (1.18) is difﬁcult to write, since one needs to state all possible exceptions to bird ﬂying in order to have a correct axiom. But even aside from the representational difﬁculties, there is a serious inferential problem. If one only knows that Tweety is a bird, one cannot use axiom (1.18) in a deductive proof. One needs to know as well that the second and third conjuncts on the left-hand side of the implication are true: that is, that Tweety is not a penguin and is not brokenwinged. Something stronger than deduction is needed here; something that permits jumping to the conclusion that Tweety ﬂies from the fact that Tweety is a bird and the

V. Lifschitz, L. Morgenstern, D. Plaisted

69

absence of any knowledge that would contradict this conclusion. This sort of default reasoning would be nonmonotonic in the set of axioms: adding further information (e.g., that Tweety is a penguin) could mean that one has to retract conclusions (that is, that Tweety ﬂies).
The need for nonmonotonic reasoning was noted, as well, by Minsky [185]. At the time Minsky wrote his critique, early work on nonmonotonicity had already begun. Several years later, most of the major formal approaches to nonmonotonic reasoning had already been mapped out [173, 224, 181]. This validated both the logicist AI approach, since it demonstrated that formal systems could be used for default reasoning, and the anti-logicists, who had from the ﬁrst argued that ﬁrst-order logic was too weak for many reasoning tasks.
Nonmonotonicity and the anti-logicists
From the time they were ﬁrst developed, nonmonotonic logics were seen as an essential logicist tool. It was expected that default reasoning would help deal with many KR difﬁculties, such as the frame problem, the problem of efﬁciently determining which things remain the same in a changing world. However, it turned out to be surprisingly difﬁcult to develop nonmonotonic theories that entailed the expected conclusions. To solve the frame problem, for example, one needs to formalize the principle of inertia—that properties tend to persist over time. However, a naive formalization of this principle along the lines of [174] leads to the multiple extension problem; a phenomenon in which the theory supports several models, some of which are unintuitive. Hanks and McDermott [110] demonstrated a particular example of this, the Yale shooting problem. They wrote up a simple nonmonotonic theory containing some general facts about actions (that loading a gun causes the gun to be loaded, and that shooting a loaded gun at someone causes that individual to die), the principle of inertia, and a particular narrative (that a gun is loaded at one time, and shot at an individual a short time after). The expected conclusion, that the individual will die, did not hold. Instead, Hanks and McDermott got multiple extensions: the expected extension, in which the individual dies; and an unexpected extension, in which the individual survives, but the gun mysteriously becomes unloaded. The difﬁculty is that the principle of inertia can apply either to the gun remaining loaded or the individual remaining alive. Intuitively we expect the principle to be applied to the gun remaining loaded; however, there was nothing in Hank’s and McDermott’s theory to enforce that.
The Yale shooting problem was not hard to handle: solutions began appearing shortly after the problem became known. (See [160, 161, 238] for some early solutions.) Nonetheless, the fact that nonmonotonic logics could lead to unexpected conclusions for such simple problems was evidence to anti-logicists of the infeasibility of logicist AI. Indeed, it led McDermott to abandon logicist AI. Nonmonotonic logic was essentially useless, McDermott argued [180], claiming that it required one to know beforehand what conclusions one wanted to draw from a set of axioms, and to build that conclusion into the premises.
In contrast, what logicist AI learned from the Yale shooting problem was the importance of a good underlying representation. The difﬁculty with Hanks and McDermott’s axiomatization was not that it was written in a nonmonotonic logic; it was that it was devoid of a concept of causation. The Yale shooting problem does not arise in an axiomatization based on a sound theory of causation [243, 187, 237].

70

1. Knowledge Representation and Classical Logic

From today’s perspective, the Yale shooting scenario is rather trivial. Over the last ten years, research related to the frame problem has concentrated on more elaborate kinds of action domains—those that include actions with indirect effects, nondeterministic actions, and interacting concurrently executed actions. Efﬁcient implementations of such advanced forms of nonmonotonic reasoning have been used in serious industrial applications, such as the design of a decision support system for the Space Shuttle [198].
The current state of research on nonmonotonic reasoning and the frame problem is described in Chapters 6, 7, and 16–20 of this Handbook.

The need for abduction and induction
Anti-logicists have pointed out that not all commonsense reasoning is deductive. Two important examples of non-deductive reasoning are abduction, explaining the cause of a phenomenon, and induction, reasoning from speciﬁc instances of a class to the entire class. Abduction, in particular, is important for both expert and commonsense reasoning. Diagnosis is a form of abduction; understanding natural language requires abduction as well [122].
Some philosophers of science [215, 117, 118] have suggested that abduction can be grounded in deduction. The idea is to hypothesize or guess an explanation for a particular phenomenon, and then try to justify this guess using deduction. A wellknown example of this approach is known as the deductive-nomological hypothesis.
McDermott [180] has argued against such attempts, pointing out what has been noted by philosophers of science [234]: the approach is overly simplistic, can justify trivial explanations, and can support multiple explanations without offering a way of choosing among candidates. But he was tilting at a straw man. In fact, the small part of logicist AI that has focused on abduction has been considerably more sophisticated in its approach. As discussed in the previous section, Hobbs, Stickel, and others have used theorem proving technology to support abductive reasoning [247, 122], but they do it by carefully examining the structure of the generated proofs, and the particular context in which the explanandum occurs. There is a deliberate and considered approach toward choosing among multiple explanations and toward ﬁltering out trivial explanations.
There is also growing interest in inductive logic programming [189]. This ﬁeld uses machine learning techniques to construct a logic program that entails all the positive and none of the negative examples of a given set of examples.

The argument: Deductive reasoning is too expensive
The decisive question [is] how a machine, even assuming it will have somehow countless millions of facts stored in its memory, will be able to pick out those facts which will serve as premises for its deduction.
Yehoshua Bar-Hillel [21]
When McCarthy ﬁrst presented his Advice Taker paper and Bar-Hillel made the above remark, automated theorem proving technology was in its infancy: resolution theorem proving was still several years away from being invented. But even with relatively advanced theorem proving techniques, Bar-Hillel’s point remains. General

V. Lifschitz, L. Morgenstern, D. Plaisted

71

automated theorem proving programs frequently cannot handle theories with several hundred axioms, let alone several million.
This point has in fact shaped much of the AI logicist research agenda. The research has progressed along several fronts. There has been a large effort to make general theorem proving more efﬁcient (this is discussed at length in Section 1.3); special-purpose reasoning techniques have been developed, e.g., by the description logic community [11] as well as by Cyc (see Section 1.4.2) to determine subsumption and disjointness of classes; and logic programming techniques (for both Prolog (see Section 1.4.4) and answer set programming (see Chapter 7)) have been developed so that relatively efﬁcient inferences can be carried out under certain restricted assumptions. The HPKB project and Cyc demonstrate that at least in some circumstances, inference is practical even with massively large knowledge bases.
The argument: Writing down all the knowledge (the right way) is infeasible
Just constructing a knowledge base is a major intellectual research problem . . . The problem of ﬁnding suitable axioms—the problem of “stating the facts” in terms of always-correct, logical, assumptions—is very much harder than is generally believed.
Marvin Minsky [185].
The problem is in fact much greater than Minsky realized, although it has taken AI logicists a while to realize the severity of the underlying issues. At the time that Minsky wrote his paper, his critique on this point was not universally appreciated by proponents of AI logicism. The sense one gets from reading the papers of Pat Hayes [113, 114, 111],6 for example, is one of conﬁdence and optimism. Hayes decried the paucity of existing domain formalizations, but at the time seemed to believe that creating the formalizations could be done as long as enough people actually sat down to write the axioms. He proposed, for the subﬁeld of naive physics, that a committee be formed, that the body of commonsense knowledge about the physical world be divided into clusters, with clusters assigned to different committee members, who would occasionally meet in order to integrate their theories.
But there never was a concerted effort to formalize naive physics. Although there have been some attempts to formalize knowledge of various domains (see, e.g., [123], and the proceedings of the various symposia on Logical Formalizations of Commonsense Knowledge), most research in knowledge representation remains at the metalevel. The result, as Davis [65] has pointed out, is that at this point constructing a theory that can reason correctly about simple tasks like staking plants in a garden is beyond our capability.
What makes it so difﬁcult to write down the necessary knowledge? It is not, certainly, merely the writing down of millions of facts. The Cyc knowledge base, as discussed in Section 1.4, has over 3 million assertions. But that knowledge base is still missing the necessary information to reason about staking plants in a garden, cracking eggs into a bowl, or many other challenge problems in commonsense reasoning and knowledge representation [183]. Size alone will not solve the problem. That is why attempts to use various web-based technologies to gather vast amounts of knowledge [170] are irrelevant to this critique of the logicist approach.
6Although [111] was published in the 1980s, a preliminary version was ﬁrst written in the late 1970s.

72

1. Knowledge Representation and Classical Logic

Rather, formalizing domains in logic is difﬁcult for at least the following reasons:
• First, it is difﬁcult to become aware of all our implicit knowledge; that is, to make this knowledge explicit, even in English or any other natural language. The careful examination of many domains or non-trivial commonsense reasoning problems makes this point clear. For example, reasoning about how and whether to organize the giving of a surprise birthday present [188] involves reasoning about the factors that cause a person to be surprised, how surprises can be foiled, joint planning, cooperation, and the importance of correct timing. The knowledge involved is complex and needs to be carefully teased out of the mass of social protocols that unknowingly govern our behavior.
• Second, as Davis [65] has pointed out, there is some knowledge that is difﬁcult to express in any language. Davis gives the example of reasoning about a screw. Although it is easy to see that a small bump in the surface will affect the functionality of a screw much more than a small pit in the surface, it is hard to express the knowledge needed to make this inference.
• Third, there are some technical difﬁculties that prevent formalization of certain types of knowledge. For example, there is still no comprehensive theory of how agents infer and reason about other agents’ ignorance (although [109] is an excellent start in this direction); this makes it difﬁcult to axiomatize realistic theories of multi-agent planning, which depend crucially on inferring what other agents do and do not know, and how they make up for their ignorance.
• Fourth, the construction of an ontology for a domain is a necessary but difﬁcult prerequisite to axiomatization. Deciding what basic constructs are necessary and how to organize them is a tricky enterprise, which often must be reworked when one starts to write down axioms and ﬁnds that it is awkward to formalize the necessary knowledge.
• Fifth, it is hard to integrate existing axiomatizations. Davis gives as an example his axiomatizations of string, and of cutting. There are various technical difﬁculties—mainly, assumptions that have been built into each domain axiomatization—that prevent a straightforward integration of the two axiomatizations into a single theory that could support simple inferences about cutting string. The problem of integration, in simpler form, will also be familiar to anyone who has ever tried to integrate ontologies. Concepts do not always line up neatly; how one alters these concepts in order to allow subsumption is a challenging task.
There have nonetheless been many successes in writing down knowledge correctly. The best known are the theories of causation and temporal reasoning that were developed in part to deal with the frame and Yale shooting problems. Other successful axiomatizations, including theories of knowledge and belief, multiple agency, spatial reasoning, and physical reasoning, are well illustrated in the domain theories in this Handbook.

V. Lifschitz, L. Morgenstern, D. Plaisted

73

The argument: Other approaches do it better and/or cheaper
Anyone familiar with AI must realize that the study of knowledge representation—at least as it applies to the “commonsense” knowledge required for reading typical text such as newspapers—is not going anywhere fast. This subﬁeld of AI has become notorious for the production of countless non-monotonic logics and almost as many logics of knowledge and belief, and none of the work shows any obvious application to actual knowledge-representation problems.
Eugene Charniak [54]
During the last ﬁfteen years, statistical learning techniques have become increasingly popular within AI, particularly for applications such as natural language processing for which classic knowledge representation techniques had once been considered essential. For decades, for example, it had been assumed that much background domain knowledge would be needed in order to correctly parse sentences. For instance, a sentence like John saw the girl with the toothbrush has two parses, one in which the prepositional phrase with the toothbrush modiﬁes the phrase John saw, and one in which it modiﬁes the noun phrase the girl. Background knowledge, however, eliminates the ﬁrst parse, since people do not see with toothbrushes. (In contrast, both parses are plausible for the sentence John saw the girl with the telescope.) The difﬁculty with KR-based approaches is that it requires a great deal of knowledge to properly process even small corpora of sentences.
Statistical learning techniques offers a different paradigm for many issues that arise in processing language. One useful concept is that of collocation [166], in which a program learns about commonly occurring collocated words and phrases, and subsequently uses this knowledge in order to parse. This is particularly useful for parsing and disambiguating phonemes for voice recognition applications. A statistical learning program might learn, for example, that weapons of mass destruction are words that are collocated with a high frequency. If this knowledge is then fed into a voice recognition program, it could be used to disambiguate between the words math and mass. The words in the phrase Weapons of math destruction are collocated with a low frequency, so that interpretation becomes less likely.
Programs using statistical learning techniques have become popular in textretrieval applications; in particular, they are used in systems that have performed well in recent TREC competitions [262–266]. Statistical-learning systems stand out because they are often cheaper to build. There is no need to painstakingly build tailormade knowledge bases for the purposes of understanding a small corpora of texts.
Nevertheless, it is unlikely that statistical-learning systems will ever obviate the need for logicist AI in these applications. Statistical techniques can go only so far. They are especially useful in domains in which language is highly restricted (e.g., newspaper texts, the example cited by Charniak), and for applications in which deep understanding is not required. But for many true AI applications, such as story understanding and deep question-answering applications, deep understanding is essential.
It is no coincidence that the rising popularity of statistical techniques has coincided with the rise of the text-retrieval competitions (TREC) as opposed to the message-understanding competitions (MUC). It is also worth noting that the successful participants in HPKB relied heavily on classical logicist KR techniques [58].

74

1. Knowledge Representation and Classical Logic

In general, this pattern appears in other applications. Statistical learning techniques do well with low cost on relatively easy problems. However, hard problems remain resistant to these techniques. For these problems, logicist-KR-based techniques appear to work best.
This may likely mean that the most successful applications in the future will make use of both approaches. As with the other critiques discussed above, the logicist research agenda is once again being set and inﬂuenced by non-logicist approaches; ultimately, this can only serve to strengthen the applicability of the logicist approach and the success of logicist-based applications.

Acknowledgements
The comments of Eyal Amir, Peter Andrews, Peter Baumgartner, Ernie Davis, Esra Erdem, Joohyung Lee, Christopher Lynch, Bill McCune, Sheila McIlraith, J. Moore, Maria Paola Bonacina, J. Hsiang, H. Kirchner, M. Rusinowitch, and Geoff Sutcliffe contributed to the material in this chapter. The ﬁrst author was partially supported by the National Science Foundation under Grant IIS-0412907.

Bibliography
[1] E. Amir and P. Maynard-Reid. Logic-based subsumption architecture. Artiﬁcial Intelligence, 153(1–2):167–237, 2004.
[2] E. Amir and S. McIlraith. Partition-based logical reasoning for ﬁrst-order and propositional theories. Artiﬁcial Intelligence, 162(1–2):49–88, 2005.
[3] P.B. Andrews. Theorem proving via general matings. Journal of the ACM, 28:193–214, 1981.
[4] P.B. Andrews, M. Bishop, S. Issar, D. Nesmith, F. Pfenning, and H. Xi. TPS: A theorem proving system for classical type theory. Journal of Automated Reasoning, 16:321–353, 1996.
[5] P.B. Andrews and C.E. Brown. TPS: A hybrid automatic-interactive system for developing proofs. Journal of Applied Logic, 4:367–395, 2006.
[6] C. Aravindan, J. Dix, and I. Niemelä. Dislop: A research project on disjunctive logic programming. AI Commun., 10(3–4):151–165, 1997.
[7] T. Arts and J. Giesl. Termination of term rewriting using dependency pairs. Theoretical Computer Science, 236(1–2):133–178, 2000.
[8] F. Baader, D. Calvanese, D.L. McGuinness, D. Nardi, and P.F. Patel-Schneider. The Description Logic Handbook: Theory, Implementation, Applications. Cambridge University Press, Cambridge, UK, 2003.
[9] F. Baader and W. Snyder. Uniﬁcation theory. In A. Robinson and A. Voronkov, editors. Handbook of Automated Reasoning, vol. I, pages 445–532. Elsevier Science, 2001 (Chapter 8).
[10] F. Baader, editor. CADE-19, 19th International Conference on Automated Deduction, Miami Beach, FL, USA, July 28–August 2, 2003. Lecture Notes in Computer Science, vol. 2741. Springer, 2003.
[11] F. Baader and T. Nipkow. Term Rewriting and All That. Cambridge University Press, Cambridge, England, 1998.

