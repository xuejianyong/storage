Available online at www.sciencedirect.com

Multiplicity of control in the basal ganglia: computational roles of striatal subregions Aaron M Bornstein and Nathaniel D Daw

The basal ganglia, in particular the striatum, are central to theories of behavioral control, and often identiÔ¨Åed as a seat of action selection. Reinforcement learning (RL) models ‚Äî which have driven much recent experimental work on this region ‚Äî cast striatum as a dynamic controller, integrating sensory and motivational information to construct efÔ¨Åcient and enriching behavioral policies. BeÔ¨Åtting this informationally central role, the BG sit at the nexus of multiple anatomical ‚Äòloops‚Äô of synaptic projections, connecting a wide range of cortical and subcortical structures. Numerous pioneering anatomical studies conducted over the past several decades have meticulously catalogued these loops, and labeled them according to the inferred functions of the connected regions. The speciÔ¨Åc cotermina of the projections are highly localized to several different subregions of the striatum, leading to the suggestion that these subregions perform complementary but distinct functions. However, until recently, the dominant computational framework outlined only a bipartite, dorsal/ventral, division of striatum. We review recent computational and experimental advances that argue for a more Ô¨Ånely fractionated delineation. In particular, experimental data provide extensive insight into unique functions subserved by the dorsomedial striatum (DMS). These functions appear to correspond well with theories of a ‚Äòmodel-based‚Äô RL subunit, and may also shed light on the suborganization of ventral striatum. Finally, we discuss the limitations of these ideas and how they point the way toward future reÔ¨Ånements of neurocomputational theories of striatal function, bringing them into contact with other areas of computational theory and other regions of the brain.
Address Center for Neural Science and Psychology Department, New York University, 4 Washington Place, New York, NY 10003, United States
Current Opinion in Neurobiology 2011, 21:374‚Äì380
This review comes from a themed issue on Behavioral and cognitive neuroscience Edited by Ann Graybiel and Richard Morris
Available online 21st March 2011
0959-4388/$ ‚Äì see front matter # 2011 Elsevier Ltd. All rights reserved.
DOI 10.1016/j.conb.2011.02.009
Introduction Perhaps more than any other brain areas, recent advances in the understanding of the basal ganglia (BG) have been driven by computational models. This is largely because of the fact that core functions commonly ascribed to the BG ‚Äî action selection and value learning ‚Äî have been

the subject of intensive study in both economics and computer science, particularly the subÔ¨Åeld of artiÔ¨Åcial intelligence known as reinforcement learning (RL) [1]. Theories from these areas propose mathematical deÔ¨Ånitions for quantities relevant to these functions and stepby-step procedures for computing them. Accordingly, these models have rapidly progressed from general frameworks for interpreting data toward playing a more integral quantitative role in experimental design and analysis, and now often serve as explicit hypotheses about trial-by-trial Ô¨Çuctuations in biological signals, such as action potentials or blood oxygenation level dependent (BOLD) signals. The poster children for this approach are the inÔ¨Çuential, albeit controversial, temporal difference (TD) learning models, which describe a reward prediction error (RPE) signal that has proved a strong match to phasic Ô¨Åring of midbrain dopamine neurons as well as BOLD in the ventral striatum (VS) [2‚Äì7]. The present review considers recent work that has expanded upon this initial achievement, shedding further light on the computational and functional suborganization of striatum, and then considers questions raised by this work in light of other empirical data and computational modeling that, together, point the way for future work in this area.
The suborganization of striatum Although anatomical considerations such as the topographical gradient of afferents from cortex to striatum have long suggested considerable functional heterogeneity (for instance, Ô¨Åve distinct corticostriatal loops in one seminal review [8]), there have until recently been surprisingly few clear correlates of this presumptive suborganization in unit recordings or functional neuroimaging within BG. In parallel, there have been few functional subdivisions suggested among the dominant computational models to motivate or guide the search for such proliÔ¨Åc variegation.
In RL models, the primary early progress on this question was the functional breakdown between learning to predict rewards (critic) and, guided by these predictions, learning advantageous stimulus‚Äìaction policies (actor) [2]. This was suggested [3] to correspond to a two-module breakdown in striatum between motoric actor functions dorsally and evaluative critic functions ventrally, an idea which resonates with data both from rodent lesions [9,10] and from human functional neuroimaging [11‚Äì13].
The actor/critic formalizes a classic psychological distinction between Pavlovian learning (about stimulus‚Äìoutcome relationships) and instrumental learning (about which actions are advantageous). However, psychologists

Current Opinion in Neurobiology 2011, 21:374‚Äì380

www.sciencedirect.com

Multiplicity of control in the basal ganglia Bornstein and Daw 375

have long known that these functions are further decomposable. Notably, instrumental learning comprises subtypes that guide actions using different learned representations: ‚Äòhabitual‚Äô actions based on stimulus‚Äì response associations versus ‚Äògoal-directed‚Äô actions supported by the representation of the particular goal (such as food) expected for an action [14,15]. This distinction is typically probed using manipulations that alter action‚Äì outcome contingency or outcome value, such as reward devaluation, and then evaluating the subsequent effect on responding. After extensive training, behavior can become insensitive to these manipulations, suggesting an isolated reliance on stimulus‚Äìresponse representations, that is, habits.
A raft of recent theoretical work exploits a parallel distinction in RL models [16‚Äì22]. TD theories, such as the actor/critic, correspond well to classical ideas about stimulus‚Äìresponse habits and how they are reinforced [23,24], but these ‚Äòmodel-free‚Äô algorithms cannot explain behavioral phenomena associated with goal-directed behavior such as devaluation sensitivity or latent learning [25]. However, an additional family of ‚Äòmodel-based‚Äô RL algorithms describe how learning about the structure of an environment can be used to evaluate candidate actions online. This evaluation, typically implemented by some sort of simulation, inference or preplay using a forward model of the task (analogous to an action‚Äìoutcome representation or cognitive map) [26‚Äì29], can plan new actions or re-evaluate old ones drawing on information other than simple reinforcement history.
Much recent experimental research has been guided by the discovery, via lesions of rodent striatum, that these behaviors are supported by distinct subregions of dorsal striatum, lateral and medial (DLS and DMS) [30‚Äì33]. The proposal that a model-free TD actor in DLS is accompanied by an additional model-based RL system in DMS has considerable promise. First, it preserves the substantial successes of the TD/dopamine models while correcting some of their shortcomings: for instance its redundancy of control may help to explain why lesions to dopaminergic nuclei do not prevent all instrumental learning [34]. Conversely, just as TD models have helped to shed light on dopaminergic habit mechanisms, modelbased RL may provide a framework for understanding neural mechanisms for goal-directed evaluation.
Such work is at a very early stage; although putative correlates of model-based computations have been reported throughout a very wide network [35‚Äì 39,40,41‚Äì43], probably the most developed data thus far concern spiking correlates for both prospective locations (in hippocampus) and associated rewards (in VS), suggesting a circuit for model-based evaluation of candidate trajectories [26,44,45]. Complementary to such ‚Äòpreplay‚Äô phenomena, ‚Äòreplay‚Äô of neural sequences

may play a similar, but ofÔ¨Çine, role in updating stored (e.g. model-free) value predictions [46‚Äì49], perhaps by sampling model-based trajectories estimated from a cognitive map [50].
Theories of model-based and model-free RL in the basal ganglia envision parallel circuits. This is consistent with Ô¨Åndings from lesion studies that the two learning processes appear to evolve side-by-side [30,31], even though they tend to dominate behavior serially: progressing, with training, from goal-directed to habitual responding. Several features of DMS and DLS unit recordings appear to reÔ¨Çect these differential temporal dynamics, with taskrelated responsivity in DMS peaking early in training (and in retraining, following task changes) [51,52]; as well as with changes in ensemble responses [53] and several measures of synaptic potentiation [54] peaking earlier in DMS than DLS.
The last results [54] resonate with at least two other predictions from RL models. The authors measure a greater concentration of dopamine D2 receptors on neurons in DLS (relative to DMS). That these receptors are uniquely sensitive to extrasynaptic tonic dopamine concentrations (which are considerably lower than those resulting from phasic bursts) supports a previously posited computational role for tonic dopamine in the modulation and motivational control of habitual expression [18]. Further, the D2-containing neurons (which are known to primarily project into the ‚Äòindirect‚Äô, striatopallidal, pathway) were also a primary site of synaptic potentiation during behavioral training. This observation is consistent with a body of computational and experimental work suggesting that these receptors are involved in learning as well as expression, perhaps speciÔ¨Åcally in learning which actions to avoid [55,56].
Questions and anomalies At the same time, many of these studies point to three serious questions for the RL models: on their overall architecture, their mechanisms for learning, and how they are deployed during choice.
Architecture and the model-based critic First, the basic project of rescuing model-free actor/critic theories of the dopamine system by augmenting them with a separate and parallel model-based RL system is challenged by a number of recent results suggesting that even areas associated with the putatively model-free critic (including ventral striatum [42,44,45], downstream ventral pallidum [39], and RPE units in the dopaminergic midbrain [59]) all show properties such as sensitivity to devaluation that are indicative of model-based RL and not easily explained by the standard model-free TD theories (see also [60]). These results suggest that the two hypothetical systems are either more interacting or hybrid than separate, consistent with the

www.sciencedirect.com

Current Opinion in Neurobiology 2011, 21:374‚Äì380

376 Behavioral and cognitive neuroscience

overlapping ‚Äòloop‚Äô architecture suggested by anatomical studies [8,61]. An intriguing alternate suggestion is that the ventral striatal critic also consists of dissociable subcomponents for model-based and model-free Pavlovian evaluation (Figure 1). Indeed, psychologists distinguish preparatory and consummatory forms of Pavlovian conditioning, which may involve distinct circuits in the core and shell of VS [33,62,63,64]. This distinction again closely tracks that between model-based and model-free
Figure 1

DLS

actor
DMS
core
critic

shell

model-free model-based

medial

lateral
Current Opinion in Neurobiology

RL, in that consummatory Pavlovian responses reÔ¨Çect knowledge of the particular outcome expected (suggesting they are derived from predictions using a world model), whereas preparatory responses, like a model-free critic‚Äôs predictions, are not outcome-speciÔ¨Åc [65].
Learning and hierarchical RL A related issue is that the considerable algorithmic differences between model-based and model-free RL approaches seem poorly matched to the basic isomorphism of circuitry between different parts of the BG [8]. While the model-free actor and critic both learn from the same error signal operating on different inputs ‚Äî thought to be consistent with a similar dopaminergic input driving plasticity in both ventral and dorsolateral striatum ‚Äî the representations used for model-based RL seem to require quite different teaching signals and learning rules [41], offering no obvious role, in these theories, for a dopaminergic RPE in DMS.
One possible direction for resolving this question arises from a somewhat different empirical and theoretical take on the function of DLS, in the ‚Äòchunking‚Äô of behavioral sequences. DLS neurons (and, importantly, not those in DMS) tend with training to cluster their responsivity at the beginning and end of a trial [53], and lesions of DLS [66] (and of a prefrontal area that may be afferent to it [32]) also suggest a causal role in behavioral chunking. In RL, such chunking of actions into multiaction ‚Äòoptions‚Äô is formalized by a family of ‚Äòhierarchical‚Äô RL models [67]. Taken as an organizing principle for striatum (e.g. with policies operating on elemental actions represented in DMS and, moving laterally, policies on progressively more chunked options), this model has the appealing feature that all levels of the hierarchy learn from a similar (in this case, model-free) RPE signal and a common learning rule.

The dual-actor/critic framework. The dorsal/ventral divide of the actor/ critic model is extended to include recent theoretical and experimental advances supporting further functional subdivisions of each region (after Yin et al. [33]). These parallel circuits implement different approaches to reinforcement learning, either ‚Äòmodel-free‚Äô (dark grey) or ‚Äòmodel-based‚Äô (light grey). The dorsal region is now divided medial/lateral (though a gradient may be more accurate [57]), each supporting a different ‚Äòactor‚Äô submodule: a dorsolateral area, supporting a model-free actor; and the dorsomedial region, a substrate for representations that enable modelbased planning. Further, current evidence suggests that the ventral region may itself be functionally subdivided, along the boundary between nucleus accumbens ‚Äòcore‚Äô and ‚Äòshell‚Äô. These regions are each crucial for different forms of Pavlovian responding: preparatory and consummatory, respectively. Computationally, they correspond to model-based and model-free critics, computing the net present expected value of the current state using, respectively, either state-value mappings (purely based on reinforcement history) or state-outcome and outcome-value predictions derived from a world model. Thus, modelbased RL may offer a process-level description of striatal subregion function that encompasses both goal-directed instrumental and consummatory Pavlovian behaviors, extending the common account of habitual action and preparatory Pavolovian responses embodied in the original, model-free, actor/critic formulation. Schematic coronal slice of rat striatum modified with permission from Paxinos and Watson [58].

However, although there appears to be some informal resonance between a stimulus‚Äìresponse habit and an automatized behavioral sequence, in RL, the inclusion of options generally crosscuts the distinction between model-based and model-free evaluative strategies. Since the latter distinction has been used to explain the signature phenomena (such as devaluation sensitivity) tying DMS and DLS to goal-directed and habitual instrumental behaviors, additional theoretical work will be needed to understand if these two approaches can be blended, for example using model-based hierarchical RL, to formalize both chunking and devaluation phenomena together.
Arbitration and choice under uncertainty A third major question raised by theories involving multiple, parallel reinforcement learners is how the brain arbitrates between the two systems‚Äô choices. One theoretical proposal is that the predictions of model-free and model-based reinforcement learners may be competitively combined based on the uncertainty about their predictions

Current Opinion in Neurobiology 2011, 21:374‚Äì380

www.sciencedirect.com

Multiplicity of control in the basal ganglia Bornstein and Daw 377

[17]. In population code representations, uncertainty may be carried by the entropy of neuronal Ô¨Åring across the population [68,69]. Indeed, over training [53] differential modulations in population entropy were observed in DLS and DMS, with the DMS representation becoming structured more quickly but ultimately overtaken in this measure by DLS.
Nevertheless, this theory has little to say about the more dynamic processes or mechanisms by which the brain combines these uncertain estimates. The accumulation of multiple noisy evidence sources has, however, been studied extensively in another heretofore largely distinct area of theoretical and experimental work on decision making about noisy sensory displays. Here, reaction times, errors, and ramping activity of neurons in posterior parietal cortex are famously captured by Bayesian models of the accumulation of evidence about stimulus identity [68,70]. According to these models, sensory decision regions interpret stimuli by drawing sucessive samples of noisy sensory input as represented in upstream sensory cortices (which may themselves incorporate a sensory prior that develops to match the distribution of natural stimuli [71]). This work is rapidly coming directly into contact with research on RL and the BG for a number of reasons.
For one, the success of these Bayesian sequential sampling models is not limited to purely sensory tasks involving the analysis of noisy percepts. Notably, they also capture human behavior in tasks involving more affective, value-driven choices, such as pricing or choosing between snack foods [20,72,73]. Thus, goal-directed valuation, too, may involve accumulating stochastic samples, here presumably drawn from memory rather than a noisy percept [74‚Äì77]. This implies a rather different mechanism for model-based evaluation than the more systematic tree search or Bayesian graph inference so far hypothesized [17,28], though perhaps one not incompatible with the relatively noisy preplay phenomena observed neurally [45,50,78]. Such a procedure for computing model-based values could incorporate uncertainty-weighted habit information (e.g. as a prior), suggesting a dynamic solution to the arbitration problem. In all these respects, it is interesting that in a sensory decision task, primate caudate neurons display activity related to evidence accumulation not unlike that seen in parietal cortex [79].
These models, Ô¨Ånally, speak to the BG‚Äôs connections with a broader anatomical and computational universe. Typical sensory and RL decision tasks exercise almost entirely complementary functions: in one case, analyzing a noisy sensory stimulus with the response rule well deÔ¨Åned, and, in the other, Ô¨Åguring out which candidate response is most valuable with no perceptual uncertainty. There has been considerable interest in how

the neurocomputational mechanisms that have been characterized for each function, separately, might interact in more complicated tasks involving both value learning and perceptual (or, in RL terms, ‚Äòstate‚Äô) uncertainty [80,81,82,83]. Models based on RL for partially observable Markov decision processes (POMDP) [80,82,84] suggest that these two mechanisms can operate serially: a cortical perceptual inference module infers a distribution over possible stimuli and this serves as input for a BG RL module operating much as before. Such a model explains dopaminergic responses in a perceptual inference task as related to prediction error as the cortical model ‚ÄòÔ¨Ågures out‚Äô whether the animal is facing an easy (likely rewarded) or hard trial [82,85]. This approach may also provide a route toward explaining dopaminergic responses related to seeking information about future reward prospects [86].
Last, since the perceptual system, on this view, must in general base its percepts on learning about the statistical structure of the task and percepts, this idea situates the RL circuit alongside substantial recent work on Bayesian models of learning latent structure [87‚Äì90]. More generally, such structure learning is a valuable component of an efÔ¨Åcient model-based system. Indeed, in realistic RL tasks involving perceptual uncertainty, such latent structure learning is also an important component of learning the world model for model-based RL. Thus, exploring how inference guides the construction and employment of associative representations may ultimately provide a synthesis between cortical belief computations and model-based striatal RL.
Acknowledgements
The authors are supported by a Scholar Award from the McKnight Foundation, a NARSAD Young Investigator Award, Human Frontiers Science Program Grant RGP0036/2009-C, and NIMH grant 1R01MH087882-01, part of the CRCNS program. We thank Fenna Krienen, Amitai Shenhav, Dylan Simon and Elliott Wimmer for helpful conversations.
References and recommended reading
Papers of particular interest, published within the period of review, have been highlighted as:
 of outstanding interest
1. Sutton R, Barto A: Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press; 1998:. ISBN 0262193981.
2. Barto AC: Adaptive critics and the basal ganglia. In Models of Information Processing in the Basal Ganglia. Edited by Houk JC, Davis JL, Beiser DG. Cambridge, MA: MIT Press; 1995. pp. 215‚Äì 232.
3. Montague PR, Dayan P, Sejnowski TJ: A framework for mesencephalic dopamine systems based on predictive Hebbian learning. J Neurosc 1996, 76:1936-1947.
4. O‚ÄôDoherty JP, Dayan P, Friston K, Critchley H, Dolan RJ: Temporal difference models and reward-related learning in the human brain. Neuron 2003, 28:329-337.
5. McClure S, Berns G, Montague P: Temporal prediction errors in a passive learning task activate human striatum. Neuron 2003, 38:339-346.

www.sciencedirect.com

Current Opinion in Neurobiology 2011, 21:374‚Äì380

378 Behavioral and cognitive neuroscience

6. Ito M, Doya K: Validation of decision-making models and analysis of decision variables in the rat basal ganglia. J Neurosci 2009, 29:9861-9874 doi: 10.1523/JNEUROSCI.615708.2009.
7. Kim H, Sul JH, Huh N, Lee D, Jung MW: Role of striatum in updating values of chosen actions. J Neurosci 2009, 29:1470114712 doi: 10.1523/JNEUROSCI.2728-09.2009.
8. Alexander GE, Delong MR, Strick PL: Parallel organization of functionally segregated circuits linking basal ganglia and cortex. Ann N Y Acad Sci 1986, 9:351-381.
9. Packard M, Knowlton B: Learning and memory functions of the basal ganglia. Annu Rev Neurosci 2002, 25:563-593.
10. Cardinal R, Parkinson J, Lachenal G, Halkerston K, Rudarakanchana N, Hall J, Morrison C, Howes S, Robbins T, Everitt B: Effects of selective excitotoxic lesions of the nucleus accumbens core, anterior cingulate cortex, and central nucleus of the amygdala on autoshaping performance in rats. Behav Neurosci 2002, 116:553-567.
11. O‚ÄôDoherty JP, Dayan P, Schultz J, Deichmann R, Friston K, Dolan RJ: Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science 2004, 304:452-454 doi: 10.1126/science.1094285.
12. Tricomi E, Delgado MR, Fiez JA: Modulation of caudate activity by action contingency. Neuron 2004, 41:281-292.
13. Tricomi E, Balleine BW, O‚ÄôDoherty JP: A speciÔ¨Åc role for posterior dorsolateral striatum in human habit learning. Eur J Neurosci 2009, 29:2225-2232 doi: 10.1111/j.14609568.2009.06796.x.
14. Adams C, Dickinson A: Instrumental responding following reinforcer devaluation. Q J Exp Psychol Sect B 1981, 33:109-121.
15. Adams C: Variations in the sensitivity of instrumental responding to reinforcer devaluation. Q J Exp Psychol Sect B 1982, 34:77-98.
16. Doya K: What are the computations of the cerebellum, the basal ganglia and the cerebral cortex? Neural Netw 1999, 12:961-974.
17. Daw ND, Niv Y, Dayan P: Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nat Neurosci 2005, 8:1704-1711.
18. Niv Y, Joel D, Dayan P: A normative perspective on motivation. Trends Cogn Sci 2006, 10:375-381 doi: 10.1016/ j.tics.2006.06.010.
19. Redish A, Jensen S, Johnson A: Addiction as vulnerabilities in the decision process. Behav Brain Sci 2008, 31:461-487.
20. Rangel A, Camerer C, Montague P: A framework for studying the neurobiology of value-based decision making. Nat Rev Neurosci 2008, 9:545-556.
21. Balleine B, Daw N, O‚ÄôDoherty J: Multiple forms of value learning and the function of dopamine. In Neuroeconomics: Decision Making and the Brain. Edited by Glimcher P et al.: Academic Press; 2008. pp. 365‚Äì388.
22. Frank MJ, Doll BB, Oas-Terpstra J, Moreno F: Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation. Nat Neurosci 2009, 12:1062-1068 doi: 10.1038/nn.2342.
23. Suri R, Schultz W: A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task. Neuroscience 1999, 91:871-890.
24. Maia T: Two-factor theory, the actor/critic model, and conditioned avoidance. Learn Behav 2010, 38:1.
25. Tolman EC: Cognitive maps in rats and men. Psychol Rev 1948, 55:189-208.
26. Johnson A, van der Meer M, Redish AD: Integrating hippocampus and striatum in decision-making. Curr Opin Neurobiol 2007:692-697 doi: 10.1016/j.conb.2008.01.003.
Current Opinion in Neurobiology 2011, 21:374‚Äì380

27. Addis D, Pan L, Vu M, Laiser N, Schacter D: Constructive episodic simulation of the future and the past: distinct subsystems of a core brain network mediate imagining and remembering. Neuropsychologia 2009, 47:2222-2238.
28. Botvinick M, An J: Goal-directed decision making in prefrontal cortex: a computational framework. In Advances in Neural Information Processing Systems (NIPS). Edited by Koller D, Bengio Y, Schuurmans D, Bouttou L, Culotta A. 2008. pp. 169‚Äì176.
29. Fermin A, Yoshida T, Ito M, Yoshimoto J, Doya K: Evidence for model-based action planning in a sequential Ô¨Ånger movement task. J Motor Behav 2010, 42:371-379.
30. Yin HH, Knowlton BJ, Balleine BW: Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning. Eur J Neurosci 2004, 19:181189 doi: 10.1046/j.1460-9568.2003.03095.
31. Yin HH, Ostlund SB, Knowlton BJ, Balleine BW: The role of the dorsomedial striatum in instrumental conditioning. Eur J Neurosci 2005, 22:513-523 doi: 10.1111/j.1460-9568.2005.04218.
32. Balleine BW, Liljeholm M, Ostlund SB: The integrative function of the basal ganglia in instrumental conditioning. Behav Brain Res 2009, 199:43-52 doi: 10.1016/j.bbr.2008.10.034.
33. Yin HH, Ostlund SB, Balleine BW: Reward-guided learning beyond dopamine in the nucleus accumbens: the integrative functions of cortico-basal ganglia networks. Eur J Neurosci 2008, 28:1437-1448 doi: 10.1111/j.1460-9568.2008.06422.x. Reward-guided.
34. Berridge K: The debate over dopamine‚Äôs role in reward: the case for incentive salience. Psychopharmacology 2007, 191:391-431.
35. Valentin V, Dickinson A, O‚ÄôDoherty J: Determining the neural substrates of goal-directed learning in the human brain. J Neurosci 2007, 27:4019-4026.
36. Frank M, Moustafa A, Haughey H, Curran T, Hutchison K: Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning. Proc Natl Acad Sci U S A 2007, 104:16311-16316.
37. Hampton AN, Bossaerts P, O‚ÄôDoherty JP: The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J Neurosci 2006, 26:8360-8367.
38. Hampton A, Bossaerts P, O‚ÄôDoherty J: Neural correlates of mentalizing-related computations during strategic interactions in humans. Proc Natl Acad Sci U S A 2008, 105:6741-6746.
39. Tindell A, Smith K, Berridge K, Aldridge J: Dynamic computation of incentive salience: wanting what was never liked. J Neurosci 2009, 29:12220-12228.
40. den Ouden HEM, Daunizeau J, Roiser J, Friston KJ, Stephan KE:  Striatal prediction error modulates cortical coupling. J
Neurosci 2010, 30:3210-3219 doi: 10.1523/JNEUROSCI.445809.2010. Using a dynamic causal model (DCM) approach, the researchers demonstrate that striatal prediction errors inÔ¨Çuence, in a trial-by-trial fashion, the degree of functional coupling between visual and motor regions during a serial response task, with greater prediction error leading to stronger connectivity. Similar to the work of Law and Gold [83], this study supports a role for the basal ganglia in establishing and tuning cortico-cortico connectivity, via a prediction error signal.
41. Gla¬® scher J, Daw ND, Dayan P, O‚ÄôDoherty JP: States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron 2010, 66:585-595 doi: 10.1016/ j.neuron.2010.04.016.
42. Simon ND, Daw ND: Neural correlates of forward planning in a spatial decision task in humans. J Neurosci; in press.
43. Daw ND, Gershman SJ, Seymour B, Dayan P, Dolan RJ: Modelbased inÔ¨Çuences on humans choices and striatal prediction errors. Neuron; in press, doi:10.1016/j.neuron.2011.02.02.
www.sciencedirect.com

Multiplicity of control in the basal ganglia Bornstein and Daw 379

44. van der Meer M, Redish A: Covert expectation-of-reward in rat  ventral striatum at decision points. Front Integr Neurosci 2009,
3:. This study examines the activity of ventral striatal neurons at decision points in a T-maze. These neurons signaled expectation of future reward at choice points, periods during which hippocampal signals have been found to preferentially reactivate candidate paths in the course of deliberation. These reward reactivations decreased with experience (and performance), consistent with accounts of reduced deliberation over experience, and suggestive of a transfer from deliberative, putatively goal-directed, processes to more automatic, habitual control.
45. Maa van der Meer A, Johnson NC, Schmitzer-Torbert A, Redish D: Triple dissociation of information processing in dorsal striatum, ventral striatum, and hippocampus on a learned spatial decision task. Neuron 2010, 67:25-32 doi: 10.1016/ j.neuron.2010.06.023.
46. Johnson A, Redish AD: Hippocampal replay contributes to within session learning in a temporal difference reinforcement learning model. Neural Networks 2005, 18:1163-1171 doi: 10.1016/j.neunet.2005.08.009.
47. Foster D, Wilson M: Reverse replay of behavioural sequences in hippocampal place cells during the awake state. Nature 2006, 440:680-683.
48. Diba K, Buzsa¬¥ ki G: Forward and reverse hippocampal place-cell sequences during ripples. Nat Neurosci 2007, 10:1241-1242.
49. Peyrache A, Khamassi M, Benchenane K, Wiener S, Battaglia F: Replay of rule-learning related neural patterns in the prefrontal cortex during sleep. Nat Neurosci 2009, 12:919-926.
50. Gupta A, van der Meer M, Touretzky D, Redish A: Hippocampal replay is not a simple function of experience. Neuron 2010, 65:695-705.
51. Kimchi EY, Laubach M: The dorsomedial striatum reÔ¨Çects response bias during learning. J Neurosci 2009, 29:1489114902 doi: 10.1523/JNEUROSCI.4060-09.2009.
52. Kimchi EY, Laubach M: Dynamic encoding of action selection by the medial striatum. J Neurosci 2009, 29:3148-3159 doi: 10.1523/JNEUROSCI.5206-08.2009.
53. Thorn Ca, Atallah H, Howe M, Graybiel AM: Differential dynamics  of activity changes in dorsolateral and dorsomedial striatal
loops during learning. Neuron 2010, 66:781-795 doi: 10.1016/ j.neuron.2010.04.036. This study examined simultaneous neural activity in DMS and DLS while rodents learned a conditional T-maze task. Across the population, DMS activity reached peak structure midway through the task, while DLS structure continued to increase. The authors suggest that DLS patterns do not effect behavioral control until DMS activity becomes less prevalent, relative to DLS.
54. Yin HH, Costa RM: Dynamic reorganization of striatal circuits during the acquisition and consolidation of a skill. Nat Neurosci 2009, 12:333-342.
55. Frank M, Doll B, Oas-Terpstra J, Moreno F: Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation. Nat Neurosci 2009, 12:1062-1068.
56. Frank M, Hutchison K: Genetic contributions to avoidancebased decisions: striatal D2 receptor polymorphisms. Neuroscience 2009, 164:131-140.
57. Voorn P, Vanderschuren LJMJ, Groenewegen HJ, Robbins TW, Pennartz CMA: Putting a spin on the dorsal‚Äìventral divide of the striatum. Trends Neurosci 2004, 27:468-474 doi: 10.1016/ j.tins.2004.06.006.
58. Paxinos G, Watson C: The Rat Brain in Stereotaxic Coordinates, 6th edition. Academic Press; 2007:. ISBN: 0125476124.
59. Bromberg-Martin ES, Matsumoto M, Hikosaka O: Distinct tonic and phasic anticipatory activity in lateral habenula and dopamine neurons. Neuron 2010, 67:144-155 doi: 10.1016/ j.neuron.2010.06.031.
60. Zhang J, Berridge KC, Tindell AJ, Smith KS, Aldridge JW: A neural computational model of incentive salience. PLoS Comput Biol 2009, 5:e1000437 doi: 10.1371/journal.pcbi.1000437.

61. Joel D, Weiner I: The connections of the dopaminergic system in rats and primates: an analysis with respect to the functional and compartmental organization of the striatum. Neuroscience 2000, 96:451-474.
62. Zahm DS: The evolving theory of basal forebrain functionalanatomical ‚Äòmacrosystems‚Äô. Neurosci Biobehav Rev 2006, 30:148-172 doi: 10.1016/j.neubiorev.2005.06.003.
63. Bouret S, Richmond BJ: Ventromedial and orbital prefrontal neurons differentially encode internally and externally driven motivational values in monkeys. J Neurosci 2010, 30:8591-8601 doi: 10.1523/JNEUROSCI.0049-10.2010.
64. ShiÔ¨Çett M, Balleine B: At the limbic-motor interface: disconnection of basolateral amygdala from nucleus accumbens core and shell reveals dissociable components of incentive motivation. Eur J Neurosci 2010:1735-1743.
65. Daw N, Courville A, Dayan P: Semi-rational models of conditioning: the case of trial order. In The Probabilistic Mind: Prospects for Bayesian Cognitive Science. Edited by Chater N, Oaksford M. Oxford: Oxford University Press; 2008. pp. 431‚Äì452.
66. Yin HH: The sensorimotor striatum is necessary for serial order learning. J Neurosci 2010, 30:14719-14723 doi: 10.1523/ JNEUROSCI.3989-10.2010.
67. Botvinick MM, Niv Y, Barto AC: Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective. Cognition 2009, 113:262-280 doi: 10.1016/ j.cognition.2008.08.011.
68. Beck JM, Ma WJ, Kiani R, Hanks T, Churchland AK, Roitman J, Shadlen MN, Latham PE, Pouget A: Probabilistic population codes for Bayesian decision making. Neuron 2008, 60:11421152 doi: 10.1016/j.neuron.2008.09.021.
69. Kiani R, Shadlen M: Representation of conÔ¨Ådence associated  with a decision by neurons in the parietal cortex. Science 2009,
324:759-764. The authors demonstrate that, while performing a saccadic decision task using noisy sensory input, monkey lateral intraparietal neurons encode a measure of decision conÔ¨Ådence, reÔ¨Çected behaviorally by the monkey‚Äôs preference for a small but certain alternate option. Across the recorded population, these trials of less-conÔ¨Ådent decisions were accompanied by a more homogeneous, medial level of activity, relative to the more peaked distributions observed when the monkey waived the certain option for one of the two gamble targets.
70. Gold J, Shadlen M: Banburismus and the brain: decoding the relationship between sensory stimuli, decisions, and reward. Neuron 2002, 36:299-308.
71. Berkes P, Orba¬¥ n G, Lengyel M, Fiser J: Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science 2011, 331:83-87.
72. Krajbich I, Armel C, Rangel A: Visual Ô¨Åxations and the computation and comparison of value in simple choice. Nat Neurosci 2010, 13:1292-1298.
73. Armel K, Beaumel A, Rangel A: Biasing simple choices by manipulating relative visual attention. Judg Decis Mak 2008, 3:396-403.
74. Erev I, Ert E, Yechiam E: Loss aversion, diminishing sensitivity, and the effect of experience on repeated decisions. J Behav Decis Mak 2008, 21:575-597.
75. Stewart N, Chater N, Brown G: Decision by sampling. Cogn Psychol 2006, 53:1-26.
76. Lengyel M, Dayan P: Hippocampal contributions to control: the third way. Adv Neural Inform Process Syst 2008, 20:889-896.
77. Constantino S, Daw N: A closer look at choice. Nat Neurosci 2010, 13:1153-1154.
78. Lansink CS, Goltstein PM, Lankelma JV, McNaughton BL, Pennartz CMA: Hippocampus leads ventral striatum in replay of place-reward information. PLoS Biol 2009, 7: doi: 10.1371/ journal.pbio.1000173.
79. Ding L, Gold JI: Caudate encodes multiple computations for  perceptual decisions. J Neurosci 2010, 30:15747-15759 doi:
10.1523/JNEUROSCI.2894-10.2010.

www.sciencedirect.com

Current Opinion in Neurobiology 2011, 21:374‚Äì380

380 Behavioral and cognitive neuroscience

The authors examine the activity of primate caudate neurons during the performance of a reaction time version of the random dot motion task. Their observations corroborate theories of striatal action selection as operating on noisy evidence about state identities and resulting action values.
80. Dayan P, Daw ND: Decision theory, reinforcement learning, and the brain. Cogn Affect Behav Neurosci 2008, 8:429-453 doi: 10.3758/CABN.8.4.429.
81. Bogacz R, Larsen T: Integration of reinforcement learning and optimal decision-making theories of the basal ganglia. Neural Comput 2010:1-35.
82. Rao RPN: Decision making under uncertainty: a neural model  based on partially observable Markov decision processes.
Front Comput Neurosci 2010, 4:1-18 doi: 10.3389/ fncom.2010.00146. This paper advances a model in which cortical representations of belief states are employed by striatum during value computation and action selection. The model is used to explain previously observations of cortical activity during a random dot motion task, and offers a new explanation of uncertainty-driven activity in dopaminergic neurons in [85].
83. Law C, Gold J: Reinforcement learning can account for associative and perceptual learning on a visual-decision task. Nat Neurosci 2009, 12:655-663.

84. Larsen T, Leslie D, Collins E, Bogacz R: Posterior weighted reinforcement learning with state uncertainty. Neural Comput 2010, 22:1149-1179.
85. Nomoto K, Schultz W, Watanabe T, Sakagami M: Temporally extended dopamine responses to perceptually demanding reward-predictive stimuli. J Neurosci 2010, 30:10692-10699.
86. Bromberg-Martin ES, Hikosaka O: Midbrain dopamine neurons signal preference for advance information about upcoming rewards. Neuron 2009, 63:119-126 doi: 10.1016/ j.neuron.2009.06.009.
87. Kemp C, Tenenbaum J: The discovery of structural form. Proc Natl Acad Sci U S A 2008, 105:10687-10692.
88. Gershman SJ, Blei DM, Niv Y: Context, learning, and extinction. Psychol Rev 2010, 117:197-209 doi: 10.1037/a0017808.
89. Gershman SJ, Niv Y: Learning latent structure: carving nature at its joints. Curr Opin Neurobiol 2010, 20:251-256 doi: 10.1016/ j.conb.2010.02.008.
90. Braun D, Mehring C, Wolpert D: Structure learning in action. Behav Brain Res 2010, 206:157-165.

Current Opinion in Neurobiology 2011, 21:374‚Äì380

www.sciencedirect.com

