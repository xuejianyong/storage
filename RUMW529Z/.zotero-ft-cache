Book chapter for Post-Cognitivist Psychology, edited by Brendan Wallace (Oct 10, 2006)
The radical constructivist dynamics of cognition
Alexander Riegler
Center Leo Apostel for Interdisciplinary Research Vrije Universiteit Brussel, Belgium ariegler@vub.ac.be
Introduction
The early days of psychology in the 19th century were those of introspection. Because introspection is a first-person approach it doesn’t lend itself to objective communicability, one of the pillars of science. With the rise of positivist philosophy (e.g., Carnap 1932), which demands science to stick solely to observable entities, B. F. Skinner and John Watson introduced the behaviorist approach which rejected any metaphysical speculation about non-observable descriptions such as cognitive processes. The task of psychology was to “predict, given the stimulus, what reaction will take place; or, given the reaction, state what the situation or stimulus is that has caused the reaction” (Watson 1930, p.11). After decades of dominating psychology in the US, doubts were cast on behaviorism as whether it is powerful enough to account for more sophisticated phenomena such as language. In his 1959 paper, Chomsky overthrew behaviorist psychology by showing that one has to assume the internalization of linguistic rules, otherwise people would not be able to generate novel and yet grammatically correct sentences they have never heard before. However, speaking about internal processes was out of reach for behaviorists as they claimed that “cognitive constructs give […] a misleading account of what [to] find inside”(Skinner 1977, p.10).
As a result, a new paradigm, cognitivism, appeared about at the same time as the discipline artificial intelligence (AI).1 Both emphasize internal computations that link sensory input with action output. As Steven Pinker (1997) pointed out, cognitive “psychology is engineering in reverse […] one figures out what a machine was designed to do.” Uneasiness with the idea that we are mere machines rather than the unique pinnacle of creation, and especially recollecting holistic philosophies such as phenomenology and early psychological streams like gestalt-psychology as well as improved computational power that allows for sophisticated dynamical interactions among a huge number of
1 It can be argued whether this change constitutes a “revolution” in the sense of Thomas Kuhn’s (1962) who characterizes science as a sequence of normality and revolution (O’Donohue, Ferguson, & Naugle 2003).

entities, let the post-cognitive movement appear on the horizon, seeking to abandon the information-processing approach of cognitivism. It includes approaches such as embodiment and dynamical systems theory. Starting from philosophical insights like Heidegger’s “Being in the world” (Dreyfus 1991) and Merleau-Ponty’s (1962)“Phenomenology of Perception”, embodiment (e.g., Riegler 2002) emphasizes the importance of placing the cognitive being in an environment, something AI missed to do when studying chess playing in isolation. Based on the post-war cybernetic movement and equipped with sophisticated formal and especially computational tools, the dynamical approach to cognition (e.g., van Gelder 1998) emphasizes the difference between classical digital computation of representational entities and dynamical processes among quantitative variables.
In this chapter I introduce (radical) constructivism as a candidate paradigm for a postcognitive psychology. It, too, borrows from (second order) cybernetics and pushes in the same direction as phenomenology without abandoning crucial ideas of cognitivists such as Neisser. For this end, I first point out why mainstream cognitivism is insufficient to explain human cognition by focusing on key notions such as “information” and “knowledge”. Then I present the main characteristics of the constructivist approach and present five key challenges for post-cognitive approaches. Finally, I outline a structuraldynamic model based on radical constructivism that provides answers to these challenges.
Cognitivism
Darwin came along and said we were fancy chimpanzees. Now along come the cognitive scientists saying that we are fancy calculators. (Dietrich 2000)
After Chomsky (1959) had defeated behaviorism, the newly emerging cognitivist paradigm was in search for a new methodological approach and adopted the information-processing (IP) concept promoted by Broadbent (1958) which describes cognitive behavior as a sensing-thinking-acting chain. In his 1967 book, Ulric Neisser coined the term “cognitive psychology” and installed the information-processing approach as leading paradigm. Cognition is “stimulus information and its vicissitudes”, i.e., “all processes by which the sensory input is transformed, reduced, elaborated, stored, recovered, and used” (Neisser 1967, p.4).2
2 However, as we will see later Neisser also contributed to the constructivist approach. He emphasizes that “seeing, hearing, and remembering are all acts of construction” and refers to “continuously creative processes by which the world of experience is constructed” (Neisser 1967).

Evidently, the information-processing paradigm resembles what Popper (1979) calls the bucket theory of mind. This is the idea that “there is nothing in our mind which has not entered through our senses.” Cognition is metaphorically considered as a bucket that is incrementally filled with knowledge through our sensory organs. In other words, the information-processing approach describes individuals as “dynamic information processing machines” (Neisser 1967) that “compute” perceptual input in order to create output. As Heinz von Foerster points out, “cognitive processes do not compute wristwatches or galaxies, but compute at best descriptions of such entities.” He proposes to “interpret cognitive processes as never ending recursive processes of computation” (Foerster 1973/2003, p.217). This poses two questions. (1) How are we to define “computation”? (2) And what and how much do we “compute”?
As for the first question, computation should not be understood as the mechanical extension of what human professionals used to do in the early 20th century and what our desktop computers are supposed to do, i.e., crunching numbers. Rather, following Foerster’s (1973/2003, p.216) suggestion, computing (from Latin com-putare) “literally means to reflect, to contemplate (putare) things in concert (com-), without any explicit reference to numerical quantities”.
The second question, however, cannot be easily answered by recurring to etymology. For natural cognitive systems which are engulfed by a huge amount of stimuli every second, computing this amount of sensual input is intractable (NP-complete in the sense of Cook 1971), that is the time required to compute all representations grows exponentially with the number of entities. Not even the navigational skills of bees could be accounted for in terms of information processing. In artificial intelligence research this problem became evident as researchers tried to extrapolate from simplified so-called microworld scenarios to realworld situations. Microworlds (e.g., Shrdlu, Winograd 1972) were the attempt to find optimal cognitive strategies that could handle simple toy worlds and to linearly augment these capabilities to deal with more complex environments later on. Since the assumption was that the world and the entities it is populated with can be described in terms of propositions the representational system of artificial intelligence programs was designed as a network of propositions reflecting the current state of the microworld. However, as soon as the number of objects in a microworld increases and transcends a certain threshold the computational effort necessary to compute them went beyond all borders. Philosophically this was called the frame-problem (Dennett 1984), the problem of how to represent empirical observations, as in any real-world environment the number of possible propositions and their mutual relationships are practically infinite. As Daniel Dennett

argues, it makes no sense to focus on processing relevant information either, because making the distinction between relevant and irrelevant input requires an additional cognitive faculty. Therefore, the IP paradigm gets stuck in a bottleneck: which are the essential features that need to be selected among the wealth of information provided by the “outside” in order to decrease the enormous degree of complexity and ensure survival?
Furthermore, if cognition is the manipulation of propositional knowledge, why then is it so difficult to explicitly describe common sense and human experts’ knowledge? In their critique of expert systems, Dreyfus & Dreyfus (1986) argued that only rudimentary levels of expert behavior can be captured by explicit rules and facts. This level of “competence” is restricted to applying textbook cases and cannot cope with unique and novel situations for which implicit knowledge is required.
Five Challenges
The issues raised so far make it evident that defining cognition as information processing and symbolic computation is a convenient yet insufficient way to explain human faculties. It is convenient as it can be readily implemented on a computer, in fact on “any device that can support and manipulate discrete functional elements – the symbols” (Varela, Thompson & Rosch 1991, p.42). Furthermore, it is convenient because it lends itself to functional decomposition, the “divide-and-conquer” method of society and technical sciences that made modern civilization based on the division of labor possible in the first place. Scientifically it enabled the Wrights to invent the airplane (Bradshaw 1993) as they approached a small number of functional problems in isolation from each other (the “function space”) rather than the much larger “design space” as their competitors did. However, dividing and scientifically conquering alleged modules of cognition such as memory, attention, perception, action, problem solving and mental imagery does not work. As Foerster (1970b/2003, p.172) points out,
[B]y separating these functions from the totality of cognitive processes one has abandoned the original problem and now searches for mechanisms that implement entirely different functions that may or may not have any semblance with some processes that are […] subservient to the maintenance of the integrity of the organism as a functioning unit.
Such a holistic definition presses for an understanding of mechanisms responsible for the generation of behavior rather than for the specification and implementation of isolated cognitive functions. The situated cognition and embodiment approach defends the idea that cognition is intimately connected with the functioning of the body (Lakoff 1987).

William Clancey (1992) claims that “perception and action arise together, dialectically forming each other” (p.5). Furthermore, the concept of representation becomes doubtful as “we can walk through a room without referring to an internal map of where things are located, by directly coordinating our behaviors through space and time in ways we have composed and sequenced them before” (p.7, italics in the original). This means that sensory, cognitive, and motor functions can no longer be considered independent and sequentially working parts of the cognitive apparatus. And the latter can no longer be described as computational device the “task” of which is to acquire propositional knowledge about the mind-independent reality by processing information that is picked up from that reality.
Based on these considerations we can formulate the following five challenging questions.
Q1. Does cognition process information?
People are known for the mistakes they make when trying to link information in a logical way (Dörner 1996). At least three reasons can be held responsible.
(1) The short-term memory can hold only a rather limited amount of “informational units”, or “chunks” (Miller 1956), which favors simple causal explanations.
(2) The bias of human long-term memory for creating conceptual patchworks of associatively linked qualitative statements. For example, Frank Keil (2003) argues that we are prone to the “illusion of explanatory depth” as his empirical findings reveal that people greatly overestimate their knowledge of facts and procedures.
(3) “Mental inertia” which is the result of an conservatively working mind that is set to the previously successful strategy (Duncker 1935/1945; “if it ain’t broken, don’t fix it”, Riegler 1998).
The fact that despite these severe restrictions of human cognition science has reached high standards gives rise to the assumption that human scientists, like human chess players (Chase & Simon 1973), do not use brute “computational” force in order to cope with the flood of sensory information from the “outside reality.” History of science suggests that a crucial part of scientific discoveries is to find the relevant details. The classical example is astronomer Johannes Kepler who had first to process a huge bulk of information collected by his predecessor Tycho Brahe and himself over many years before he figured which geometrical figure would represent the orbits of the planets (Kozhamthadam 1994). It took him thirteen years to come up with the answer even though the (from our perspective simple) solution had been “there” from the beginning, in the data. However, any simple computational discovery system (e.g., Bacon as described in Langley et al. 1987) can reproduce the astronomer’s findings in little more than an instant.

Also at the “other” end of the bottleneck there is a huge variety of behaviors people are able to produce. For example, in his critique on behaviorism, Chomsky already pointed to the fact that people can generate sheer infinite number of grammatically correct sentences (Figure 1).
The conclusion is that human cognition is a miserable processor of information.
Figure 1: The bottleneck of information processing
Q2. Does cognition need representation and reality? Terry Winograd and Fernando Flores (1986, p.73) point out that the common rationalistic view
“accepts the existence of an objective reality, made up of things bearing properties and entering into relations. A cognitive being “gathers information” about those things and builds up a “mental model” which will be in some respects correct (a faithful representation of reality) and in other respects incorrect. Knowledge is a storehouse of representations, which can be called upon for use in reasoning and which can be translated into language. Thinking is a process of manipulating representations.” If cognition is supposed to work on descriptions of the outside reality and this descriptions are distilled from sensory information how can we be sure that this picture is accurate? For the correspondence theory of representation (cf. Wittgenstein 1922, “In order to tell whether a picture is true or false we must compare it with reality”) the subjective picture of the world is accurate if it corresponds to the physical state of the world. However, neurophysiological experiments point in a different direction. In the 1960’s Humberto Maturana and colleagues (Maturana, Uribe and Frenk 1968) investigated whether the spectral composition of colors correlates with the activities in the retina. The results were chastening. It turned out that the activity of the retina can only be connected to the names of colors, which are considered to be rough indicators of how colors are subjectively experienced. Their conclusions were that the objective of their research had turned from establishing empirical support for the correspondence between reality and subject to comparing “the activity of the nervous system with the activity of the nervous system”

(quoted in Pörksen 2004, p.61). In other words, cognition does not seem to compare pictures with “reality”.
Ernst von Glasersfeld (1981, p.89) summarizes the “skeptical dilemma” as follows. [I]t is impossible to compare our image of reality with a reality outside […] because in order to check whether our representation is a “true” picture of reality we should have to have access not only to our representation but also to that outside reality before we get to know it. [italics in the original]
Figure 2: The skeptical dilemma
Q3. What is information at all? It can be argued that information has to be distinguished from data, which is unstructured, lacks context and may not be relevant to the receiver. Only when data is organized, filtered and accompanied by context it can become information. However, if human cognition is bad at coping with information and if we cannot validate the origin of information, perhaps there is something wrong about the notion of “information” in the first place? Etymologically, the word information has its origin in the Latin verb informare, to give form to, to form an idea of. Taking this literally, the question arises: To form an idea of what?
At least from a neuropsychological perspective it is clear what sort of “information” nervous signals carry from the sensory surfaces to the brain: Nervous signals are just electrochemical impulses void of meaning. This century-old insight let Foerster formulate the principle of undifferentiated encoding: “The response of a nerve cell does not encode the physical nature of the agents that caused its response. Encoded is only ‘how much’ at this point on my body, but not ‘what’” (Foerster 1973, pp. 214–215; Figure 3). In this perspective, there is no information about entities in the world, no information that could be processed by human cognition.

Figure 3: The principle of undifferentiated encoding.
However, since “the physical nature of the stimulus – its quality – is not encoded into nervous activity, the fundamental question arises as to how does our brain conjure up the tremendous variety of this colorful world as we experience it any moment while awake, and sometimes in dreams while asleep.” (Foerster 1973/2003, p.215). This is the “second-order cybernetics” perspective. It transcends cybernetics which describes observed systems in terms of regulatory mechanisms, and moves the focus of attention from the observed object to the observing subject.
Q4. Is cognition the manipulation of propositional knowledge?
The classical understanding of representation is largely based on the idea that propositions (e.g., Fodor 1981) represent the (internal and external) environment in a more or less linguistically transparent way. Pylyshyn (1984) claims that cognition was computation, and since computation was symbol manipulation, cognition must be propositional. By distinguishing “cognitive penetrability” from “impenetrability” he defined the criterion for what was to be considered as cognitive, i.e., cognitive is what can be modified by what we know explicitly. That which eludes modification was “subcognitive,” and therefore not part of cognitive research. For example, habits and hormonal changes do not meet this criterion.
As pointed out in the beginning, propositions lend themselves conveniently to computer implementations. However, what is referred to as a proposition is the result of extremely complex processes occurring in the neural dynamics and leading to the externalization of “propositional categories”, e.g., in form of symbols and language (Peschl & Riegler 1999).
From the psychological literature it is known that cognitively sophisticated activities such as problem-solving do not depend on propositional representations, even worse, such propositional knowledge may even make finding the solution more difficult. For example, Knowlton, Mangels, and Squire (1996) describe the “weather prediction” experiment where subjects are asked to figure out the link between a configuration of cards with geometrical figures and a weather situation. As the authors point out, the mapping between the cues and the outcomes is too complex for the hippocampus, which is usually

responsible for learning such propositional representations. Therefore the success of the subjects in this experiment must be attributed to unconscious and therefore nonpropositional mechanisms.
Generally, in large propositional spaces, each experiential element has the potential of being linked with almost any other but only a very few of this connections make sense. For example, it is quite senseless to assume that the color of the walls in your room influences the weather. Tracking down all possible causal links would render taking actions impossible. Ignoring links altogether would make us superstitious neurotics who practice elaborate rituals and ceremonies to invoke magical powers for safety and protection (Malinowski 1948). The complexity of the problem is demonstrated by Varela (1988) who compares the crystalline structure of the game of chess, in which entities and relations are clearly defined, with the rather chaotic network of a car driver’s knowledge which quickly dissolves into the ambiguities of common sense (Figure 4). If formally well-defined areas such as chess already lead to vast combinatorial possibilities how much more complicated must the propositional description of common sense be?
Figure 4: The crystalline structure of the world of chess and the ambiguous world of common sense. Redrawn from Varela (1988).
Q5. What is knowledge? If there is no information which could inform cognition about the mind-independent reality, what does human knowledge consist of? Mainstream philosophy defines knowledge as justified true beliefs (JTB). This traditional perspective suffers from the problem that its three components – justification, truth, and belief – do not seem to be sufficient to account for knowledge, as demonstrated by Edmund Gettier (1963) and others who further extended Gettier’s original counter examples. From the angle of postcognitivist approaches, the difficulties of the conventional definition of knowledge can be traced back to at least two elementary problems:

1. The lack of a proper definition of “truth” (i.e., when is a given proposition true?). As we noticed above, the verification of whether a given statement is true seems problematic due to the skeptical dilemma.
2. The restriction of the notion of knowledge to human cognition based on the assumption that knowledge must be formulated in the form of propositions. For example, young blackbirds open their beaks if simple dummies are presented. Juvenile gulls mistake a pointed stick for an adult bird. Dummies that do not even closely resemble the appearance of the animal cause aggressive behavior in male sticklebacks. In all these cases, the dummies, although they do not resemble the “real thing,” have a meaning for the animal (Figure 5). From the perspective of conventional philosophy, however, the young birds do not know anything although they may believe that they are approached by the parenting bird, and this belief is certainly justified by the fact that hungry young birds usually get food. But then, given the evolutionary success of their behavior, what else if not knowledge has been transmitted from generation to generation? Despite the animals’ susceptibility to rather inaccurate stimuli their knowledge is successful on an evolutionary scale.
Figure 5: Dummies as counterexample for knowledge as justified true belief.
It is evident that correspondence with the real environment is not required for the generation of knowledge. For example, Keil (2003) points out that proponents of the phlogiston theory were not a bunch of cranks. They seriously tried to give coherence and sense to their observations and created the first comprehensive chemistry theory. But as careful experimentation revealed, they did not arrive at the “accurate” knowledge about combustion. They happened to try one of the virtually infinite possible mappings from sensory data to explanatory mechanisms but they were as wrong as a randomly guessing amateur. So is knowledge nothing but an accidental correspondence between the structure of the cognitive apparatus and the phenomenon in question?

Radical constructivism
The core problem of finding an appropriate account for cognition and knowledge arises from confusing two persepctives P1 and P2 (cf. Riegler 2005 for a in-depth discussion).
P1. Observers such as scientists focus on the (measurable) output of an individual. Even worse they attribute goals to the measured behavior without being able to know the intentions which are at the base of the behavior.
P2. In contrast to the observer, humans (and animals) control their input (perception) rather than their output (behavior), or as Powers (1973) put it, “behavior is the process by which organisms control their input sensory data” (cf. also Porr & Wörgötter 2005 who discuss input control in more technical terms). This perspective is based on the concept of homeostasis (Cannon 1932), which says that a living organism has to keep its intrinsic variables within certain limits in order to survive. Such “essential variables” (Ashby 1952) include body temperature, levels of water, minerals and glucose, and similar physiological parameters, as well as other proprioceptively or consciously accessible aspects in higher animals and human beings. Consequently, individuals execute certain actions in order to control and change their input state, such as avoiding the perception of an obstacle or drinking to quench their thirst. The state of homeostasis is reached by the process of negative feedback. Like in a thermostat, a given parameter is kept under control by appropriate counteractions. As well known, the thermostat adjusts the temperature by turning off the heating as soon as a certain temperature is reached and turning it on as soon as the temperature drops below a certain reference value. While such a simple feedback loop may suffice for primitive intrinsic variables, higher order goals are accomplished in a hierarchical structural-dynamic assemble of feedback loops in which each level provides the reference value for the next lower level: “The entire hierarchy is organized around a single concept: control by means of adjusting reference-signals for lower-order systems” (Powers 1973). So at higher levels the system controls the output of lower levels, at the bottom, however, its perceptual input.
P2 suggests that post-cognitivist accounts must not be behavioristically guided by copying observable behavior of existing natural systems. Rather, we need a deeper structural insight. The radical constructivist notion of organizational closure is a good point to start with. It is a necessary quality of the nervous system, and is based on the principle of undifferentiated encoding of nervous signals, as descibed above. In the case of the example of blackbirds presented in Q5, the nervous signals in the young birds that open their beaks at the sight of simple dummies do in no way convey the information of seeing a dummy (or the genuine parent bird it substitutes). Philosophically speaking, the

cognitive system is in a brain-in-a-vat situation (Putnam 1982) as it has no independent reference to what has caused the incoming electro-chemical signals. With Maturana and Varela (1987), we can compare the situation with that of the navigator in a submarine. He avoids reefs and other obstacles by simply maintaining a certain dynamic relationship (homeostasis) between levers and gauges inside the vessel.
Radical constructivism (RC, Glasersfeld 1995) is the conceptual framework that builds on this insight. According to the radical constructivist postulate (Riegler 2001) the cognitive system (mind) is organizationally closed. It necessarily interacts only with its own states. Or, as Winograd and Flores (1986) put it, the nervous system is “a closed network of interacting neurons such that any change in the state of relative activity of a collection of neurons leads to a change in the state of relative activity of other or the same collection of neurons” (p.42). Cognition is, therefore, a continuously self-transforming activity. There is no purpose attached to this dynamics, no goals imposed from outside the cognitive apparatus. According to Rudolfo Llinás (2001) the nervous system is able to generate sensory experiences of any type. Therefore, we are facing the fact that “we are basically dreaming machines that construct virtual models” (p.94). Llinás’s closed-system hypothesis describes the mind primarily as a self-activating system, “one whose organization is geared toward the generation of intrinsic images” (p.57). The global picture is that cognition acts independently of the environment. It merely requests confirmation for its ongoing dynamical functioning and works autonomously otherwise: “Although the brain may use the senses to take in the richness of the world, it is not limited by those senses; it is capable of doing what it does without any sensory input whatsoever” (p.94).
As a result, cognition must be considered a closed-loop system whose primary goal is to regulate its input. This definition refers to Maturana and Varela’s (1980) concept of autopoietic systems, which have to be distinguished from allopoietic ones. Autopoiesis refers to mutually chained processes that produce the components necessary to run these processes. Evidently, in the physical space of living systems, autopoiesis is instantiated by material processes, which produce, as it were, material and behavioral by-products visible to an observer. However, these “outputs” do not define the autopoietic system.
Knowledge and information
Evidently, in cognitively closed systems, knowledge cannot refer to mapping between an external state of affairs and cognitive structures. The conventional JTB definition of knowledge can no longer be applied in this context since there are neither propositions nor can their truth be specified. Following Glasersfeld’s characterization of RC, knowledge

must not be considered to be passively received but actively built up by the cognizing subject because the “function of cognition is adaptive and serves the organization of the experiential world, not the discovery of ontological reality” (Glasersfeld 1988, p.83). This leads to an alternative understanding of knowledge that refrains from assuming that differently constructed conceptual frameworks in individuals gradually converge towards an “objectively valid” knowledge system representing the “reality.” Since from the perspective of RC no such convergence takes place, the emphasis is to be put on mechanisms of knowledge construction, and on the fact that cognitive systems actively construct their world rather than being passively flooded by information from the outside. Knowledge does not reside somewhere else and is not independent of the cognitive system that generates it. Hence, whatever it is that the cognitive apparatus picks up, it cannot be considered knowledge: “The environment contains no information. The environment is as it is.” (Foerster 1970b/2003, p. 189).
Neisser’s (1975) schemata-guided pickup-paradigm is one way to describe this matter of fact. The configuration of perception-anticipating “slots” in schemata varies over time, i.e., what is being perceived (and taken in) now may not be perceived at a later instant. This accounts for the variation of meaning we and cognitive systems in general encounter over time. This “constructivist-anticipatory principle” (1994) assumes that knowledge is implemented in the form of schemata, which consist of conditions and a sequence of actions. Schemata can be mutually embedded. So can conditions and actions. The purpose of the condition part is to provide context matching which allows the schema that best fits the present context to execute its action sequence. Since conditions can also be part of a sequence, they act as checkpoints for determining whether the anticipated meaning embodied by the schema is still on the right track. After a schema and all its subordinated elements finished, the cycle starts again. In this model, knowledge refers to the capability of the system to bridge between momentary perception and older experiences that are embedded in its schemata. Further details are described in the next section.
In other words, radical constructivism moves the focus of attention from defining a cognitive system as information processor to describing it as information producer.
That cognition excels at producing information becomes clear when we consider the phenomenon of superstitious perception which occurs for example when people see faces in clouds or figures in stellar constellations. In several experiments, Frédéric Gosselin and Philippe Schyns (2003) stimulated the visual system of test subjects with unstructured white noise, i.e., a static bit pattern that has equal energy at all spatial frequencies and does not correlate across trials. The subjects were asked to discriminate between a smiling

and a nonsmiling face, which was allegedly present in 50% of the presentations. As a result, the subjects perceived the expected face. These findings confirm the information producing view in the sense that the anticipated pattern was projected onto (partially correlated with) the perception of the white noise.
The information-producing view has gained great acceptance in the literature. For example. Susan Oyama (1985) and William Clancey (1991) also refer to the fact that information is not “retrieved” but rather “created” by the system. Dennett (1991) claims that the brain holds only a few salient details and fills in the rest from memory. Kevin O’Regan (1992) concludes that “we only see what we attend to”. According to Gerhard Roth (quoted in Pörksen 2001/2004, p. 121) the “re-enactment of the image, released by only a few sign stimuli, is far quicker than it would be if the eye had to scan the environment atomistically every single time.” However, radical constructivism goes an important step further. Due to the organizational closure of the cognitive apparatus it constructs its reality without “knowing” that these inputs come from the sensory surface as there is no way to distinguish sensory signals from any other nervous signal. The idea of a sensory signal is only reconstructed a posteriori.
The information-producing perspective offers a crucial advantage over informationprocessing. As we can no longer speak of information input and the vicissitude of stimuli, organisms are no longer exposed to information overload as a result of processing the entirely available information. They no longer need to devote their cognitive resources to filter out irrelevant information in order to retain useful knowledge. It becomes clear that even insect brains can accomplish navigational tasks and sophisticated cognitive deeds in nontrivial environments without falling prey to the frame problem.
Therefore, cognitive research on perception should not focus on filtering mechanisms and data reduction. Information anxiety (Wurman 1990) and cognitive overload (Kirsh 2000) should not be considered a problem of the environment, as it is the case when talking, e.g., about the overload that comes with the information flood on the internet. Perception has to be explored in terms of the organism that performs the perceptive act.
Structure and dynamics
How does the structure and dynamics of such a radical constructivist cognitive system look like?
In order to remain explainable (but without forcing linguistic transparency by introducing propositional representations) the actual representational elements are schemata, which consist of conditions and actions.

It is important to note that since the apparatus does not start with an a priori defined knowledge base but relies on various learning mechanisms as described below, it does not import the problems associated with the impossibility of making knowledge explicit (Dreyfus & Dreyfus 1986). In fact due to the learning component the initial set of rules in an individual can either be empty or inherited from ancestral individuals.
In contrast to most artificial neural networks, which work with several layers of discrete nodes, the apparatus uses only one layer that consists of a set S of discrete cells s. These cells can be modified read, and compared with each other. The single-layer approach reduces the structural complexity in favor of a procedural hierarchy, as explained further below. The separation between cognitive apparatus and environment required by the cognitive closure of the cognitive system (cf. Q3) entails that all schemata deal with semantic-free states rather than with propositional representations of entities. To bridge the gap, a “transduction shell” is introduced, that is, a sensorimotor surface which takes over the function of nervous signals by mapping perceptive stimuli from the outside onto internal states S and vice versa. Not only does this “anonymity” implement cognitive closure
The analogue character of nervous signals suggests the states to be read and compared in a fuzzy manner, i.e., the condition parts of schemata have smoothly varying, non-linear, interpolable classification boundaries comparable with the concept of partial membership described by fuzzy set theory (Zadeh 1965). Therefore, the query response of conditions is defined over a continuous Gaussian curve rather than a discrete interval. That is, a querying a cell q(s, c) determines the extent to which the value of the addressed cell s falls within the Gaussian interval defined by c.
Each fuzzy schema consists of an independent condition part and action part. Both components obey the building block principle. For the condition part there are single queries and assemblies of queries, so-called concepts, which combine single conditions to represent composed entities. For the observer of the system, who can only access the system’s output, such composed entities can be interpreted as multimodal perceptions, complex representations, composed hypotheses, etc.
As with single conditions, single actions, too, can be chained to form action sequences. Both condition and action chains can contain single and combined elements. There are two types of actions. The set action modifies the state of a particular cell. The call action can insert any arbitrary building block (i.e., primitive condition or action, concept or action sequence, or an entire schema) into the action part of a schema. From this description it follows that both condition and action part form nested hierarchies in which

encapsulations are reused as building blocks. Elements can be functionally coupled as they make use of encapsulations and/or are part of encapsulations themselves. Therefore, the cognitive apparatus is a hierarchical system. (cf. Figure 6).
Finally, a single action may contain various flags such as stopping the current schema, or moving the execution of the current schema into the background such that the apparatus can start the concurrent execution of another schema.
Figure 6: The hierarchical structure of the cognitive system Procedurally, the apparatus works as follows. The algorithm uses the current internal context, i.e., the internal states S, which have partly been set by the transduction shell, to find a match among the condition parts of the existing rules. The matching is biased by the priority of the rules, which is inversely proportional to the length of the respective rule’s condition part. These rules are at the lower end of the behavioral spectrum and could be anthropomorphically called “reflexes.” The priority measure is motivated by the fact that reflexes are rather unspecific rules with low computational costs. It is also in alignment with the default hierarchy concept of Holland et al. (1986) where the most general elements have the shortest access path. Generally speaking, these reflex rules enable the apparatus to respond to critical events that endanger the existence of the cognitive system. Another conflict resolution criterion is the algorithm’s preference for rules of low generality (as defined below). At the other end of the behavioral spectrum are rules with long action parts and embedded conditions, which in the extreme case could make initial rule triggering superfluous as they keep on branching to other action sequences and rules. They implement cognitive flexibility and dynamical continuity. However, an action sequence can come to a halt after the last action element is executed or if an embedded condition

explicitly stops the execution of the current rule. In this case the algorithm again matches the current state against the condition parts to determine which rule may fire.
Learning is guided by maximizing two parameters. The generality of a schema is the weighted sum of the widths of the Gaussian intervals over all included conditions. It reflects the schema’s specialization. The lower the generality the more specialized the schema is. The importance of a schema is the (weighted) sum of maximal degrees of membership over all included conditions. Rules with a higher importance are more likely to be carried out. In other words, these parameters enable the cognitive system to accommodate its schemata without reference to external entities and rewarding schemes (cf. Q2). Rather, the system follows the general idea that the more often a schema has been carried out in the past, the higher its probability of being executed in the future. Ultimately, the success of cognition is defined in terms of success in the process of life (Heschl 1990; Stewart 1996). This definition extends the notion of cognition to animals: It is unlikely that the cognitive individual would have survived so far if frequently calling a particular schemata had a negative impact on its survival. Glasersfeld refers to this matter of fact as “viability”: “[W]e construct ideas, hypotheses, theories, and models, and as long they survive, which is to say, as long as our experience can be successfully fitted into them, they are viable” (Glasersfeld 1981, p.90, italics in the original).
In a population of reproducing individuals there is also phylogenetic learning which includes the application of genetic mutation and recombination. They can alter the composition of combined conditions and actions as well as schemata by adding, removing, or permuting their components. Compression and expansion are used to encapsulate such combined elements in order to increase the likelihood of being passed on to the next generation. Applying genetic operators to the compressed genotypes leads to functional couplings between control and structural genes as they implement homeotic mutations, which accelerate phylogenetic learning. Couplings enable coordinated development between structural elements of the cognitive apparatus in the sense that a regulatory element at a higher hierarchical level concurrently modifies elements at lower levels. Any mutation in a regulatory element simultaneously changes all connected elements as well. As a result, adaptability is increased because regulatory elements make synchronization possible among those dynamical elements which have to develop congruently as their proper functioning depends on each other.
Given the perpetual interplay of actions and conditions, which is rarely interrupted by initial rule firing, the system does not compute cognitive functions. Rather, it is a

dynamical system that unfolds cognitive competence in time, as characterized by the dynamical approach to cognition (van Gelder 1998).
Conclusion
Radical constructivism objects to considering knowledge a justified belief that is true in the sense of referentially mapping propositional descriptions of the environment onto cognitive structures. Rather, knowledge must be system-relative. This term refers to situated cognitive processes the dynamics of which is merely modulated by their environment on request of the cognitive apparatus rather than instructed by it. Furthermore, it becomes evident that knowledge is a relational dynamical structure rather than a set of propositions (cf. Q4). What a person knows today can have a completely different significance tomorrow. This dynamics cannot be captured in a static blueprint we refer to as declarative or procedural knowledge. Rather, knowledge is the process of continuous constructions, the dovetailing of cognitive structures, which occasionally allow for assimilation of and accommodation to picked-up data from the environment (cf. Q5). These data (or signals) are neither computed nor do they constitute information (Q3). In the hierarchy of cognitive structures only a few schemata are situated at the top where they are externalized as propositions, i.e., schemata with a high degree of abstraction and distance from the lowest levels.
The radical constructivist information-producing paradigm emphasizes the primacy of the internal cognitive dynamics over influences from the outside. This negates Q1. Cognitive decisions are checkpoints embedded in action sequences. Both conditions and actions form schemata that populate the cognitive apparatus. Their dovetailing is continuously changing in function of phylogenetic and ontogenetic learning leading to a hierarchical organization and, ultimately, to canalizations that force certain paths of schema execution, thereby foregoing external determination through reality (Q2). Interestingly, in this paradigm emotions do not play the role as arbiter because there is no need to cope with the computational costs of filtering and evaluating the flood of perceptual stimuli (Riegler 2005).
In summary, the radical constructivist perspective points in the direction of a postcognitivist psychology which (a) does not get stuck in perceptual overload, (b) does not run into epistemological problems of (propositional) knowledge representation, (c) takes the undifferentiated encoding of nervous signals into consideration, (d) does not exclude animals from being cognitive, and (e) accounts for implicit knowledge.

References
Ashby, W. R. (1952) Design for a brain. London: Chapman and Hall. Bradshaw, G. (1993) The airplane and the logic of invention. In: Giere, R. N. (ed.)
Cognitive models of science. Univ. of Minnesota Press, pp. 239–250. Broadbent, D. E. (1958) Perception and communication. New York: Pergamon. Cannon, W. B. (1932) The wisdom of the body. New York: Norton. Carnap, R. (1932) Psychologie in physikalischer Sprache. Erkenntnis 3: 107–142. English
Translation: Carnap, R. (1959) Psychology in physical language (trans. by G. Schick). In: Ayer, A. J. (ed.) Logical positivism. New York: Free Press, 165–197. Chase, W. G. & Simon, H. A. (1973) Perception in chess. Cognitive Psychology 4: 55–81. Chomsky, N. (1959) A review of B.F. Skinner’s Verbal Behavior. Language 35: 26–58. Clancey (1991) Review of Rosenfield’s “The Invention of Memory”. Artificial Intelligence 50: 241–284. Clancey, W. J. (1992) “Situated” means coordinating without deliberation. McDonnel Foundation Conference. Santa Fe, NM. Cook, S. A. (1971) The complexity of theorem proving procedures. Proceedings of the third annual ACM Symposium on the theory of computing. ACM, New York, pp. 151–158. Dennett, D. C. (1984) Cognitive wheels. In: Hookway, C. (ed.) Minds, machines, and evolution: Philosophical studies. London: Cambridge University Press, pp. 129–151. Dennett, D. C. (1991) Consciousness explained. London: Little, Brown & Co. Dietrich, E. (2000) Cognitive science and the mechanistic forces of darkness, or why the computational science of mind suffers the slings and arrows of outrageous fortune. Techné 5(2). Dörner, D. (1996) The logic of failure. Metropolitan Books: New York. Duncker, K. (1945) On problem solving. Psychological Monographs 58: 1–112. German original published in 1935. Dreyfus, H. & Dreyfus, S. (1986) Man over machine: The power of human intuition and expertise in the era of the computer. New York: The Free Press, Macmillan, Inc. Dreyfus, H. (1991) Being-in-the-World: A commentary on Heidegger’s Being and Time, Division I. The MIT Press, Cambridge. Foerster, H. von (1970a) Molecular ethology. An immodest proposal for semantic clarification. In: Ungar, G. (ed.) Molecular mechanisms in memory and learning. New York: Plenum Press, pp. 213–248. Reprinted in Foerster, H. von (1982) Observing systems. Intersystems Publications: Seaside, pp. 149–188.

Foerster, H. von (1970b) Thoughts and notes on cognition. In: P. Garvin (ed.) Cognition: A multiple view. New York: Spartan Books, pp. 25–48. Reprinted in: Foerster, H. von (2003) Understanding understanding. New York: Springer, pp. 169–190.
Foerster, H. von (1972) Perception of the future and the future of perception. Instructional Science 1: 31–43. Reprinted in: Foerster, H. von (2003) Understanding understanding. New York: Springer, pp. 199–210.
Foerster, H. von (1973) On constructing a reality. In: Preiser, F. E. (ed.) Environmental design research, Vol. 2. Dowden, Hutchinson & Ross, Stroudberg, pp. 35-46. Reprinted in: Foerster, H. von (2003) Understanding understanding. New York: Springer, pp. 211–228.
Fodor, J. A. (1981) Representations: Philosophical essays on the foundations of cognitive science. Cambridge, MA: MIT Press.
Gettier, E. L. (1963) Is justified true belief knowledge? Analysis 23: 121–123. Gigerenzer, G. (2000) Adaptive thinking. Oxford University Press Glasersfeld, E. von (1981) The concepts of adaptation and viability in a radical
constructivist theory of knowledge. In: I. E. Sigel, D. M. Brodzinsky, and R. M. Golinkoff (eds.) Piagetian theory and research. Hillsdale, NJ: Erlbaum, pp. 87–95. Glasersfeld, E. von (1988) The reluctance to change a way of thinking. The Irish Journal of Psychology 9(1): 83–90. Glasersfeld, E. von (1995) Radical constructivism: A way of knowing and learning. London: Falmer Press. Gosselin, F. & Schyns, P. G. (2003) Superstitious perceptions reveal properties of internal representations. Psychological Science 14: 505–509. Heschl, A. (1990) L = C: A simple equation with astonishing consequences. Journal of Theoretical Biology 145: 13–40. Keil, F. C. (2003) Folkscience: Coarse interpretations of a complex reality. Trends in Cognitive Sciences 7: 368–373. Kirsh D. (2000) A few thoughts on cognitive overload. Intellectica 30: 19–51. Knowlton, B., Mangels, J. & Squire, L. (1996) A neostriatal habit learning system in humans. Science 273 (5280): 1399-1402. Kozhamthadam, J. (1994) The discovery of Kepler’s laws. University of Notre Dame Press, Notre Dame. Kuhn, T. S. (1962) The structure of scientific revolutions. Cambridge, MA: Univ. of Chicago Press. Lakoff, G. (1987) Women, fire and dangerous things. Chicago: Chicago University Press.

Langley, P., Simon, H., Bradhaw, G. L. & Zytkow, J. M. (1987) Scientific discovery: Computational explorations of the creative processes. MIT Press, Cambridge MA.
Llinás, R. R. (2001) I of the vortex. Cambridge MA: MIT Press. Malinowski, B. (1948) Magic, science and religion and other essays. Glencoe, IL: Free
Press. Originally published in 1925. Maturana, H. R. & Varela, F. J. (1980) Autopoiesis and cognition. Dordrecht: Reidel. Maturana, H. R. & Varela, F. J. (1987) The tree of knowledge. Boston MA: Shambhala. Maturana, H. R., Uribe, G. & Frenk, S. (1968) A biological theory of relativistic colour
coding in the primate retina: A discussion of nervous system closure with reference to certain visual effects. Archiva de Biologia y Medicina Experimentales (Suplemento Vol. 1): 1–30. McAllister, J. W. (1997) Phenomena and patterns in data sets. Erkenntnis 47:217–228. Merleau-Ponty, M. (1962) Phenomenology of perception. (trans. by C. Smith). London: Routledge & Kegan Paul. Miller, G. A. (1956) The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review 63: 81–97. Nagel, T. (1974) What is it like to be a bat? Philosophical Review 83: 435-450. Neisser, U. (1967) Cognitive psychology. New York: Meredith. Neisser, U. (1975) Cognition and reality. San Francisco: W. H. Freeman. O’Donohue, W., Ferguson, K.E. & Naugle, A.E. (2003) The structure of the cognitive revolution. An examination from the philosophy of science. The Behavior Analyst, 26, 85-110. O’Regan, J. K. (1992) Solving the “real” mysteries of visual perception: The world as an outside memory. Canadian Journal of Psychology 46: 461–488. Oyama, S. (1985) The ontogeny of information: Developmental systems and evolution. Cambridge University Press: Cambridge MA. Republished in 2000. Peschl, M. & Riegler, A. (1999) Does representation need reality? In: Riegler, A., Peschl, M. & Stein, A. von (eds.) (1999) Understanding representation in the cognitive sciences. New York: Kluwer Academic / Plenum Publishers, pp. 9–17. Pinker, S. (1997) How the mind works. New York: Norton. Popper, K. (1979) Objective knowledge: An evolutionary approach (rev. ed.). Oxford: Clarendon Press. Pörksen, B. (2004) The certainty of uncertainty. Exeter: Imprint. German original appeared in 2001.

Porr, B. & Wörgötter, F. (2005) Inside embodiment. What means embodiment to radical constructivists? Kybernetes 34 (1/2): 105–117.
Powers, W. T. (1973) Behavior. The control of perception. New York: Aldine de Gruyter. Putnam, H. (1982) Reason, truth, and history. Cambridge: Cambridge Univ. Press. Pylyshyn, Z. (1984) Computation and cognition: Toward a foundation for cognitive
science. Cambridge, MA: Bradford Books/MIT Press. Riegler, A. (1994) Constructivist artificial life: The constructivist–anticipatory principle
and functional coupling. In: Hopf, J. (ed.) Proceedings of the 18th German Conference on Artificial Intelligence (KI-94) Workshop on Genetic Algorithms within the Framework of Evolutionary Computation. Max-Planck-Institute Report No. MPI-I-94241, pp. 73–83. Riegler, A. (1998) The end of science: Can we overcome cognitive limitations? Evolution & Cognition 4: 37–50. Riegler, A. (2001) Towards a radical constructivist understanding of science. Foundations of Science 6 (1–3): 1–30. Riegler, A. (2002) When is a cognitive system embodied? Cognitive Systems Research, special issue on “Situated and embodied cognition”, 3, 339–348. Riegler, A. (2005) Decision-making and anticipations: Why Buridani’s ass is an unlikely creature. In: Smit, I., Wallach, W. & Lasker, G. E. (eds.) Cognitive, emotive and ethical aspects of decision making in humans and in AI. Volume IV. Windsor, Canada: The International Institute for Advanced Studies in Systems Research and Cybernetics, pp. 35–40. Riegler, A. (2006) The paradox of autonomy: The interaction between humans and autonomous cognitive artifacts. In: Dodig-Crnkovic, G. & Susan Stuart, S. (eds.) Computing, philosophy, and cognitive science. Cambridge Scholars Press, in press. Skinner, B. F. (1977) Why I am not a Cognitive Psychologist. Behaviorism 5: 1–10. Stewart, J. (1996) Cognition = life: Implications for higher-level cognition. Behavioural Processes 35: 311–326. van Gelder, T. J. (1998) The dynamical hypothesis in cognitive science. Behavioral and Brain Sciences 21: 1–14. Varela, F. J. (1988) Cognitive science: A cartography of current ideas. Paris: CREA, Ecole Polytechnique. Watson, J. (1930) Behaviorism. Norton: New York. Winograd, T. (1972) Understanding natural language. New York: Academic Press.

Winograd, T. & Flores, F. (1986) Understanding computers and cognition: A new foundation for design. Norwood, NJ: Ablex.
Wittgenstein, L. (1922) Tractatus logico-philosophicus. London: Routledge. Wurman, R. S. (1990) Information anxiety: What to do when information doesn’t tell you
what you need to know. New York: Bantam Books. Zadeh, L. (1965) Fuzzy sets. Journal of Information and Control 8: 338–353.
Word count: 6845 (without references)

