Understanding Machine Learning: From Theory to Algorithms
c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press.
This copy is for personal use only. Not for distribution. Do not post. Please link to:
http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning
Please note: This copy is almost, but not entirely, identical to the printed version of the book. In particular, page numbers are not identical (but section numbers are the same).

Understanding Machine Learning
Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the ﬁeld, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and nonexpert readers in statistics, computer science, mathematics, and engineering.
Shai Shalev-Shwartz is an Associate Professor at the School of Computer Science and Engineering at The Hebrew University, Israel.
Shai Ben-David is a Professor in the School of Computer Science at the University of Waterloo, Canada.

UNDERSTANDING MACHINE LEARNING
From Theory to Algorithms
Shai Shalev-Shwartz
The Hebrew University, Jerusalem
Shai Ben-David
University of Waterloo, Canada

32 Avenue of the Americas, New York, NY 10013-2473, USA
Cambridge University Press is part of the University of Cambridge. It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning and research at the highest international levels of excellence.
www.cambridge.org Information on this title: www.cambridge.org/9781107057135
⃝c Shai Shalev-Shwartz and Shai Ben-David 2014
This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press.
First published 2014
Printed in the United States of America
A catalog record for this publication is available from the British Library
Library of Congress Cataloging in Publication Data
ISBN 978-1-107-05713-5 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party Internet Web sites referred to in this publication, and does not guarantee that any content on such Web sites is, or will remain, accurate or appropriate.

Triple-S dedicates the book to triple-M

vii
Preface
The term machine learning refers to the automated detection of meaningful patterns in data. In the past couple of decades it has become a common tool in almost any task that requires information extraction from large data sets. We are surrounded by a machine learning based technology: search engines learn how to bring us the best results (while placing proﬁtable ads), anti-spam software learns to ﬁlter our email messages, and credit card transactions are secured by a software that learns how to detect frauds. Digital cameras learn to detect faces and intelligent personal assistance applications on smart-phones learn to recognize voice commands. Cars are equipped with accident prevention systems that are built using machine learning algorithms. Machine learning is also widely used in scientiﬁc applications such as bioinformatics, medicine, and astronomy.
One common feature of all of these applications is that, in contrast to more traditional uses of computers, in these cases, due to the complexity of the patterns that need to be detected, a human programmer cannot provide an explicit, ﬁnedetailed speciﬁcation of how such tasks should be executed. Taking example from intelligent beings, many of our skills are acquired or reﬁned through learning from our experience (rather than following explicit instructions given to us). Machine learning tools are concerned with endowing programs with the ability to “learn” and adapt.
The ﬁrst goal of this book is to provide a rigorous, yet easy to follow, introduction to the main concepts underlying machine learning: What is learning? How can a machine learn? How do we quantify the resources needed to learn a given concept? Is learning always possible? Can we know if the learning process succeeded or failed?
The second goal of this book is to present several key machine learning algorithms. We chose to present algorithms that on one hand are successfully used in practice and on the other hand give a wide spectrum of diﬀerent learning techniques. Additionally, we pay speciﬁc attention to algorithms appropriate for large scale learning (a.k.a. “Big Data”), since in recent years, our world has become increasingly “digitized” and the amount of data available for learning is dramatically increasing. As a result, in many applications data is plentiful and computation time is the main bottleneck. We therefore explicitly quantify both the amount of data and the amount of computation time needed to learn a given concept.
The book is divided into four parts. The ﬁrst part aims at giving an initial rigorous answer to the fundamental questions of learning. We describe a generalization of Valiant’s Probably Approximately Correct (PAC) learning model, which is a ﬁrst solid answer to the question “what is learning?”. We describe the Empirical Risk Minimization (ERM), Structural Risk Minimization (SRM), and Minimum Description Length (MDL) learning rules, which shows “how can a machine learn”. We quantify the amount of data needed for learning using the ERM, SRM, and MDL rules and show how learning might fail by deriving

viii
a “no-free-lunch” theorem. We also discuss how much computation time is required for learning. In the second part of the book we describe various learning algorithms. For some of the algorithms, we ﬁrst present a more general learning principle, and then show how the algorithm follows the principle. While the ﬁrst two parts of the book focus on the PAC model, the third part extends the scope by presenting a wider variety of learning models. Finally, the last part of the book is devoted to advanced theory.
We made an attempt to keep the book as self-contained as possible. However, the reader is assumed to be comfortable with basic notions of probability, linear algebra, analysis, and algorithms. The ﬁrst three parts of the book are intended for ﬁrst year graduate students in computer science, engineering, mathematics, or statistics. It can also be accessible to undergraduate students with the adequate background. The more advanced chapters can be used by researchers intending to gather a deeper theoretical understanding.
Acknowledgements
The book is based on Introduction to Machine Learning courses taught by Shai Shalev-Shwartz at the Hebrew University and by Shai Ben-David at the University of Waterloo. The ﬁrst draft of the book grew out of the lecture notes for the course that was taught at the Hebrew University by Shai Shalev-Shwartz during 2010–2013. We greatly appreciate the help of Ohad Shamir, who served as a TA for the course in 2010, and of Alon Gonen, who served as a TA for the course in 2011–2013. Ohad and Alon prepared few lecture notes and many of the exercises. Alon, to whom we are indebted for his help throughout the entire making of the book, has also prepared a solution manual.
We are deeply grateful for the most valuable work of Dana Rubinstein. Dana has scientiﬁcally proofread and edited the manuscript, transforming it from lecture-based chapters into ﬂuent and coherent text.
Special thanks to Amit Daniely, who helped us with a careful read of the advanced part of the book and also wrote the advanced chapter on multiclass learnability. We are also grateful for the members of a book reading club in Jerusalem that have carefully read and constructively criticized every line of the manuscript. The members of the reading club are: Maya Alroy, Yossi Arjevani, Aharon Birnbaum, Alon Cohen, Alon Gonen, Roi Livni, Ofer Meshi, Dan Rosenbaum, Dana Rubinstein, Shahar Somin, Alon Vinnikov, and Yoav Wald. We would also like to thank Gal Elidan, Amir Globerson, Nika Haghtalab, Shie Mannor, Amnon Shashua, Nati Srebro, and Ruth Urner for helpful discussions.
Shai Shalev-Shwartz, Jerusalem, Israel Shai Ben-David, Waterloo, Canada

Contents

Preface

page vii

1

Introduction

19

1.1 What Is Learning?

19

1.2 When Do We Need Machine Learning?

21

1.3 Types of Learning

22

1.4 Relations to Other Fields

24

1.5 How to Read This Book

25

1.5.1 Possible Course Plans Based on This Book

26

1.6 Notation

27

Part I Foundations

31

2

A Gentle Start

33

2.1 A Formal Model – The Statistical Learning Framework

33

2.2 Empirical Risk Minimization

35

2.2.1 Something May Go Wrong – Overﬁtting

35

2.3 Empirical Risk Minimization with Inductive Bias

36

2.3.1 Finite Hypothesis Classes

37

2.4 Exercises

41

3

A Formal Learning Model

43

3.1 PAC Learning

43

3.2 A More General Learning Model

44

3.2.1 Releasing the Realizability Assumption – Agnostic PAC

Learning

45

3.2.2 The Scope of Learning Problems Modeled

47

3.3 Summary

49

3.4 Bibliographic Remarks

50

3.5 Exercises

50

4

Learning via Uniform Convergence

54

4.1 Uniform Convergence Is Suﬃcient for Learnability

54

4.2 Finite Classes Are Agnostic PAC Learnable

55

Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

x

Contents

4.3 Summary

58

4.4 Bibliographic Remarks

58

4.5 Exercises

58

5

The Bias-Complexity Tradeoﬀ

60

5.1 The No-Free-Lunch Theorem

61

5.1.1 No-Free-Lunch and Prior Knowledge

63

5.2 Error Decomposition

64

5.3 Summary

65

5.4 Bibliographic Remarks

66

5.5 Exercises

66

6

The VC-Dimension

67

6.1 Inﬁnite-Size Classes Can Be Learnable

67

6.2 The VC-Dimension

68

6.3 Examples

70

6.3.1 Threshold Functions

70

6.3.2 Intervals

71

6.3.3 Axis Aligned Rectangles

71

6.3.4 Finite Classes

72

6.3.5 VC-Dimension and the Number of Parameters

72

6.4 The Fundamental Theorem of PAC learning

72

6.5 Proof of Theorem 6.7

73

6.5.1 Sauer’s Lemma and the Growth Function

73

6.5.2 Uniform Convergence for Classes of Small Eﬀective Size

75

6.6 Summary

78

6.7 Bibliographic remarks

78

6.8 Exercises

78

7

Nonuniform Learnability

83

7.1 Nonuniform Learnability

83

7.1.1 Characterizing Nonuniform Learnability

84

7.2 Structural Risk Minimization

85

7.3 Minimum Description Length and Occam’s Razor

89

7.3.1 Occam’s Razor

91

7.4 Other Notions of Learnability – Consistency

92

7.5 Discussing the Diﬀerent Notions of Learnability

93

7.5.1 The No-Free-Lunch Theorem Revisited

95

7.6 Summary

96

7.7 Bibliographic Remarks

97

7.8 Exercises

97

8

The Runtime of Learning

100

8.1 Computational Complexity of Learning

101

Contents

xi

8.1.1 Formal Deﬁnition*

102

8.2 Implementing the ERM Rule

103

8.2.1 Finite Classes

104

8.2.2 Axis Aligned Rectangles

105

8.2.3 Boolean Conjunctions

106

8.2.4 Learning 3-Term DNF

107

8.3 Eﬃciently Learnable, but Not by a Proper ERM

107

8.4 Hardness of Learning*

108

8.5 Summary

110

8.6 Bibliographic Remarks

110

8.7 Exercises

110

Part II From Theory to Algorithms

115

9

Linear Predictors

117

9.1 Halfspaces

118

9.1.1 Linear Programming for the Class of Halfspaces

119

9.1.2 Perceptron for Halfspaces

120

9.1.3 The VC Dimension of Halfspaces

122

9.2 Linear Regression

123

9.2.1 Least Squares

124

9.2.2 Linear Regression for Polynomial Regression Tasks

125

9.3 Logistic Regression

126

9.4 Summary

128

9.5 Bibliographic Remarks

128

9.6 Exercises

128

10

Boosting

130

10.1 Weak Learnability

131

10.1.1 Eﬃcient Implementation of ERM for Decision Stumps

133

10.2 AdaBoost

134

10.3 Linear Combinations of Base Hypotheses

137

10.3.1 The VC-Dimension of L(B, T )

139

10.4 AdaBoost for Face Recognition

140

10.5 Summary

141

10.6 Bibliographic Remarks

141

10.7 Exercises

142

11

Model Selection and Validation

144

11.1 Model Selection Using SRM

145

11.2 Validation

146

11.2.1 Hold Out Set

146

11.2.2 Validation for Model Selection

147

11.2.3 The Model-Selection Curve

148

xii

Contents

11.2.4 k-Fold Cross Validation

149

11.2.5 Train-Validation-Test Split

150

11.3 What to Do If Learning Fails

151

11.4 Summary

154

11.5 Exercises

154

12

Convex Learning Problems

156

12.1 Convexity, Lipschitzness, and Smoothness

156

12.1.1 Convexity

156

12.1.2 Lipschitzness

160

12.1.3 Smoothness

162

12.2 Convex Learning Problems

163

12.2.1 Learnability of Convex Learning Problems

164

12.2.2 Convex-Lipschitz/Smooth-Bounded Learning Problems

166

12.3 Surrogate Loss Functions

167

12.4 Summary

168

12.5 Bibliographic Remarks

169

12.6 Exercises

169

13

Regularization and Stability

171

13.1 Regularized Loss Minimization

171

13.1.1 Ridge Regression

172

13.2 Stable Rules Do Not Overﬁt

173

13.3 Tikhonov Regularization as a Stabilizer

174

13.3.1 Lipschitz Loss

176

13.3.2 Smooth and Nonnegative Loss

177

13.4 Controlling the Fitting-Stability Tradeoﬀ

178

13.5 Summary

180

13.6 Bibliographic Remarks

180

13.7 Exercises

181

14

Stochastic Gradient Descent

184

14.1 Gradient Descent

185

14.1.1 Analysis of GD for Convex-Lipschitz Functions

186

14.2 Subgradients

188

14.2.1 Calculating Subgradients

189

14.2.2 Subgradients of Lipschitz Functions

190

14.2.3 Subgradient Descent

190

14.3 Stochastic Gradient Descent (SGD)

191

14.3.1 Analysis of SGD for Convex-Lipschitz-Bounded Functions 191

14.4 Variants

193

14.4.1 Adding a Projection Step

193

14.4.2 Variable Step Size

194

14.4.3 Other Averaging Techniques

195

Contents

xiii

14.4.4 Strongly Convex Functions*

195

14.5 Learning with SGD

196

14.5.1 SGD for Risk Minimization

196

14.5.2 Analyzing SGD for Convex-Smooth Learning Problems

198

14.5.3 SGD for Regularized Loss Minimization

199

14.6 Summary

200

14.7 Bibliographic Remarks

200

14.8 Exercises

201

15

Support Vector Machines

202

15.1 Margin and Hard-SVM

202

15.1.1 The Homogenous Case

205

15.1.2 The Sample Complexity of Hard-SVM

205

15.2 Soft-SVM and Norm Regularization

206

15.2.1 The Sample Complexity of Soft-SVM

208

15.2.2 Margin and Norm-Based Bounds versus Dimension

208

15.2.3 The Ramp Loss*

209

15.3 Optimality Conditions and “Support Vectors”*

210

15.4 Duality*

211

15.5 Implementing Soft-SVM Using SGD

212

15.6 Summary

213

15.7 Bibliographic Remarks

213

15.8 Exercises

214

16

Kernel Methods

215

16.1 Embeddings into Feature Spaces

215

16.2 The Kernel Trick

217

16.2.1 Kernels as a Way to Express Prior Knowledge

221

16.2.2 Characterizing Kernel Functions*

222

16.3 Implementing Soft-SVM with Kernels

222

16.4 Summary

224

16.5 Bibliographic Remarks

225

16.6 Exercises

225

17

Multiclass, Ranking, and Complex Prediction Problems

227

17.1 One-versus-All and All-Pairs

227

17.2 Linear Multiclass Predictors

230

17.2.1 How to Construct Ψ

230

17.2.2 Cost-Sensitive Classiﬁcation

232

17.2.3 ERM

232

17.2.4 Generalized Hinge Loss

233

17.2.5 Multiclass SVM and SGD

234

17.3 Structured Output Prediction

236

17.4 Ranking

238

xiv

Contents

17.4.1 Linear Predictors for Ranking

240

17.5 Bipartite Ranking and Multivariate Performance Measures

243

17.5.1 Linear Predictors for Bipartite Ranking

245

17.6 Summary

247

17.7 Bibliographic Remarks

247

17.8 Exercises

248

18

Decision Trees

250

18.1 Sample Complexity

251

18.2 Decision Tree Algorithms

252

18.2.1 Implementations of the Gain Measure

253

18.2.2 Pruning

254

18.2.3 Threshold-Based Splitting Rules for Real-Valued Features 255

18.3 Random Forests

255

18.4 Summary

256

18.5 Bibliographic Remarks

256

18.6 Exercises

256

19

Nearest Neighbor

258

19.1 k Nearest Neighbors

258

19.2 Analysis

259

19.2.1 A Generalization Bound for the 1-NN Rule

260

19.2.2 The “Curse of Dimensionality”

263

19.3 Eﬃcient Implementation*

264

19.4 Summary

264

19.5 Bibliographic Remarks

264

19.6 Exercises

265

20

Neural Networks

268

20.1 Feedforward Neural Networks

269

20.2 Learning Neural Networks

270

20.3 The Expressive Power of Neural Networks

271

20.3.1 Geometric Intuition

273

20.4 The Sample Complexity of Neural Networks

274

20.5 The Runtime of Learning Neural Networks

276

20.6 SGD and Backpropagation

277

20.7 Summary

281

20.8 Bibliographic Remarks

281

20.9 Exercises

282

Part III Additional Learning Models

285

21

Online Learning

287

21.1 Online Classiﬁcation in the Realizable Case

288

Contents

xv

21.1.1 Online Learnability

290

21.2 Online Classiﬁcation in the Unrealizable Case

294

21.2.1 Weighted-Majority

295

21.3 Online Convex Optimization

300

21.4 The Online Perceptron Algorithm

301

21.5 Summary

304

21.6 Bibliographic Remarks

305

21.7 Exercises

305

22

Clustering

307

22.1 Linkage-Based Clustering Algorithms

310

22.2 k-Means and Other Cost Minimization Clusterings

311

22.2.1 The k-Means Algorithm

313

22.3 Spectral Clustering

315

22.3.1 Graph Cut

315

22.3.2 Graph Laplacian and Relaxed Graph Cuts

315

22.3.3 Unnormalized Spectral Clustering

317

22.4 Information Bottleneck*

317

22.5 A High Level View of Clustering

318

22.6 Summary

320

22.7 Bibliographic Remarks

320

22.8 Exercises

320

23

Dimensionality Reduction

323

23.1 Principal Component Analysis (PCA)

324

23.1.1 A More Eﬃcient Solution for the Case d m

326

23.1.2 Implementation and Demonstration

326

23.2 Random Projections

329

23.3 Compressed Sensing

330

23.3.1 Proofs*

333

23.4 PCA or Compressed Sensing?

338

23.5 Summary

338

23.6 Bibliographic Remarks

339

23.7 Exercises

339

24

Generative Models

342

24.1 Maximum Likelihood Estimator

343

24.1.1 Maximum Likelihood Estimation for Continuous Ran-

dom Variables

344

24.1.2 Maximum Likelihood and Empirical Risk Minimization

345

24.1.3 Generalization Analysis

345

24.2 Naive Bayes

347

24.3 Linear Discriminant Analysis

347

24.4 Latent Variables and the EM Algorithm

348

xvi

Contents

24.4.1 EM as an Alternate Maximization Algorithm

350

24.4.2 EM for Mixture of Gaussians (Soft k-Means)

352

24.5 Bayesian Reasoning

353

24.6 Summary

355

24.7 Bibliographic Remarks

355

24.8 Exercises

356

25

Feature Selection and Generation

357

25.1 Feature Selection

358

25.1.1 Filters

359

25.1.2 Greedy Selection Approaches

360

25.1.3 Sparsity-Inducing Norms

363

25.2 Feature Manipulation and Normalization

365

25.2.1 Examples of Feature Transformations

367

25.3 Feature Learning

368

25.3.1 Dictionary Learning Using Auto-Encoders

368

25.4 Summary

370

25.5 Bibliographic Remarks

371

25.6 Exercises

371

Part IV Advanced Theory

373

26

Rademacher Complexities

375

26.1 The Rademacher Complexity

375

26.1.1 Rademacher Calculus

379

26.2 Rademacher Complexity of Linear Classes

382

26.3 Generalization Bounds for SVM

383

26.4 Generalization Bounds for Predictors with Low 1 Norm

386

26.5 Bibliographic Remarks

386

27

Covering Numbers

388

27.1 Covering

388

27.1.1 Properties

388

27.2 From Covering to Rademacher Complexity via Chaining

389

27.3 Bibliographic Remarks

391

28

Proof of the Fundamental Theorem of Learning Theory

392

28.1 The Upper Bound for the Agnostic Case

392

28.2 The Lower Bound for the Agnostic Case

393

28.2.1 Showing That m( , δ) ≥ 0.5 log(1/(4δ))/ 2

393

28.2.2 Showing That m( , 1/8) ≥ 8d/ 2

395

28.3 The Upper Bound for the Realizable Case

398

28.3.1 From -Nets to PAC Learnability

401

Contents

xvii

29

Multiclass Learnability

402

29.1 The Natarajan Dimension

402

29.2 The Multiclass Fundamental Theorem

403

29.2.1 On the Proof of Theorem 29.3

403

29.3 Calculating the Natarajan Dimension

404

29.3.1 One-versus-All Based Classes

404

29.3.2 General Multiclass-to-Binary Reductions

405

29.3.3 Linear Multiclass Predictors

405

29.4 On Good and Bad ERMs

406

29.5 Bibliographic Remarks

408

29.6 Exercises

409

30

Compression Bounds

410

30.1 Compression Bounds

410

30.2 Examples

412

30.2.1 Axis Aligned Rectangles

412

30.2.2 Halfspaces

412

30.2.3 Separating Polynomials

413

30.2.4 Separation with Margin

414

30.3 Bibliographic Remarks

414

31

PAC-Bayes

415

31.1 PAC-Bayes Bounds

415

31.2 Bibliographic Remarks

417

31.3 Exercises

417

Appendix A Technical Lemmas

419

Appendix B Measure Concentration

422

Appendix C Linear Algebra

430

Notes

435

References

437

Index

447

1 Introduction
The subject of this book is automated learning, or, as we will more often call it, Machine Learning (ML). That is, we wish to program computers so that they can “learn” from input available to them. Roughly speaking, learning is the process of converting experience into expertise or knowledge. The input to a learning algorithm is training data, representing experience, and the output is some expertise, which usually takes the form of another computer program that can perform some task. Seeking a formal-mathematical understanding of this concept, we’ll have to be more explicit about what we mean by each of the involved terms: What is the training data our programs will access? How can the process of learning be automated? How can we evaluate the success of such a process (namely, the quality of the output of a learning program)?
1.1 What Is Learning?
Let us begin by considering a couple of examples from naturally occurring animal learning. Some of the most fundamental issues in ML arise already in that context, which we are all familiar with.
Bait Shyness – Rats Learning to Avoid Poisonous Baits: When rats encounter food items with novel look or smell, they will ﬁrst eat very small amounts, and subsequent feeding will depend on the ﬂavor of the food and its physiological eﬀect. If the food produces an ill eﬀect, the novel food will often be associated with the illness, and subsequently, the rats will not eat it. Clearly, there is a learning mechanism in play here – the animal used past experience with some food to acquire expertise in detecting the safety of this food. If past experience with the food was negatively labeled, the animal predicts that it will also have a negative eﬀect when encountered in the future.
Inspired by the preceding example of successful learning, let us demonstrate a typical machine learning task. Suppose we would like to program a machine that learns how to ﬁlter spam e-mails. A naive solution would be seemingly similar to the way rats learn how to avoid poisonous baits. The machine will simply memorize all previous e-mails that had been labeled as spam e-mails by the human user. When a new e-mail arrives, the machine will search for it in the set
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

20

Introduction

of previous spam e-mails. If it matches one of them, it will be trashed. Otherwise, it will be moved to the user’s inbox folder.
While the preceding “learning by memorization” approach is sometimes useful, it lacks an important aspect of learning systems – the ability to label unseen e-mail messages. A successful learner should be able to progress from individual examples to broader generalization. This is also referred to as inductive reasoning or inductive inference. In the bait shyness example presented previously, after the rats encounter an example of a certain type of food, they apply their attitude toward it on new, unseen examples of food of similar smell and taste. To achieve generalization in the spam ﬁltering task, the learner can scan the previously seen e-mails, and extract a set of words whose appearance in an e-mail message is indicative of spam. Then, when a new e-mail arrives, the machine can check whether one of the suspicious words appears in it, and predict its label accordingly. Such a system would potentially be able correctly to predict the label of unseen e-mails.
However, inductive reasoning might lead us to false conclusions. To illustrate this, let us consider again an example from animal learning.
Pigeon Superstition: In an experiment performed by the psychologist B. F. Skinner, he placed a bunch of hungry pigeons in a cage. An automatic mechanism had been attached to the cage, delivering food to the pigeons at regular intervals with no reference whatsoever to the birds’ behavior. The hungry pigeons went around the cage, and when food was ﬁrst delivered, it found each pigeon engaged in some activity (pecking, turning the head, etc.). The arrival of food reinforced each bird’s speciﬁc action, and consequently, each bird tended to spend some more time doing that very same action. That, in turn, increased the chance that the next random food delivery would ﬁnd each bird engaged in that activity again. What results is a chain of events that reinforces the pigeons’ association of the delivery of the food with whatever chance actions they had been performing when it was ﬁrst delivered. They subsequently continue to perform these same actions diligently.1
What distinguishes learning mechanisms that result in superstition from useful learning? This question is crucial to the development of automated learners. While human learners can rely on common sense to ﬁlter out random meaningless learning conclusions, once we export the task of learning to a machine, we must provide well deﬁned crisp principles that will protect the program from reaching senseless or useless conclusions. The development of such principles is a central goal of the theory of machine learning.
What, then, made the rats’ learning more successful than that of the pigeons? As a ﬁrst step toward answering this question, let us have a closer look at the bait shyness phenomenon in rats.
Bait Shyness revisited – rats fail to acquire conditioning between food and electric shock or between sound and nausea: The bait shyness mechanism in
1 See: http://psychclassics.yorku.ca/Skinner/Pigeon

1.2 When Do We Need Machine Learning?

21

rats turns out to be more complex than what one may expect. In experiments carried out by Garcia (Garcia & Koelling 1996), it was demonstrated that if the unpleasant stimulus that follows food consumption is replaced by, say, electrical shock (rather than nausea), then no conditioning occurs. Even after repeated trials in which the consumption of some food is followed by the administration of unpleasant electrical shock, the rats do not tend to avoid that food. Similar failure of conditioning occurs when the characteristic of the food that implies nausea (such as taste or smell) is replaced by a vocal signal. The rats seem to have some “built in” prior knowledge telling them that, while temporal correlation between food and nausea can be causal, it is unlikely that there would be a causal relationship between food consumption and electrical shocks or between sounds and nausea.
We conclude that one distinguishing feature between the bait shyness learning and the pigeon superstition is the incorporation of prior knowledge that biases the learning mechanism. This is also referred to as inductive bias. The pigeons in the experiment are willing to adopt any explanation for the occurrence of food. However, the rats “know” that food cannot cause an electric shock and that the co-occurrence of noise with some food is not likely to aﬀect the nutritional value of that food. The rats’ learning process is biased toward detecting some kind of patterns while ignoring other temporal correlations between events.
It turns out that the incorporation of prior knowledge, biasing the learning process, is inevitable for the success of learning algorithms (this is formally stated and proved as the “No-Free-Lunch theorem” in Chapter 5). The development of tools for expressing domain expertise, translating it into a learning bias, and quantifying the eﬀect of such a bias on the success of learning is a central theme of the theory of machine learning. Roughly speaking, the stronger the prior knowledge (or prior assumptions) that one starts the learning process with, the easier it is to learn from further examples. However, the stronger these prior assumptions are, the less ﬂexible the learning is – it is bound, a priori, by the commitment to these assumptions. We shall discuss these issues explicitly in Chapter 5.
1.2 When Do We Need Machine Learning?
When do we need machine learning rather than directly program our computers to carry out the task at hand? Two aspects of a given problem may call for the use of programs that learn and improve on the basis of their “experience”: the problem’s complexity and the need for adaptivity.
Tasks That Are Too Complex to Program.
• Tasks Performed by Animals/Humans: There are numerous tasks that we human beings perform routinely, yet our introspection concerning how we do them is not suﬃciently elaborate to extract a well

22

Introduction

deﬁned program. Examples of such tasks include driving, speech recognition, and image understanding. In all of these tasks, state of the art machine learning programs, programs that “learn from their experience,” achieve quite satisfactory results, once exposed to suﬃciently many training examples. • Tasks beyond Human Capabilities: Another wide family of tasks that beneﬁt from machine learning techniques are related to the analysis of very large and complex data sets: astronomical data, turning medical archives into medical knowledge, weather prediction, analysis of genomic data, Web search engines, and electronic commerce. With more and more available digitally recorded data, it becomes obvious that there are treasures of meaningful information buried in data archives that are way too large and too complex for humans to make sense of. Learning to detect meaningful patterns in large and complex data sets is a promising domain in which the combination of programs that learn with the almost unlimited memory capacity and ever increasing processing speed of computers opens up new horizons.
Adaptivity. One limiting feature of programmed tools is their rigidity – once the program has been written down and installed, it stays unchanged. However, many tasks change over time or from one user to another. Machine learning tools – programs whose behavior adapts to their input data – oﬀer a solution to such issues; they are, by nature, adaptive to changes in the environment they interact with. Typical successful applications of machine learning to such problems include programs that decode handwritten text, where a ﬁxed program can adapt to variations between the handwriting of diﬀerent users; spam detection programs, adapting automatically to changes in the nature of spam e-mails; and speech recognition programs.
1.3 Types of Learning
Learning is, of course, a very wide domain. Consequently, the ﬁeld of machine learning has branched into several subﬁelds dealing with diﬀerent types of learning tasks. We give a rough taxonomy of learning paradigms, aiming to provide some perspective of where the content of this book sits within the wide ﬁeld of machine learning.
We describe four parameters along which learning paradigms can be classiﬁed.
Supervised versus Unsupervised Since learning involves an interaction between the learner and the environment, one can divide learning tasks according to the nature of that interaction. The ﬁrst distinction to note is the diﬀerence between supervised and unsupervised learning. As an

1.3 Types of Learning

23

illustrative example, consider the task of learning to detect spam e-mail

versus the task of anomaly detection. For the spam detection task, we

consider a setting in which the learner receives training e-mails for which

the label spam/not-spam is provided. On the basis of such training the

learner should ﬁgure out a rule for labeling a newly arriving e-mail mes-

sage. In contrast, for the task of anomaly detection, all the learner gets

as training is a large body of e-mail messages (with no labels) and the

learner’s task is to detect “unusual” messages.

More abstractly, viewing learning as a process of “using experience

to gain expertise,” supervised learning describes a scenario in which the

“experience,” a training example, contains signiﬁcant information (say,

the spam/not-spam labels) that is missing in the unseen “test examples”

to which the learned expertise is to be applied. In this setting, the ac-

quired expertise is aimed to predict that missing information for the test

data. In such cases, we can think of the environment as a teacher that

“supervises” the learner by providing the extra information (labels). In

unsupervised learning, however, there is no distinction between training

and test data. The learner processes input data with the goal of coming

up with some summary, or compressed version of that data. Clustering

a data set into subsets of similar objets is a typical example of such a

task.

There is also an intermediate learning setting in which, while the

training examples contain more information than the test examples, the

learner is required to predict even more information for the test exam-

ples. For example, one may try to learn a value function that describes for

each setting of a chess board the degree by which White’s position is bet-

ter than the Black’s. Yet, the only information available to the learner at

training time is positions that occurred throughout actual chess games,

labeled by who eventually won that game. Such learning frameworks are

mainly investigated under the title of reinforcement learning.

Active versus Passive Learners Learning paradigms can vary by the role

played by the learner. We distinguish between “active” and “passive”

learners. An active learner interacts with the environment at training

time, say, by posing queries or performing experiments, while a passive

learner only observes the information provided by the environment (or

the teacher) without inﬂuencing or directing it. Note that the learner of a

spam ﬁlter is usually passive – waiting for users to mark the e-mails com-

ing to them. In an active setting, one could imagine asking users to label

speciﬁc e-mails chosen by the learner, or even composed by the learner, to

enhance

its

understanding

of

what

spam is.

Helpfulness of the Teacher When one thinks about human learning, of a

baby at home or a student at school, the process often involves a helpful

teacher, who is trying to feed the learner with the information most use-

24

Introduction

ful for achieving the learning goal. In contrast, when a scientist learns about nature, the environment, playing the role of the teacher, can be best thought of as passive – apples drop, stars shine, and the rain falls without regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learner’s experience) is generated by some random process. This is the basic building block in the branch of “statistical learning.” Finally, learning also occurs when the learner’s input is generated by an adversarial “teacher.” This may be the case in the spam ﬁltering example (if the spammer makes an eﬀort to mislead the spam ﬁltering designer) or in learning to detect fraud. One also uses an adversarial teacher model as a worst-case scenario, when no milder setup can be safely assumed. If you can learn against an adversarial teacher, you are guaranteed to succeed interacting any odd teacher. Online versus Batch Learning Protocol The last parameter we mention is the distinction between situations in which the learner has to respond online, throughout the learning process, and settings in which the learner has to engage the acquired expertise only after having a chance to process large amounts of data. For example, a stockbroker has to make daily decisions, based on the experience collected so far. He may become an expert over time, but might have made costly mistakes in the process. In contrast, in many data mining settings, the learner – the data miner – has large amounts of training data to play with before having to output conclusions.
In this book we shall discuss only a subset of the possible learning paradigms. Our main focus is on supervised statistical batch learning with a passive learner (for example, trying to learn how to generate patients’ prognoses, based on large archives of records of patients that were independently collected and are already labeled by the fate of the recorded patients). We shall also brieﬂy discuss online learning and batch unsupervised learning (in particular, clustering).
1.4 Relations to Other Fields
As an interdisciplinary ﬁeld, machine learning shares common threads with the mathematical ﬁelds of statistics, information theory, game theory, and optimization. It is naturally a subﬁeld of computer science, as our goal is to program machines so that they will learn. In a sense, machine learning can be viewed as a branch of AI (Artiﬁcial Intelligence), since, after all, the ability to turn experience into expertise or to detect meaningful patterns in complex sensory data is a cornerstone of human (and animal) intelligence. However, one should note that, in contrast with traditional AI, machine learning is not trying to build automated imitation of intelligent behavior, but rather to use the strengths and

1.5 How to Read This Book

25

special abilities of computers to complement human intelligence, often performing tasks that fall way beyond human capabilities. For example, the ability to scan and process huge databases allows machine learning programs to detect patterns that are outside the scope of human perception.
The component of experience, or training, in machine learning often refers to data that is randomly generated. The task of the learner is to process such randomly generated examples toward drawing conclusions that hold for the environment from which these examples are picked. This description of machine learning highlights its close relationship with statistics. Indeed there is a lot in common between the two disciplines, in terms of both the goals and techniques used. There are, however, a few signiﬁcant diﬀerences of emphasis; if a doctor comes up with the hypothesis that there is a correlation between smoking and heart disease, it is the statistician’s role to view samples of patients and check the validity of that hypothesis (this is the common statistical task of hypothesis testing). In contrast, machine learning aims to use the data gathered from samples of patients to come up with a description of the causes of heart disease. The hope is that automated techniques may be able to ﬁgure out meaningful patterns (or hypotheses) that may have been missed by the human observer.
In contrast with traditional statistics, in machine learning in general, and in this book in particular, algorithmic considerations play a major role. Machine learning is about the execution of learning by computers; hence algorithmic issues are pivotal. We develop algorithms to perform the learning tasks and are concerned with their computational eﬃciency. Another diﬀerence is that while statistics is often interested in asymptotic behavior (like the convergence of sample-based statistical estimates as the sample sizes grow to inﬁnity), the theory of machine learning focuses on ﬁnite sample bounds. Namely, given the size of available samples, machine learning theory aims to ﬁgure out the degree of accuracy that a learner can expect on the basis of such samples.
There are further diﬀerences between these two disciplines, of which we shall mention only one more here. While in statistics it is common to work under the assumption of certain presubscribed data models (such as assuming the normality of data-generating distributions, or the linearity of functional dependencies), in machine learning the emphasis is on working under a “distribution-free” setting, where the learner assumes as little as possible about the nature of the data distribution and allows the learning algorithm to ﬁgure out which models best approximate the data-generating process. A precise discussion of this issue requires some technical preliminaries, and we will come back to it later in the book, and in particular in Chapter 5.
1.5 How to Read This Book
The ﬁrst part of the book provides the basic theoretical principles that underlie machine learning (ML). In a sense, this is the foundation upon which the rest

26

Introduction

of the book is built. This part could serve as a basis for a minicourse on the theoretical foundations of ML.
The second part of the book introduces the most commonly used algorithmic approaches to supervised machine learning. A subset of these chapters may also be used for introducing machine learning in a general AI course to computer science, Math, or engineering students.
The third part of the book extends the scope of discussion from statistical classiﬁcation to other learning models. It covers online learning, unsupervised learning, dimensionality reduction, generative models, and feature learning.
The fourth part of the book, Advanced Theory, is geared toward readers who have interest in research and provides the more technical mathematical techniques that serve to analyze and drive forward the ﬁeld of theoretical machine learning.
The Appendixes provide some technical tools used in the book. In particular, we list basic results from measure concentration and linear algebra.
A few sections are marked by an asterisk, which means they are addressed to more advanced students. Each chapter is concluded with a list of exercises. A solution manual is provided in the course Web site.

1.5.1

Possible Course Plans Based on This Book
A 14 Week Introduction Course for Graduate Students:
1. Chapters 2–4. 2. Chapter 9 (without the VC calculation). 3. Chapters 5–6 (without proofs). 4. Chapter 10. 5. Chapters 7, 11 (without proofs). 6. Chapters 12, 13 (with some of the easier proofs). 7. Chapter 14 (with some of the easier proofs). 8. Chapter 15. 9. Chapter 16. 10. Chapter 18. 11. Chapter 22. 12. Chapter 23 (without proofs for compressed sensing). 13. Chapter 24. 14. Chapter 25.
A 14 Week Advanced Course for Graduate Students:
1. Chapters 26, 27. 2. (continued) 3. Chapters 6, 28. 4. Chapter 7. 5. Chapter 31.

1.6 Notation

27

6. Chapter 30. 7. Chapters 12, 13. 8. Chapter 14. 9. Chapter 8. 10. Chapter 17. 11. Chapter 29. 12. Chapter 19. 13. Chapter 20. 14. Chapter 21.
1.6 Notation
Most of the notation we use throughout the book is either standard or deﬁned on the spot. In this section we describe our main conventions and provide a table summarizing our notation (Table 1.1). The reader is encouraged to skip this section and return to it if during the reading of the book some notation is unclear.
We denote scalars and abstract objects with lowercase letters (e.g. x and λ). Often, we would like to emphasize that some object is a vector and then we use boldface letters (e.g. x and λ). The ith element of a vector x is denoted by xi. We use uppercase letters to denote matrices, sets, and sequences. The meaning should be clear from the context. As we will see momentarily, the input of a learning algorithm is a sequence of training examples. We denote by z an abstract example and by S = z1, . . . , zm a sequence of m examples. Historically, S is often referred to as a training set; however, we will always assume that S is a sequence rather than a set. A sequence of m vectors is denoted by x1, . . . , xm. The ith element of xt is denoted by xt,i.
Throughout the book, we make use of basic notions from probability. We denote by D a distribution over some set,2 for example, Z. We use the notation z ∼ D to denote that z is sampled according to D. Given a random variable f : Z → R, its expected value is denoted by Ez∼D[f (z)]. We sometimes use the shorthand E[f ] when the dependence on z is clear from the context. For f : Z → {true, false} we also use Pz∼D[f (z)] to denote D({z : f (z) = true}). In the next chapter we will also introduce the notation Dm to denote the probability over Zm induced by sampling (z1, . . . , zm) where each point zi is sampled from D independently of the other points.
In general, we have made an eﬀort to avoid asymptotic notation. However, we occasionally use it to clarify the main results. In particular, given f : R → R+ and g : R → R+ we write f = O(g) if there exist x0, α ∈ R+ such that for all x > x0 we have f (x) ≤ αg(x). We write f = o(g) if for every α > 0 there exists
2 To be mathematically precise, D should be deﬁned over some σ-algebra of subsets of Z. The user who is not familiar with measure theory can skip the few footnotes and remarks regarding more formal measurability deﬁnitions and assumptions.

28

Introduction

symbol

Table 1.1 Summary of notation meaning

R Rd
R+
N O, o, Θ, ω, Ω, O˜ 1[Boolean expression] [a]+ [n] x, v, w xi, vi, wi
x, v
x 2 or x
x1 x∞ x0 A ∈ Rd,k A Ai,j xx x1, . . . , xm xi,j w(1), . . . , w(T ) wi(t) X Y Z H : H × Z → R+ D D(A) z∼D S = z1, . . . , zm S ∼ Dm P, E Pz∼D[f (z)] Ez∼D[f (z)] N (µ, C) f (x) f (x)
∂f (w) ∂wi
∇f (w)
∂f (w) minx∈C f (x) maxx∈C f (x) argminx∈C f (x) argmaxx∈C f (x) log

the set of real numbers

the set of d-dimensional vectors over R

the set of non-negative real numbers

the set of natural numbers

asymptotic notation (see text)

indicator function (equals 1 if expression is true and 0 o.w.)

= max{0, a}

the set {1, . . . , n} (for n ∈ N)

(column) vectors

the ith element of a vector

=

d i=1

xivi

(inner

product)

= x, x (the 2 norm of x)

=

d i=1

|xi|

(the

1 norm of x)

= maxi |xi| (the ∞ norm of x)

the number of nonzero elements of x

a d × k matrix over R

the transpose of A

the (i, j) element of A the d × d matrix A s.t. Ai,j = xixj (where x ∈ Rd) a sequence of m vectors

the jth element of the ith vector in the sequence

the values of a vector w during an iterative algorithm

the ith element of the vector w(t)

instances domain (a set)

labels domain (a set)

examples domain (a set)

hypothesis class (a set)

loss function

a distribution over some set (usually over Z or over X )

the probability of a set A ⊆ Z according to D

sampling z according to D

a sequence of m examples

sampling S = z1, . . . , zm i.i.d. according to D probability and expectation of a random variable

= D({z : f (z) = true}) for f : Z → {true, false}

expectation of the random variable f : Z → R

Gaussian distribution with expectation µ and covariance C

the derivative of a function f : R → R at x

the second derivative of a function f : R → R at x the partial derivative of a function f : Rd → R at w w.r.t. wi the gradient of a function f : Rd → R at w the diﬀerential set of a function f : Rd → R at w

= min{f (x) : x ∈ C} (minimal value of f over C)

= max{f (x) : x ∈ C} (maximal value of f over C)

the set {x ∈ C : f (x) = minz∈C f (z)} the set {x ∈ C : f (x) = maxz∈C f (z)} the natural logarithm

1.6 Notation

29

x0 such that for all x > x0 we have f (x) ≤ αg(x). We write f = Ω(g) if there

exist x0, α ∈ R+ such that for all x > x0 we have f (x) ≥ αg(x). The notation

f = ω(g) is deﬁned analogously. The notation f = Θ(g) means that f = O(g) and g = O(f ). Finally, the notation f = O˜(g) means that there exists k ∈ N such that f (x) = O(g(x) logk(g(x))).

The inner product between vectors x and w is denoted by x, w . Whenever we

do not specify the vector space we assume that it is the d-dimensional Euclidean

space and then x, w =

d i=1

xiwi.

The

Euclidean

(or

2) norm of a vector w is

w 2 = w, w . We omit the subscript from the 2 norm when it is clear from

the context. We also use other p norms, w p = ( i |wi|p)1/p, and in particular

w 1 = i |wi| and w ∞ = maxi |wi|.

We use the notation minx∈C f (x) to denote the minimum value of the set

{f (x) : x ∈ C}. To be mathematically more precise, we should use infx∈C f (x)

whenever the minimum is not achievable. However, in the context of this book

the distinction between inﬁmum and minimum is often of little interest. Hence,

to simplify the presentation, we sometimes use the min notation even when inf

is more adequate. An analogous remark applies to max versus sup.

Part I Foundations

2 A Gentle Start
Let us begin our mathematical analysis by showing how successful learning can be achieved in a relatively simpliﬁed setting. Imagine you have just arrived in some small Paciﬁc island. You soon ﬁnd out that papayas are a signiﬁcant ingredient in the local diet. However, you have never before tasted papayas. You have to learn how to predict whether a papaya you see in the market is tasty or not. First, you need to decide which features of a papaya your prediction should be based on. On the basis of your previous experience with other fruits, you decide to use two features: the papaya’s color, ranging from dark green, through orange and red to dark brown, and the papaya’s softness, ranging from rock hard to mushy. Your input for ﬁguring out your prediction rule is a sample of papayas that you have examined for color and softness and then tasted and found out whether they were tasty or not. Let us analyze this task as a demonstration of the considerations involved in learning problems.
Our ﬁrst step is to describe a formal model aimed to capture such learning tasks.
2.1 A Formal Model – The Statistical Learning Framework
• The learner’s input: In the basic statistical learning setting, the learner has access to the following: – Domain set: An arbitrary set, X . This is the set of objects that we may wish to label. For example, in the papaya learning problem mentioned before, the domain set will be the set of all papayas. Usually, these domain points will be represented by a vector of features (like the papaya’s color and softness). We also refer to domain points as instances and to X as instance space. – Label set: For our current discussion, we will restrict the label set to be a two-element set, usually {0, 1} or {−1, +1}. Let Y denote our set of possible labels. For our papayas example, let Y be {0, 1}, where 1 represents being tasty and 0 stands for being not-tasty. – Training data: S = ((x1, y1) . . . (xm, ym)) is a ﬁnite sequence of pairs in X × Y: that is, a sequence of labeled domain points. This is the input that the learner has access to (like a set of papayas that have been
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

34

A Gentle Start

tasted and their color, softness, and tastiness). Such labeled examples are often called training examples. We sometimes also refer to S as a training set.1 • The learner’s output: The learner is requested to output a prediction rule, h : X → Y. This function is also called a predictor, a hypothesis, or a classiﬁer. The predictor can be used to predict the label of new domain points. In our papayas example, it is a rule that our learner will employ to predict whether future papayas he examines in the farmers’ market are going to be tasty or not. We use the notation A(S) to denote the hypothesis that a learning algorithm, A, returns upon receiving the training sequence S. • A simple data-generation model We now explain how the training data is generated. First, we assume that the instances (the papayas we encounter) are generated by some probability distribution (in this case, representing the environment). Let us denote that probability distribution over X by D. It is important to note that we do not assume that the learner knows anything about this distribution. For the type of learning tasks we discuss, this could be any arbitrary probability distribution. As to the labels, in the current discussion we assume that there is some “correct” labeling function, f : X → Y, and that yi = f (xi) for all i. This assumption will be relaxed in the next chapter. The labeling function is unknown to the learner. In fact, this is just what the learner is trying to ﬁgure out. In summary, each pair in the training data S is generated by ﬁrst sampling a point xi according to D and then labeling it by f . • Measures of success: We deﬁne the error of a classiﬁer to be the probability that it does not predict the correct label on a random data point generated by the aforementioned underlying distribution. That is, the error of h is the probability to draw a random instance x, according to the distribution D, such that h(x) does not equal f (x). Formally, given a domain subset,2 A ⊂ X , the probability distribution, D, assigns a number, D(A), which determines how likely it is to observe a point x ∈ A. In many cases, we refer to A as an event and express it using a function π : X → {0, 1}, namely, A = {x ∈ X : π(x) = 1}. In that case, we also use the notation Px∼D[π(x)] to express D(A). We deﬁne the error of a prediction rule, h : X → Y, to be

LD,f (h) d=ef P [h(x) = f (x)] d=ef D({x : h(x) = f (x)}).
x∼D

(2.1)

That is, the error of such h is the probability of randomly choosing an example x for which h(x) = f (x). The subscript (D, f ) indicates that the error is measured with respect to the probability distribution D and the

1 Despite the “set” notation, S is a sequence. In particular, the same example may appear twice in S and some algorithms can take into account the order of examples in S.
2 Strictly speaking, we should be more careful and require that A is a member of some σ-algebra of subsets of X , over which D is deﬁned. We will formally deﬁne our measurability assumptions in the next chapter.

2.2 Empirical Risk Minimization

35

correct labeling function f . We omit this subscript when it is clear from the context. L(D,f)(h) has several synonymous names such as the generalization error, the risk, or the true error of h, and we will use these names interchangeably throughout the book. We use the letter L for the error, since we view this error as the loss of the learner. We will later also discuss other possible formulations of such loss.
• A note about the information available to the learner The learner is blind to the underlying distribution D over the world and to the labeling function f. In our papayas example, we have just arrived in a new island and we have no clue as to how papayas are distributed and how to predict their tastiness. The only way the learner can interact with the environment is through observing the training set.
In the next section we describe a simple learning paradigm for the preceding setup and analyze its performance.

2.2 Empirical Risk Minimization

As mentioned earlier, a learning algorithm receives as input a training set S, sampled from an unknown distribution D and labeled by some target function f , and should output a predictor hS : X → Y (the subscript S emphasizes the fact that the output predictor depends on S). The goal of the algorithm is to ﬁnd hS that minimizes the error with respect to the unknown D and f .
Since the learner does not know what D and f are, the true error is not directly available to the learner. A useful notion of error that can be calculated by the learner is the training error – the error the classiﬁer incurs over the training sample:

LS (h) d=ef

|{i ∈ [m] : h(xi) = yi}| , m

(2.2)

where [m] = {1, . . . , m}. The terms empirical error and empirical risk are often used interchangeably
for this error. Since the training sample is the snapshot of the world that is available to the
learner, it makes sense to search for a solution that works well on that data. This learning paradigm – coming up with a predictor h that minimizes LS(h) – is called Empirical Risk Minimization or ERM for short.

2.2.1

Something May Go Wrong – Overﬁtting
Although the ERM rule seems very natural, without being careful, this approach may fail miserably.
To demonstrate such a failure, let us go back to the problem of learning to

36

A Gentle Start

predict the taste of a papaya on the basis of its softness and color. Consider a sample as depicted in the following:

Assume that the probability distribution D is such that instances are distributed uniformly within the gray square and the labeling function, f , determines the label to be 1 if the instance is within the inner blue square, and 0 otherwise. The area of the gray square in the picture is 2 and the area of the blue square is 1. Consider the following predictor:

hS(x) = yi if ∃i ∈ [m] s.t. xi = x 0 otherwise.

(2.3)

While this predictor might seem rather artiﬁcial, in Exercise 1 we show a natural representation of it using polynomials. Clearly, no matter what the sample is, LS(hS) = 0, and therefore this predictor may be chosen by an ERM algorithm (it is one of the empirical-minimum-cost hypotheses; no classiﬁer can have smaller error). On the other hand, the true error of any classiﬁer that predicts the label 1 only on a ﬁnite number of instances is, in this case, 1/2. Thus, LD(hS) = 1/2. We have found a predictor whose performance on the training set is excellent, yet its performance on the true “world” is very poor. This phenomenon is called overﬁtting. Intuitively, overﬁtting occurs when our hypothesis ﬁts the training data “too well” (perhaps like the everyday experience that a person who provides a perfect detailed explanation for each of his single actions may raise suspicion).

2.3 Empirical Risk Minimization with Inductive Bias
We have just demonstrated that the ERM rule might lead to overﬁtting. Rather than giving up on the ERM paradigm, we will look for ways to rectify it. We will search for conditions under which there is a guarantee that ERM does not overﬁt, namely, conditions under which when the ERM predictor has good performance with respect to the training data, it is also highly likely to perform well over the underlying data distribution.
A common solution is to apply the ERM learning rule over a restricted search space. Formally, the learner should choose in advance (before seeing the data) a set of predictors. This set is called a hypothesis class and is denoted by H. Each h ∈ H is a function mapping from X to Y. For a given class H, and a training sample, S, the ERMH learner uses the ERM rule to choose a predictor h ∈ H,

2.3 Empirical Risk Minimization with Inductive Bias

37

with the lowest possible error over S. Formally,
ERMH(S) ∈ argmin LS(h),
h∈H
where argmin stands for the set of hypotheses in H that achieve the minimum value of LS(h) over H. By restricting the learner to choosing a predictor from H, we bias it toward a particular set of predictors. Such restrictions are often called an inductive bias. Since the choice of such a restriction is determined before the learner sees the training data, it should ideally be based on some prior knowledge about the problem to be learned. For example, for the papaya taste prediction problem we may choose the class H to be the set of predictors that are determined by axis aligned rectangles (in the space determined by the color and softness coordinates). We will later show that ERMH over this class is guaranteed not to overﬁt. On the other hand, the example of overﬁtting that we have seen previously, demonstrates that choosing H to be a class of predictors that includes all functions that assign the value 1 to a ﬁnite set of domain points does not suﬃce to guarantee that ERMH will not overﬁt.
A fundamental question in learning theory is, over which hypothesis classes ERMH learning will not result in overﬁtting. We will study this question later in the book.
Intuitively, choosing a more restricted hypothesis class better protects us against overﬁtting but at the same time might cause us a stronger inductive bias. We will get back to this fundamental tradeoﬀ later.

2.3.1

Finite Hypothesis Classes

The simplest type of restriction on a class is imposing an upper bound on its size (that is, the number of predictors h in H). In this section, we show that if H is a ﬁnite class then ERMH will not overﬁt, provided it is based on a suﬃciently large training sample (this size requirement will depend on the size of H).
Limiting the learner to prediction rules within some ﬁnite hypothesis class may be considered as a reasonably mild restriction. For example, H can be the set of all predictors that can be implemented by a C++ program written in at most 109 bits of code. In our papayas example, we mentioned previously the class of axis aligned rectangles. While this is an inﬁnite class, if we discretize the representation of real numbers, say, by using a 64 bits ﬂoating-point representation, the hypothesis class becomes a ﬁnite class.
Let us now analyze the performance of the ERMH learning rule assuming that H is a ﬁnite class. For a training sample, S, labeled according to some f : X → Y, let hS denote a result of applying ERMH to S, namely,

hS ∈ argmin LS(h).
h∈H

(2.4)

In this chapter, we make the following simplifying assumption (which will be relaxed in the next chapter).

38

A Gentle Start

definition 2.1 (The Realizability Assumption) There exists h ∈ H s.t. L(D,f)(h ) = 0. Note that this assumption implies that with probability 1 over random samples, S, where the instances of S are sampled according to D and are labeled by f , we have LS(h ) = 0.
The realizability assumption implies that for every ERM hypothesis we have that3 LS(hS) = 0. However, we are interested in the true risk of hS, L(D,f)(hS), rather than its empirical risk.
Clearly, any guarantee on the error with respect to the underlying distribution, D, for an algorithm that has access only to a sample S should depend on the relationship between D and S. The common assumption in statistical machine learning is that the training sample S is generated by sampling points from the distribution D independently of each other. Formally,
• The i.i.d. assumption: The examples in the training set are independently and identically distributed (i.i.d.) according to the distribution D. That is, every xi in S is freshly sampled according to D and then labeled according to the labeling function, f . We denote this assumption by S ∼ Dm where m is the size of S, and Dm denotes the probability over m-tuples induced by applying D to pick each element of the tuple independently of the other members of the tuple. Intuitively, the training set S is a window through which the learner gets partial information about the distribution D over the world and the labeling function, f . The larger the sample gets, the more likely it is to reﬂect more accurately the distribution and labeling used to generate it.
Since L(D,f)(hS) depends on the training set, S, and that training set is picked by a random process, there is randomness in the choice of the predictor hS and, consequently, in the risk L(D,f)(hS). Formally, we say that it is a random variable. It is not realistic to expect that with full certainty S will suﬃce to direct the learner toward a good classiﬁer (from the point of view of D), as there is always some probability that the sampled training data happens to be very nonrepresentative of the underlying D. If we go back to the papaya tasting example, there is always some (small) chance that all the papayas we have happened to taste were not tasty, in spite of the fact that, say, 70% of the papayas in our island are tasty. In such a case, ERMH(S) may be the constant function that labels every papaya as “not tasty” (and has 70% error on the true distribution of papapyas in the island). We will therefore address the probability to sample a training set for which L(D,f)(hS) is not too large. Usually, we denote the probability of getting a nonrepresentative sample by δ, and call (1 − δ) the conﬁdence parameter of our prediction.
On top of that, since we cannot guarantee perfect label prediction, we introduce another parameter for the quality of prediction, the accuracy parameter,
3 Mathematically speaking, this holds with probability 1. To simplify the presentation, we sometimes omit the “with probability 1” speciﬁer.

2.3 Empirical Risk Minimization with Inductive Bias

39

commonly denoted by . We interpret the event L(D,f)(hS) > as a failure of the learner, while if L(D,f)(hS) ≤ we view the output of the algorithm as an approximately correct predictor. Therefore (ﬁxing some labeling function f : X → Y), we are interested in upper bounding the probability to sample m-tuple of instances that will lead to failure of the learner. Formally, let S|x = (x1, . . . , xm) be the instances of the training set. We would like to upper bound
Dm({S|x : L(D,f)(hS) > }).
Let HB be the set of “bad” hypotheses, that is,
HB = {h ∈ H : L(D,f)(h) > }.
In addition, let
M = {S|x : ∃h ∈ HB, LS(h) = 0}
be the set of misleading samples: Namely, for every S|x ∈ M , there is a “bad” hypothesis, h ∈ HB, that looks like a “good” hypothesis on S|x. Now, recall that we would like to bound the probability of the event L(D,f)(hS) > . But, since the realizability assumption implies that LS(hS) = 0, it follows that the event L(D,f)(hS) > can only happen if for some h ∈ HB we have LS(h) = 0. In other words, this event will only happen if our sample is in the set of misleading samples, M . Formally, we have shown that

{S|x : L(D,f)(hS) > } ⊆ M . Note that we can rewrite M as

Hence,

M=

{S|x : LS(h) = 0}.

h∈HB

(2.5)

Dm({S|x : L(D,f)(hS) > }) ≤ Dm(M ) = Dm(∪h∈HB {S|x : LS(h) = 0}). (2.6)
Next, we upper bound the right-hand side of the preceding equation using the union bound – a basic property of probabilities.

lemma 2.2 (Union Bound) For any two sets A, B and a distribution D we have
D(A ∪ B) ≤ D(A) + D(B).

Applying the union bound to the right-hand side of Equation (2.6) yields

Dm({S|x : L(D,f)(hS) > }) ≤

Dm({S|x : LS(h) = 0}).

h∈HB

(2.7)

Next, let us bound each summand of the right-hand side of the preceding inequality. Fix some “bad” hypothesis h ∈ HB. The event LS(h) = 0 is equivalent

40

A Gentle Start

to the event ∀i, h(xi) = f (xi). Since the examples in the training set are sampled i.i.d. we get that

Dm({S|x : LS(h) = 0}) = Dm({S|x : ∀i, h(xi) = f (xi)})
m
= D({xi : h(xi) = f (xi)}).
i=1

(2.8)

For each individual sampling of an element of the training set we have

D({xi : h(xi) = yi}) = 1 − L(D,f)(h) ≤ 1 − ,

where the last inequality follows from the fact that h ∈ HB. Combining the previous equation with Equation (2.8) and using the inequality 1 − ≤ e− we
obtain that for every h ∈ HB,

Dm({S|x : LS(h) = 0}) ≤ (1 − )m ≤ e− m.

(2.9)

Combining this equation with Equation (2.7) we conclude that

Dm({S|x : L(D,f)(hS) > }) ≤ |HB| e− m ≤ |H| e− m.

A graphical illustration which explains how we used the union bound is given in Figure 2.1.

Figure 2.1 Each point in the large circle represents a possible m-tuple of instances. Each colored oval represents the set of “misleading” m-tuple of instances for some “bad” predictor h ∈ HB. The ERM can potentially overﬁt whenever it gets a misleading training set S. That is, for some h ∈ HB we have LS(h) = 0. Equation (2.9) guarantees that for each individual bad hypothesis, h ∈ HB, at most (1 − )m-fraction of the training sets would be misleading. In particular, the larger m is, the smaller each of these colored ovals becomes. The union bound formalizes the fact that the area representing the training sets that are misleading with respect to some h ∈ HB (that is, the training sets in M ) is at most the sum of the areas of the colored ovals. Therefore, it is bounded by |HB| times the maximum size of a colored oval. Any sample S outside the colored ovals cannot cause the ERM rule to overﬁt.
corollary 2.3 Let H be a ﬁnite hypothesis class. Let δ ∈ (0, 1) and > 0

2.4 Exercises

41

and let m be an integer that satisﬁes

log(|H|/δ)

m≥

.

Then, for any labeling function, f , and for any distribution, D, for which the realizability assumption holds (that is, for some h ∈ H, L(D,f)(h) = 0), with probability of at least 1 − δ over the choice of an i.i.d. sample S of size m, we have that for every ERM hypothesis, hS, it holds that
L(D,f)(hS ) ≤ .
The preceeding corollary tells us that for a suﬃciently large m, the ERMH rule over a ﬁnite hypothesis class will be probably (with conﬁdence 1−δ) approximately (up to an error of ) correct. In the next chapter we formally deﬁne the model of Probably Approximately Correct (PAC) learning.

2.4 Exercises

1. Overﬁtting of polynomial matching: We have shown that the predictor deﬁned in Equation (2.3) leads to overﬁtting. While this predictor seems to be very unnatural, the goal of this exercise is to show that it can be described as a thresholded polynomial. That is, show that given a training set S = {(xi, f (xi))}im=1 ⊆ (Rd × {0, 1})m, there exists a polynomial pS such that hS(x) = 1 if and only if pS(x) ≥ 0, where hS is as deﬁned in Equation (2.3). It follows that learning the class of all thresholded polynomials using the ERM rule may lead to overﬁtting.
2. Let H be a class of binary classiﬁers over a domain X . Let D be an unknown distribution over X , and let f be the target hypothesis in H. Fix some h ∈ H. Show that the expected value of LS(h) over the choice of S|x equals L(D,f)(h), namely,
E [LS(h)] = L(D,f)(h).
S |x ∼Dm
3. Axis aligned rectangles: An axis aligned rectangle classiﬁer in the plane is a classiﬁer that assigns the value 1 to a point if and only if it is inside a certain rectangle. Formally, given real numbers a1 ≤ b1, a2 ≤ b2, deﬁne the classiﬁer h(a1,b1,a2,b2) by

h(a1,b1,a2,b2)(x1, x2) =

1 0

if a1 ≤ x1 ≤ b1 and a2 ≤ x2 ≤ b2 . otherwise

(2.10)

The class of all axis aligned rectangles in the plane is deﬁned as

Hr2ec = {h(a1,b1,a2,b2) : a1 ≤ b1, and a2 ≤ b2}.
Note that this is an inﬁnite size hypothesis class. Throughout this exercise we rely on the realizability assumption.

42

A Gentle Start

1. Let A be the algorithm that returns the smallest rectangle enclosing all
positive examples in the training set. Show that A is an ERM. 2. Show that if A receives a training set of size ≥ 4 log(4/δ) then, with proba-
bility of at least 1 − δ it returns a hypothesis with error of at most . Hint: Fix some distribution D over X , let R∗ = R(a1∗, b∗1, a2∗, b2∗) be the rectangle that generates the labels, and let f be the corresponding hypothesis. Let a1 ≥ a∗1 be a number such that the probability mass (with respect to D) of the rectangle R1 = R(a∗1, a1, a∗2, b2∗) is exactly /4. Similarly, let b1, a2, b2 be numbers such that the probability masses of the rectangles R2 = R(b1, b1∗, a2∗, b2∗), R3 = R(a1∗, b∗1, a∗2, a2), R4 = R(a1∗, b1∗, b2, b∗2) are all exactly /4. Let R(S) be the rectangle returned by A. See illustration in
Figure 2.2.

+

R∗ + R(S)

+

-

+

+
R1
Figure 2.2 Axis aligned rectangles.
• Show that R(S) ⊆ R∗. • Show that if S contains (positive) examples in all of the rectangles
R1, R2, R3, R4, then the hypothesis returned by A has error of at most . • For each i ∈ {1, . . . , 4}, upper bound the probability that S does not contain an example from Ri. • Use the union bound to conclude the argument. 3. Repeat the previous question for the class of axis aligned rectangles in Rd. 4. Show that the runtime of applying the algorithm A mentioned earlier is polynomial in d, 1/ , and in log(1/δ).

3 A Formal Learning Model
In this chapter we deﬁne our main formal learning model – the PAC learning model and its extensions. We will consider other notions of learnability in Chapter 7.
3.1 PAC Learning
In the previous chapter we have shown that for a ﬁnite hypothesis class, if the ERM rule with respect to that class is applied on a suﬃciently large training sample (whose size is independent of the underlying distribution or labeling function) then the output hypothesis will be probably approximately correct. More generally, we now deﬁne Probably Approximately Correct (PAC) learning.
definition 3.1 (PAC Learnability) A hypothesis class H is PAC learnable if there exist a function mH : (0, 1)2 → N and a learning algorithm with the following property: For every , δ ∈ (0, 1), for every distribution D over X , and for every labeling function f : X → {0, 1}, if the realizable assumption holds with respect to H, D, f , then when running the learning algorithm on m ≥ mH( , δ) i.i.d. examples generated by D and labeled by f , the algorithm returns a hypothesis h such that, with probability of at least 1 − δ (over the choice of the examples), L(D,f)(h) ≤ .
The deﬁnition of Probably Approximately Correct learnability contains two approximation parameters. The accuracy parameter determines how far the output classiﬁer can be from the optimal one (this corresponds to the “approximately correct”), and a conﬁdence parameter δ indicating how likely the classiﬁer is to meet that accuracy requirement (corresponds to the “probably” part of “PAC”). Under the data access model that we are investigating, these approximations are inevitable. Since the training set is randomly generated, there may always be a small chance that it will happen to be noninformative (for example, there is always some chance that the training set will contain only one domain point, sampled over and over again). Furthermore, even when we are lucky enough to get a training sample that does faithfully represent D, because it is just a ﬁnite sample, there may always be some ﬁne details of D that it fails
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

44

A Formal Learning Model

to reﬂect. Our accuracy parameter, , allows “forgiving” the learner’s classiﬁer for making minor errors.

Sample Complexity The function mH : (0, 1)2 → N determines the sample complexity of learning H: that is, how many examples are required to guarantee a probably approximately correct solution. The sample complexity is a function of the accuracy ( ) and conﬁdence (δ) parameters. It also depends on properties of the hypothesis class H – for example, for a ﬁnite class we showed that the sample complexity depends on log the size of H.
Note that if H is PAC learnable, there are many functions mH that satisfy the requirements given in the deﬁnition of PAC learnability. Therefore, to be precise, we will deﬁne the sample complexity of learning H to be the “minimal function,” in the sense that for any , δ, mH( , δ) is the minimal integer that satisﬁes the requirements of PAC learning with accuracy and conﬁdence δ.
Let us now recall the conclusion of the analysis of ﬁnite hypothesis classes from the previous chapter. It can be rephrased as stating:

corollary 3.2 Every ﬁnite hypothesis class is PAC learnable with sample complexity

log(|H|/δ)

mH( , δ) ≤

.

There are inﬁnite classes that are learnable as well (see, for example, Exercise 3). Later on we will show that what determines the PAC learnability of a class is not its ﬁniteness but rather a combinatorial measure called the VC dimension.

3.2 A More General Learning Model
The model we have just described can be readily generalized, so that it can be made relevant to a wider scope of learning tasks. We consider generalizations in two aspects:
Removing the Realizability Assumption We have required that the learning algorithm succeeds on a pair of data distribution D and labeling function f provided that the realizability assumption is met. For practical learning tasks, this assumption may be too strong (can we really guarantee that there is a rectangle in the color-hardness space that fully determines which papayas are tasty?). In the next subsection, we will describe the agnostic PAC model in which this realizability assumption is waived.

3.2 A More General Learning Model

45

Learning Problems beyond Binary Classiﬁcation The learning task that we have been discussing so far has to do with predicting a binary label to a given example (like being tasty or not). However, many learning tasks take a diﬀerent form. For example, one may wish to predict a real valued number (say, the temperature at 9:00 p.m. tomorrow) or a label picked from a ﬁnite set of labels (like the topic of the main story in tomorrow’s paper). It turns out that our analysis of learning can be readily extended to such and many other scenarios by allowing a variety of loss functions. We shall discuss that in Section 3.2.2 later.

3.2.1

Releasing the Realizability Assumption – Agnostic PAC Learning
A More Realistic Model for the Data-Generating Distribution Recall that the realizability assumption requires that there exists h ∈ H such that Px∼D[h (x) = f (x)] = 1. In many practical problems this assumption does not hold. Furthermore, it is maybe more realistic not to assume that the labels are fully determined by the features we measure on input elements (in the case of the papayas, it is plausible that two papayas of the same color and softness will have diﬀerent taste). In the following, we relax the realizability assumption by replacing the “target labeling function” with a more ﬂexible notion, a data-labels generating distribution.
Formally, from now on, let D be a probability distribution over X × Y, where, as before, X is our domain set and Y is a set of labels (usually we will consider Y = {0, 1}). That is, D is a joint distribution over domain points and labels. One can view such a distribution as being composed of two parts: a distribution Dx over unlabeled domain points (sometimes called the marginal distribution) and a conditional probability over labels for each domain point, D((x, y)|x). In the papaya example, Dx determines the probability of encountering a papaya whose color and hardness fall in some color-hardness values domain, and the conditional probability is the probability that a papaya with color and hardness represented by x is tasty. Indeed, such modeling allows for two papayas that share the same color and hardness to belong to diﬀerent taste categories.

The empirical and the True Error Revised For a probability distribution, D, over X × Y, one can measure how likely h is to make an error when labeled points are randomly drawn according to D. We redeﬁne the true error (or risk) of a prediction rule h to be

LD(h) d=ef P [h(x) = y] d=ef D({(x, y) : h(x) = y}).
(x,y)∼D

(3.1)

We would like to ﬁnd a predictor, h, for which that error will be minimized. However, the learner does not know the data generating D. What the learner does have access to is the training data, S. The deﬁnition of the empirical risk

46

A Formal Learning Model

remains the same as before, namely,

LS (h) d=ef

|{i ∈ [m] : h(xi) = yi}| . m

Given S, a learner can compute LS(h) for any function h : X → {0, 1}. Note that LS (h) = LD(uniform over S)(h).

The Goal We wish to ﬁnd some hypothesis, h : X → Y, that (probably approximately) minimizes the true risk, LD(h).

The Bayes Optimal Predictor. Given any probability distribution D over X × {0, 1}, the best label predicting function from X to {0, 1} will be
fD(x) = 1 if P[y = 1|x] ≥ 1/2 0 otherwise
It is easy to verify (see Exercise 7) that for every probability distribution D, the Bayes optimal predictor fD is optimal, in the sense that no other classiﬁer, g : X → {0, 1} has a lower error. That is, for every classiﬁer g, LD(fD) ≤ LD(g).
Unfortunately, since we do not know D, we cannot utilize this optimal predictor fD. What the learner does have access to is the training sample. We can now present the formal deﬁnition of agnostic PAC learnability, which is a natural extension of the deﬁnition of PAC learnability to the more realistic, nonrealizable, learning setup we have just discussed.
Clearly, we cannot hope that the learning algorithm will ﬁnd a hypothesis whose error is smaller than the minimal possible error, that of the Bayes predictor.
Furthermore, as we shall prove later, once we make no prior assumptions about the data-generating distribution, no algorithm can be guaranteed to ﬁnd a predictor that is as good as the Bayes optimal one. Instead, we require that the learning algorithm will ﬁnd a predictor whose error is not much larger than the best possible error of a predictor in some given benchmark hypothesis class. Of course, the strength of such a requirement depends on the choice of that hypothesis class.
definition 3.3 (Agnostic PAC Learnability) A hypothesis class H is agnostic PAC learnable if there exist a function mH : (0, 1)2 → N and a learning algorithm with the following property: For every , δ ∈ (0, 1) and for every distribution D over X ×Y, when running the learning algorithm on m ≥ mH( , δ) i.i.d. examples generated by D, the algorithm returns a hypothesis h such that, with probability of at least 1 − δ (over the choice of the m training examples),
LD(h) ≤ min LD(h ) + .
h ∈H

3.2 A More General Learning Model

47

Clearly, if the realizability assumption holds, agnostic PAC learning provides the same guarantee as PAC learning. In that sense, agnostic PAC learning generalizes the deﬁnition of PAC learning. When the realizability assumption does not hold, no learner can guarantee an arbitrarily small error. Nevertheless, under the deﬁnition of agnostic PAC learning, a learner can still declare success if its error is not much larger than the best error achievable by a predictor from the class H. This is in contrast to PAC learning, in which the learner is required to achieve a small error in absolute terms and not relative to the best error achievable by the hypothesis class.

3.2.2

The Scope of Learning Problems Modeled
We next extend our model so that it can be applied to a wide variety of learning tasks. Let us consider some examples of diﬀerent learning tasks.
• Multiclass Classiﬁcation Our classiﬁcation does not have to be binary. Take, for example, the task of document classiﬁcation: We wish to design a program that will be able to classify given documents according to topics (e.g., news, sports, biology, medicine). A learning algorithm for such a task will have access to examples of correctly classiﬁed documents and, on the basis of these examples, should output a program that can take as input a new document and output a topic classiﬁcation for that document. Here, the domain set is the set of all potential documents. Once again, we would usually represent documents by a set of features that could include counts of diﬀerent key words in the document, as well as other possibly relevant features like the size of the document or its origin. The label set in this task will be the set of possible document topics (so Y will be some large ﬁnite set). Once we determine our domain and label sets, the other components of our framework look exactly the same as in the papaya tasting example; Our training sample will be a ﬁnite sequence of (feature vector, label) pairs, the learner’s output will be a function from the domain set to the label set, and, ﬁnally, for our measure of success, we can use the probability, over (document, topic) pairs, of the event that our predictor suggests a wrong label.
• Regression In this task, one wishes to ﬁnd some simple pattern in the data – a functional relationship between the X and Y components of the data. For example, one wishes to ﬁnd a linear function that best predicts a baby’s birth weight on the basis of ultrasound measures of his head circumference, abdominal circumference, and femur length. Here, our domain set X is some subset of R3 (the three ultrasound measurements), and the set of “labels,” Y, is the the set of real numbers (the weight in grams). In this context, it is more adequate to call Y the target set. Our training data as well as the learner’s output are as before (a ﬁnite sequence of (x, y) pairs, and a function from X to Y respectively). However, our measure of success is

48

A Formal Learning Model

diﬀerent. We may evaluate the quality of a hypothesis function, h : X → Y,

by the expected square diﬀerence between the true labels and their predicted

values, namely,

LD(h) d=ef E (h(x) − y)2.
(x,y)∼D

(3.2)

To accommodate a wide range of learning tasks we generalize our formalism of the measure of success as follows:

Generalized Loss Functions Given any set H (that plays the role of our hypotheses, or models) and some domain Z let be any function from H×Z to the set of nonnegative real numbers,
: H × Z → R+. We call such functions loss functions. Note that for prediction problems, we have that Z = X × Y. However, our notion of the loss function is generalized beyond prediction tasks, and therefore it allows Z to be any domain of examples (for instance, in unsupervised learning tasks such as the one described in Chapter 22, Z is not a product of an instance domain and a label domain). We now deﬁne the risk function to be the expected loss of a classiﬁer, h ∈ H, with respect to a probability distribution D over Z, namely,

LD(h) d=ef E [ (h, z)].
z∼D

(3.3)

That is, we consider the expectation of the loss of h over objects z picked randomly according to D. Similarly, we deﬁne the empirical risk to be the expected loss over a given sample S = (z1, . . . , zm) ∈ Zm, namely,

LS (h) d=ef

1m m

(h, zi).

i=1

(3.4)

The loss functions used in the preceding examples of classiﬁcation and regression tasks are as follows:

• 0–1 loss: Here, our random variable z ranges over the set of pairs X × Y and the loss function is

0−1(h, (x, y)) d=ef 0 if h(x) = y 1 if h(x) = y

This loss function is used in binary or multiclass classiﬁcation problems. One should note that, for a random variable, α, taking the values {0, 1}, Eα∼D[α] = Pα∼D[α = 1]. Consequently, for this loss function, the deﬁnitions of LD(h) given in Equation (3.3) and Equation (3.1) coincide. • Square Loss: Here, our random variable z ranges over the set of pairs X × Y and the loss function is
sq(h, (x, y)) d=ef (h(x) − y)2.

3.3 Summary

49

This loss function is used in regression problems.
We will later see more examples of useful instantiations of loss functions.
To summarize, we formally deﬁne agnostic PAC learnability for general loss functions.
definition 3.4 (Agnostic PAC Learnability for General Loss Functions) A hypothesis class H is agnostic PAC learnable with respect to a set Z and a loss function : H × Z → R+, if there exist a function mH : (0, 1)2 → N and a learning algorithm with the following property: For every , δ ∈ (0, 1) and for every distribution D over Z, when running the learning algorithm on m ≥ mH( , δ) i.i.d. examples generated by D, the algorithm returns h ∈ H such that, with probability of at least 1 − δ (over the choice of the m training examples),
LD(h) ≤ min LD(h ) + ,
h ∈H
where LD(h) = Ez∼D[ (h, z)].
Remark 3.1 (A Note About Measurability*) In the aforementioned deﬁnition, for every h ∈ H, we view the function (h, ·) : Z → R+ as a random variable and deﬁne LD(h) to be the expected value of this random variable. For that, we need to require that the function (h, ·) is measurable. Formally, we assume that there is a σ-algebra of subsets of Z, over which the probability D is deﬁned, and that the preimage of every initial segment in R+ is in this σ-algebra. In the speciﬁc case of binary classiﬁcation with the 0−1 loss, the σ-algebra is over X × {0, 1} and our assumption on is equivalent to the assumption that for every h, the set {(x, h(x)) : x ∈ X } is in the σ-algebra. Remark 3.2 (Proper versus Representation-Independent Learning*) In the preceding deﬁnition, we required that the algorithm will return a hypothesis from H. In some situations, H is a subset of a set H , and the loss function can be naturally extended to be a function from H × Z to the reals. In this case, we may allow the algorithm to return a hypothesis h ∈ H , as long as it satisﬁes the requirement LD(h ) ≤ minh∈H LD(h) + . Allowing the algorithm to output a hypothesis from H is called representation independent learning, while proper learning occurs when the algorithm must output a hypothesis from H. Representation independent learning is sometimes called “improper learning,” although there is nothing improper in representation independent learning.
3.3 Summary
In this chapter we deﬁned our main formal learning model – PAC learning. The basic model relies on the realizability assumption, while the agnostic variant does

50

A Formal Learning Model

not impose any restrictions on the underlying distribution over the examples. We also generalized the PAC model to arbitrary loss functions. We will sometimes refer to the most general model simply as PAC learning, omitting the “agnostic” preﬁx and letting the reader infer what the underlying loss function is from the context. When we would like to emphasize that we are dealing with the original PAC setting we mention that the realizability assumption holds. In Chapter 7 we will discuss other notions of learnability.
3.4 Bibliographic Remarks
Our most general deﬁnition of agnostic PAC learning with general loss functions follows the works of Vladimir Vapnik and Alexey Chervonenkis (Vapnik & Chervonenkis 1971). In particular, we follow Vapnik’s general setting of learning (Vapnik 1982, Vapnik 1992, Vapnik 1995, Vapnik 1998).
PAC learning was introduced by Valiant (1984). Valiant was named the winner of the 2010 Turing Award for the introduction of the PAC model. Valiant’s deﬁnition requires that the sample complexity will be polynomial in 1/ and in 1/δ, as well as in the representation size of hypotheses in the class (see also Kearns & Vazirani (1994)). As we will see in Chapter 6, if a problem is at all PAC learnable then the sample complexity depends polynomially on 1/ and log(1/δ). Valiant’s deﬁnition also requires that the runtime of the learning algorithm will be polynomial in these quantities. In contrast, we chose to distinguish between the statistical aspect of learning and the computational aspect of learning. We will elaborate on the computational aspect later on in Chapter 8, where we introduce the full PAC learning model of Valiant. For expository reasons, we use the term PAC learning even when we ignore the runtime aspect of learning. Finally, the formalization of agnostic PAC learning is due to Haussler (1992).
3.5 Exercises
1. Monotonicity of Sample Complexity: Let H be a hypothesis class for a binary classiﬁcation task. Suppose that H is PAC learnable and its sample complexity is given by mH(·, ·). Show that mH is monotonically nonincreasing in each of its parameters. That is, show that given δ ∈ (0, 1), and given 0 < 1 ≤ 2 < 1, we have that mH( 1, δ) ≥ mH( 2, δ). Similarly, show that given ∈ (0, 1), and given 0 < δ1 ≤ δ2 < 1, we have that mH( , δ1) ≥ mH( , δ2).
2. Let X be a discrete domain, and let HSingleton = {hz : z ∈ X } ∪ {h−}, where for each z ∈ X , hz is the function deﬁned by hz(x) = 1 if x = z and hz(x) = 0 if x = z. h− is simply the all-negative hypothesis, namely, ∀x ∈ X, h−(x) = 0. The realizability assumption here implies that the true hypothesis f labels negatively all examples in the domain, perhaps except one.

3.5 Exercises

51

1. Describe an algorithm that implements the ERM rule for learning HSingleton in the realizable setup.
2. Show that HSingleton is PAC learnable. Provide an upper bound on the sample complexity.
3. Let X = R2, Y = {0, 1}, and let H be the class of concentric circles in the plane, that is, H = {hr : r ∈ R+}, where hr(x) = 1[ x ≤r]. Prove that H is PAC learnable (assume realizability), and its sample complexity is bounded by

log(1/δ)

mH( , δ) ≤

.

4. In this question, we study the hypothesis class of Boolean conjunctions deﬁned as follows. The instance space is X = {0, 1}d and the label set is Y = {0, 1}. A literal over the variables x1, . . . , xd is a simple Boolean function that takes the form f (x) = xi, for some i ∈ [d], or f (x) = 1 − xi for some i ∈ [d]. We use the notation x¯i as a shorthand for 1 − xi. A conjunction is any product of literals. In Boolean logic, the product is denoted using the ∧ sign. For example, the function h(x) = x1 · (1 − x2) is written as x1 ∧ x¯2. We consider the hypothesis class of all conjunctions of literals over the d variables. The empty conjunction is interpreted as the all-positive hypothesis (namely, the function that returns h(x) = 1 for all x). The conjunction x1 ∧x¯1 (and similarly any conjunction involving a literal and its negation) is allowed and interpreted as the all-negative hypothesis (namely, the conjunction that returns h(x) = 0 for all x). We assume realizability: Namely, we assume that there exists a Boolean conjunction that generates the labels. Thus, each example (x, y) ∈ X × Y consists of an assignment to the d Boolean variables x1, . . . , xd, and its truth value (0 for false and 1 for true). For instance, let d = 3 and suppose that the true conjunction is x1 ∧ x¯2. Then, the training set S might contain the following instances:
((1, 1, 1), 0), ((1, 0, 1), 1), ((0, 1, 0), 0)((1, 0, 0), 1).
Prove that the hypothesis class of all conjunctions over d variables is PAC learnable and bound its sample complexity. Propose an algorithm that implements the ERM rule, whose runtime is polynomial in d · m. 5. Let X be a domain and let D1, D2, . . . , Dm be a sequence of distributions over X . Let H be a ﬁnite class of binary classiﬁers over X and let f ∈ H. Suppose we are getting a sample S of m examples, such that the instances are independent but are not identically distributed; the ith instance is sampled from Di and then yi is set to be f (xi). Let D¯m denote the average, that is, D¯m = (D1 + · · · + Dm)/m.

Fix an accuracy parameter ∈ (0, 1). Show that P ∃h ∈ H s.t. L(D¯m,f)(h) > and L(S,f)(h) = 0 ≤ |H|e− m.

52

A Formal Learning Model

Hint: Use the geometric-arithmetic mean inequality. 6. Let H be a hypothesis class of binary classiﬁers. Show that if H is agnostic
PAC learnable, then H is PAC learnable as well. Furthermore, if A is a successful agnostic PAC learner for H, then A is also a successful PAC learner for H. 7. (*) The Bayes optimal predictor: Show that for every probability distribution D, the Bayes optimal predictor fD is optimal, in the sense that for every classiﬁer g from X to {0, 1}, LD(fD) ≤ LD(g). 8. (*) We say that a learning algorithm A is better than B with respect to some probability distribution, D, if
LD(A(S)) ≤ LD(B(S))
for all samples S ∈ (X ×{0, 1})m. We say that a learning algorithm A is better than B, if it is better than B with respect to all probability distributions D over X × {0, 1}. 1. A probabilistic label predictor is a function that assigns to every domain
point x a probability value, h(x) ∈ [0, 1], that determines the probability of predicting the label 1. That is, given such an h and an input, x, the label for x is predicted by tossing a coin with bias h(x) toward Heads and predicting 1 iﬀ the coin comes up Heads. Formally, we deﬁne a probabilistic label predictor as a function, h : X → [0, 1]. The loss of such h on an example (x, y) is deﬁned to be |h(x) − y|, which is exactly the probability that the prediction of h will not be equal to y. Note that if h is deterministic, that is, returns values in {0, 1}, then |h(x) − y| = 1[h(x)=y]. Prove that for every data-generating distribution D over X × {0, 1}, the Bayes optimal predictor has the smallest risk (w.r.t. the loss function (h, (x, y)) = |h(x)−y|, among all possible label predictors, including probabilistic ones). 2. Let X be a domain and {0, 1} be a set of labels. Prove that for every distribution D over X × {0, 1}, there exist a learning algorithm AD that is better than any other learning algorithm with respect to D. 3. Prove that for every learning algorithm A there exist a probability distribution, D, and a learning algorithm B such that A is not better than B w.r.t. D. 9. Consider a variant of the PAC model in which there are two example oracles: one that generates positive examples and one that generates negative examples, both according to the underlying distribution D on X . Formally, given a target function f : X → {0, 1}, let D+ be the distribution over X + = {x ∈ X : f (x) = 1} deﬁned by D+(A) = D(A)/D(X +), for every A ⊂ X +. Similarly, D− is the distribution over X − induced by D. The deﬁnition of PAC learnability in the two-oracle model is the same as the standard deﬁnition of PAC learnability except that here the learner has access to m+H( , δ) i.i.d. examples from D+ and m−( , δ) i.i.d. examples from D−. The learner’s goal is to output h s.t. with probability at least 1 − δ (over the choice

3.5 Exercises

53

of the two training sets, and possibly over the nondeterministic decisions made by the learning algorithm), both L(D+,f)(h) ≤ and L(D−,f)(h) ≤ . 1. (*) Show that if H is PAC learnable (in the standard one-oracle model),
then H is PAC learnable in the two-oracle model. 2. (**) Deﬁne h+ to be the always-plus hypothesis and h− to be the always-
minus hypothesis. Assume that h+, h− ∈ H. Show that if H is PAC learnable in the two-oracle model, then H is PAC learnable in the standard one-oracle model.

4 Learning via Uniform Convergence
The ﬁrst formal learning model that we have discussed was the PAC model. In Chapter 2 we have shown that under the realizability assumption, any ﬁnite hypothesis class is PAC learnable. In this chapter we will develop a general tool, uniform convergence, and apply it to show that any ﬁnite class is learnable in the agnostic PAC model with general loss functions, as long as the range loss function is bounded.
4.1 Uniform Convergence Is Suﬃcient for Learnability
The idea behind the learning condition discussed in this chapter is very simple. Recall that, given a hypothesis class, H, the ERM learning paradigm works as follows: Upon receiving a training sample, S, the learner evaluates the risk (or error) of each h in H on the given sample and outputs a member of H that minimizes this empirical risk. The hope is that an h that minimizes the empirical risk with respect to S is a risk minimizer (or has risk close to the minimum) with respect to the true data probability distribution as well. For that, it suﬃces to ensure that the empirical risks of all members of H are good approximations of their true risk. Put another way, we need that uniformly over all hypotheses in the hypothesis class, the empirical risk will be close to the true risk, as formalized in the following.
definition 4.1 ( -representative sample) A training set S is called -representative (w.r.t. domain Z, hypothesis class H, loss function , and distribution D) if
∀h ∈ H, |LS(h) − LD(h)| ≤ . The next simple lemma states that whenever the sample is ( /2)-representative, the ERM learning rule is guaranteed to return a good hypothesis.
lemma 4.2 Assume that a training set S is 2 -representative (w.r.t. domain Z, hypothesis class H, loss function , and distribution D). Then, any output of ERMH(S), namely, any hS ∈ argminh∈H LS(h), satisﬁes
LD(hS) ≤ min LD(h) + .
h∈H
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

4.2 Finite Classes Are Agnostic PAC Learnable

55

Proof For every h ∈ H,
LD(hS) ≤ LS(hS) + 2 ≤ LS(h) + 2 ≤ LD(h) + 2 + 2 = LD(h) + ,
where the ﬁrst and third inequalities are due to the assumption that S is 2 representative (Deﬁnition 4.1) and the second inequality holds since hS is an ERM predictor.
The preceding lemma implies that to ensure that the ERM rule is an agnostic PAC learner, it suﬃces to show that with probability of at least 1 − δ over the random choice of a training set, it will be an -representative training set. The uniform convergence condition formalizes this requirement.
definition 4.3 (Uniform Convergence) We say that a hypothesis class H has the uniform convergence property (w.r.t. a domain Z and a loss function ) if there exists a function mUHC : (0, 1)2 → N such that for every , δ ∈ (0, 1) and for every probability distribution D over Z, if S is a sample of m ≥ mUHC( , δ) examples drawn i.i.d. according to D, then, with probability of at least 1 − δ, S is -representative.
Similar to the deﬁnition of sample complexity for PAC learning, the function mUHC measures the (minimal) sample complexity of obtaining the uniform convergence property, namely, how many examples we need to ensure that with probability of at least 1 − δ the sample would be -representative. The term uniform here refers to having a ﬁxed sample size that works for all members of H and over all possible probability distributions over the domain.
The following corollary follows directly from Lemma 4.2 and the deﬁnition of uniform convergence.
corollary 4.4 If a class H has the uniform convergence property with a function mUHC then the class is agnostically PAC learnable with the sample complexity mH( , δ) ≤ mHUC( /2, δ). Furthermore, in that case, the ERMH paradigm is a successful agnostic PAC learner for H.
4.2 Finite Classes Are Agnostic PAC Learnable
In view of Corollary 4.4, the claim that every ﬁnite hypothesis class is agnostic PAC learnable will follow once we establish that uniform convergence holds for a ﬁnite hypothesis class.
To show that uniform convergence holds we follow a two step argument, similar to the derivation in Chapter 2. The ﬁrst step applies the union bound while the second step employs a measure concentration inequality. We now explain these two steps in detail.
Fix some , δ. We need to ﬁnd a sample size m that guarantees that for any D, with probability of at least 1 − δ of the choice of S = (z1, . . . , zm) sampled

56

Learning via Uniform Convergence

i.i.d. from D we have that for all h ∈ H, |LS(h) − LD(h)| ≤ . That is, Dm({S : ∀h ∈ H, |LS(h) − LD(h)| ≤ }) ≥ 1 − δ.

Equivalently, we need to show that Dm({S : ∃h ∈ H, |LS(h) − LD(h)| > }) < δ.

Writing

{S : ∃h ∈ H, |LS(h) − LD(h)| > } = ∪h∈H{S : |LS(h) − LD(h)| > },

and applying the union bound (Lemma 2.2) we obtain

Dm({S : ∃h ∈ H, |LS(h) − LD(h)| > }) ≤ Dm({S : |LS(h) − LD(h)| > }).

h∈H
(4.1)

Our second step will be to argue that each summand of the right-hand side

of this inequality is small enough (for a suﬃciently large m). That is, we will

show that for any ﬁxed hypothesis, h, (which is chosen in advance prior to the

sampling of the training set), the gap between the true and empirical risks,

|LS(h) − LD(h)|, is likely to be small.

Recall

that

LD (h)

=

Ez∼D [

(h, z)]

and

that

LS (h)

=

1 m

m i=1

(h, zi). Since

each zi is sampled i.i.d. from D, the expected value of the random variable

(h, zi) is LD(h). By the linearity of expectation, it follows that LD(h) is also

the expected value of LS(h). Hence, the quantity |LD(h)−LS(h)| is the deviation

of the random variable LS(h) from its expectation. We therefore need to show

that the measure of LS(h) is concentrated around its expected value.

A basic statistical fact, the law of large numbers, states that when m goes to

inﬁnity, empirical averages converge to their true expectation. This is true for

LS(h), since it is the empirical average of m i.i.d random variables. However, since

the law of large numbers is only an asymptotic result, it provides no information

about the gap between the empirically estimated error and its true value for any

given, ﬁnite, sample size.

Instead, we will use a measure concentration inequality due to Hoeﬀding, which

quantiﬁes the gap between empirical averages and their expected value.

lemma 4.5 (Hoeﬀding’s Inequality) Let θ1, . . . , θm be a sequence of i.i.d. random variables and assume that for all i, E[θi] = µ and P[a ≤ θi ≤ b] = 1. Then, for any > 0

m

P

1 m

θi − µ >

i=1

≤ 2 exp −2 m 2/(b − a)2 .

The proof can be found in Appendix B.

Getting back to our problem, let θi be the random variable (h, zi). Since h

is ﬁxed and z1, . . . , zm are sampled i.i.d., it follows that θ1, . . . , θm are also i.i.d.

random

variables.

Furthermore,

LS (h)

=

1 m

m i=1

θi

and

LD (h)

=

µ.

Let

us

4.2 Finite Classes Are Agnostic PAC Learnable

57

further assume that the range of is [0, 1] and therefore θi ∈ [0, 1]. We therefore obtain that

m

Dm({S : |LS(h) − LD(h)| > }) = P

1 m

θi − µ >

i=1

Combining this with Equation (4.1) yields

≤ 2 exp −2 m 2 . (4.2)

Dm({S : ∃h ∈ H, |LS(h) − LD(h)| > }) ≤ 2 exp −2 m 2
h∈H
= 2 |H| exp −2 m 2 .

Finally, if we choose

log(2|H|/δ)

m≥

22

then

Dm({S : ∃h ∈ H, |LS(h) − LD(h)| > }) ≤ δ.

corollary 4.6 Let H be a ﬁnite hypothesis class, let Z be a domain, and let : H × Z → [0, 1] be a loss function. Then, H enjoys the uniform convergence
property with sample complexity

log(2|H|/δ)

mUHC( , δ) ≤

22

.

Furthermore, the class is agnostically PAC learnable using the ERM algorithm with sample complexity

2 log(2|H|/δ)

mH( , δ) ≤ mHUC( /2, δ) ≤

2

.

Remark 4.1 (The “Discretization Trick”) While the preceding corollary only applies to ﬁnite hypothesis classes, there is a simple trick that allows us to get a very good estimate of the practical sample complexity of inﬁnite hypothesis classes. Consider a hypothesis class that is parameterized by d parameters. For example, let X = R, Y = {±1}, and the hypothesis class, H, be all functions of the form hθ(x) = sign(x − θ). That is, each hypothesis is parameterized by one parameter, θ ∈ R, and the hypothesis outputs 1 for all instances larger than θ and outputs −1 for instances smaller than θ. This is a hypothesis class of an inﬁnite size. However, if we are going to learn this hypothesis class in practice, using a computer, we will probably maintain real numbers using ﬂoating point representation, say, of 64 bits. It follows that in practice, our hypothesis class is parameterized by the set of scalars that can be represented using a 64 bits ﬂoating point number. There are at most 264 such numbers; hence the actual size of our hypothesis class is at most 264. More generally, if our hypothesis class is parameterized by d numbers, in practice we learn a hypothesis class of size at most 264d. Applying Corollary 4.6 we obtain that the sample complexity of such

58

Learning via Uniform Convergence

classes

is

bounded

by

128d+2

log(2/δ)
2

.

This

upper

bound

on

the

sample

complex-

ity has the deﬁciency of being dependent on the speciﬁc representation of real

numbers used by our machine. In Chapter 6 we will introduce a rigorous way

to analyze the sample complexity of inﬁnite size hypothesis classes. Neverthe-

less, the discretization trick can be used to get a rough estimate of the sample

complexity in many practical situations.

4.3 Summary
If the uniform convergence property holds for a hypothesis class H then in most cases the empirical risks of hypotheses in H will faithfully represent their true risks. Uniform convergence suﬃces for agnostic PAC learnability using the ERM rule. We have shown that ﬁnite hypothesis classes enjoy the uniform convergence property and are hence agnostic PAC learnable.

4.4 Bibliographic Remarks
Classes of functions for which the uniform convergence property holds are also called Glivenko-Cantelli classes, named after Valery Ivanovich Glivenko and Francesco Paolo Cantelli, who proved the ﬁrst uniform convergence result in the 1930s. See (Dudley, Gine & Zinn 1991). The relation between uniform convergence and learnability was thoroughly studied by Vapnik – see (Vapnik 1992, Vapnik 1995, Vapnik 1998). In fact, as we will see later in Chapter 6, the fundamental theorem of learning theory states that in binary classiﬁcation problems, uniform convergence is not only a suﬃcient condition for learnability but is also a necessary condition. This is not the case for more general learning problems (see (Shalev-Shwartz, Shamir, Srebro & Sridharan 2010)).

4.5 Exercises

1. In this exercise, we show that the ( , δ) requirement on the convergence of errors in our deﬁnitions of PAC learning, is, in fact, quite close to a simpler looking requirement about averages (or expectations). Prove that the following two statements are equivalent (for any learning algorithm A, any probability distribution D, and any loss function whose range is [0, 1]): 1. For every , δ > 0, there exists m( , δ) such that ∀m ≥ m( , δ)

P [LD(A(S)) > ] < δ
S∼Dm

2.

lim
m→∞

E
S∼Dm

[LD (A(S ))]

=

0

4.5 Exercises

59

(where ES∼Dm denotes the expectation over samples S of size m). 2. Bounded loss functions: In Corollary 4.6 we assumed that the range of the
loss function is [0, 1]. Prove that if the range of the loss function is [a, b] then the sample complexity satisﬁes

2 log(2|H|/δ)(b − a)2

mH( , δ) ≤ mHUC( /2, δ) ≤

2

.

5 The Bias-Complexity Tradeoﬀ
In Chapter 2 we saw that unless one is careful, the training data can mislead the learner, and result in overﬁtting. To overcome this problem, we restricted the search space to some hypothesis class H. Such a hypothesis class can be viewed as reﬂecting some prior knowledge that the learner has about the task – a belief that one of the members of the class H is a low-error model for the task. For example, in our papayas taste problem, on the basis of our previous experience with other fruits, we may assume that some rectangle in the color-hardness plane predicts (at least approximately) the papaya’s tastiness.
Is such prior knowledge really necessary for the success of learning? Maybe there exists some kind of universal learner, that is, a learner who has no prior knowledge about a certain task and is ready to be challenged by any task? Let us elaborate on this point. A speciﬁc learning task is deﬁned by an unknown distribution D over X × Y, where the goal of the learner is to ﬁnd a predictor h : X → Y, whose risk, LD(h), is small enough. The question is therefore whether there exist a learning algorithm A and a training set size m, such that for every distribution D, if A receives m i.i.d. examples from D, there is a high chance it outputs a predictor h that has a low risk.
The ﬁrst part of this chapter addresses this question formally. The No-FreeLunch theorem states that no such universal learner exists. To be more precise, the theorem states that for binary classiﬁcation prediction tasks, for every learner there exists a distribution on which it fails. We say that the learner fails if, upon receiving i.i.d. examples from that distribution, its output hypothesis is likely to have a large risk, say, ≥ 0.3, whereas for the same distribution, there exists another learner that will output a hypothesis with a small risk. In other words, the theorem states that no learner can succeed on all learnable tasks – every learner has tasks on which it fails while other learners succeed.
Therefore, when approaching a particular learning problem, deﬁned by some distribution D, we should have some prior knowledge on D. One type of such prior knowledge is that D comes from some speciﬁc parametric family of distributions. We will study learning under such assumptions later on in Chapter 24. Another type of prior knowledge on D, which we assumed when deﬁning the PAC learning model, is that there exists h in some predeﬁned hypothesis class H, such that LD(h) = 0. A softer type of prior knowledge on D is assuming that minh∈H LD(h) is small. In a sense, this weaker assumption on D is a prerequisite for using the
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

5.1 The No-Free-Lunch Theorem

61

agnostic PAC model, in which we require that the risk of the output hypothesis will not be much larger than minh∈H LD(h).
In the second part of this chapter we study the beneﬁts and pitfalls of using a hypothesis class as a means of formalizing prior knowledge. We decompose the error of an ERM algorithm over a class H into two components. The ﬁrst component reﬂects the quality of our prior knowledge, measured by the minimal risk of a hypothesis in our hypothesis class, minh∈H LD(h). This component is also called the approximation error, or the bias of the algorithm toward choosing a hypothesis from H. The second component is the error due to overﬁtting, which depends on the size or the complexity of the class H and is called the estimation error. These two terms imply a tradeoﬀ between choosing a more complex H (which can decrease the bias but increases the risk of overﬁtting) or a less complex H (which might increase the bias but decreases the potential overﬁtting).
5.1 The No-Free-Lunch Theorem
In this part we prove that there is no universal learner. We do this by showing that no learner can succeed on all learning tasks, as formalized in the following theorem:
theorem 5.1 (No-Free-Lunch) Let A be any learning algorithm for the task of binary classiﬁcation with respect to the 0 − 1 loss over a domain X . Let m be any number smaller than |X |/2, representing a training set size. Then, there exists a distribution D over X × {0, 1} such that:
1. There exists a function f : X → {0, 1} with LD(f ) = 0. 2. With probability of at least 1/7 over the choice of S ∼ Dm we have that
LD(A(S)) ≥ 1/8.
This theorem states that for every learner, there exists a task on which it fails, even though that task can be successfully learned by another learner. Indeed, a trivial successful learner in this case would be an ERM learner with the hypothesis class H = {f }, or more generally, ERM with respect to any ﬁnite hypothesis class that contains f and whose size satisﬁes the equation m ≥ 8 log(7|H|/6) (see Corollary 2.3).
Proof Let C be a subset of X of size 2m. The intuition of the proof is that any learning algorithm that observes only half of the instances in C has no information on what should be the labels of the rest of the instances in C. Therefore, there exists a “reality,” that is, some target function f , that would contradict the labels that A(S) predicts on the unobserved instances in C.
Note that there are T = 22m possible functions from C to {0, 1}. Denote these functions by f1, . . . , fT . For each such function, let Di be a distribution over

62

The Bias-Complexity Tradeoﬀ

C × {0, 1} deﬁned by

Di({(x, y)}) = 1/|C| if y = fi(x)

0

otherwise.

That is, the probability to choose a pair (x, y) is 1/|C| if the label y is indeed the true label according to fi, and the probability is 0 if y = fi(x). Clearly, LDi (fi) = 0.
We will show that for every algorithm, A, that receives a training set of m examples from C × {0, 1} and returns a function A(S) : C → {0, 1}, it holds that

max
i∈[T ]

E
S∼Dim

[LDi

(A(S))]

≥

1/4.

(5.1)

Clearly, this means that for every algorithm, A , that receives a training set of m examples from X ×{0, 1} there exist a function f : X → {0, 1} and a distribution D over X × {0, 1}, such that LD(f ) = 0 and

E [LD(A (S))] ≥ 1/4.
S∼Dm

(5.2)

It is easy to verify that the preceding suﬃces for showing that P[LD(A (S)) ≥
1/8] ≥ 1/7, which is what we need to prove (see Exercise 1).
We now turn to proving that Equation (5.1) holds. There are k = (2m)m
possible sequences of m examples from C. Denote these sequences by S1, . . . , Sk. Also, if Sj = (x1, . . . , xm) we denote by Sji the sequence containing the instances in Sj labeled by the function fi, namely, Sji = ((x1, fi(x1)), . . . , (xm, fi(xm))). If the distribution is Di then the possible training sets A can receive are S1i , . . . , Ski , and all these training sets have the same probability of being sampled. Therefore,

S

E
∼Dim

[LDi

(A(S

))]

=

1 k

k

LDi (A(Sji)).

j=1

(5.3)

Using the facts that “maximum” is larger than “average” and that “average” is larger than “minimum,” we have

1 max i∈[T ] k

k

LDi (A(Sji)) ≥

1 T

T

1 k

k

LDi (A(Sji))

j=1

i=1 j=1

1k 1T =
kT

LDi (A(Sji))

j=1 i=1

1T ≥ min
j∈[k] T

LDi (A(Sji)).

i=1

(5.4)

Next, ﬁx some j ∈ [k]. Denote Sj = (x1, . . . , xm) and let v1, . . . , vp be the examples in C that do not appear in Sj. Clearly, p ≥ m. Therefore, for every

5.1 The No-Free-Lunch Theorem

63

function h : C → {0, 1} and every i we have

1

LDi (h) = 2m

1[h(x)=fi (x)]

x∈C

1p

≥ 2m

1[h(vr )=fi (vr )]

r=1

1p

≥ 2p

1[h(vr )=fi (vr )] .

r=1

(5.5)

Hence,

1T T
i=1

LDi (A(Sji))

1T 1 p

≥ T

2p

1[A(Sji )(vr )=fi(vr )]

i=1

r=1

1 p1T

= 2p T

1[A(Sji )(vr )=fi(vr )]

r=1 i=1

1

1T

≥ · min 2 r∈[p] T

1[A(Sji )(vr)=fi(vr)].

i=1

(5.6)

Next, ﬁx some r ∈ [p]. We can partition all the functions in f1, . . . , fT into T /2
disjoint pairs, where for a pair (fi, fi ) we have that for every c ∈ C, fi(c) = fi (c) if and only if c = vr. Since for such a pair we must have Sji = Sji , it follows that

which yields

1[A(Sji)(vr)=fi(vr)] + 1[A(Sji )(vr)=fi (vr)] = 1,

1T

1

T

1[A(Sji )(vr )=fi(vr )]

=

. 2

i=1

Combining this with Equation (5.6), Equation (5.4), and Equation (5.3), we obtain that Equation (5.1) holds, which concludes our proof.

5.1.1

No-Free-Lunch and Prior Knowledge
How does the No-Free-Lunch result relate to the need for prior knowledge? Let us consider an ERM predictor over the hypothesis class H of all the functions f from X to {0, 1}. This class represents lack of prior knowledge: Every possible function from the domain to the label set is considered a good candidate. According to the No-Free-Lunch theorem, any algorithm that chooses its output from hypotheses in H, and in particular the ERM predictor, will fail on some learning task. Therefore, this class is not PAC learnable, as formalized in the following corollary:
corollary 5.2 Let X be an inﬁnite domain set and let H be the set of all functions from X to {0, 1}. Then, H is not PAC learnable.

64

The Bias-Complexity Tradeoﬀ

Proof Assume, by way of contradiction, that the class is learnable. Choose some < 1/8 and δ < 1/7. By the deﬁnition of PAC learnability, there must be some learning algorithm A and an integer m = m( , δ), such that for any data-generating distribution over X × {0, 1}, if for some function f : X → {0, 1}, LD(f ) = 0, then with probability greater than 1 − δ when A is applied to samples S of size m, generated i.i.d. by D, LD(A(S)) ≤ . However, applying the No-Free-Lunch theorem, since |X | > 2m, for every learning algorithm (and in particular for the algorithm A), there exists a distribution D such that with probability greater than 1/7 > δ, LD(A(S)) > 1/8 > , which leads to the desired contradiction.
How can we prevent such failures? We can escape the hazards foreseen by the No-Free-Lunch theorem by using our prior knowledge about a speciﬁc learning task, to avoid the distributions that will cause us to fail when learning that task. Such prior knowledge can be expressed by restricting our hypothesis class.
But how should we choose a good hypothesis class? On the one hand, we want to believe that this class includes the hypothesis that has no error at all (in the PAC setting), or at least that the smallest error achievable by a hypothesis from this class is indeed rather small (in the agnostic setting). On the other hand, we have just seen that we cannot simply choose the richest class – the class of all functions over the given domain. This tradeoﬀ is discussed in the following section.
5.2 Error Decomposition
To answer this question we decompose the error of an ERMH predictor into two components as follows. Let hS be an ERMH hypothesis. Then, we can write
LD(hS) = app + est where : app = min LD(h), est = LD(hS) − app. (5.7)
h∈H
• The Approximation Error – the minimum risk achievable by a predictor in the hypothesis class. This term measures how much risk we have because we restrict ourselves to a speciﬁc class, namely, how much inductive bias we have. The approximation error does not depend on the sample size and is determined by the hypothesis class chosen. Enlarging the hypothesis class can decrease the approximation error. Under the realizability assumption, the approximation error is zero. In the agnostic case, however, the approximation error can be large.1
1 In fact, it always includes the error of the Bayes optimal predictor (see Chapter 3), the minimal yet inevitable error, because of the possible nondeterminism of the world in this model. Sometimes in the literature the term approximation error refers not to minh∈H LD(h), but rather to the excess error over that of the Bayes optimal predictor, namely, minh∈H LD(h) − Bayes.

5.3 Summary

65

• The Estimation Error – the diﬀerence between the approximation error and the error achieved by the ERM predictor. The estimation error results because the empirical risk (i.e., training error) is only an estimate of the true risk, and so the predictor minimizing the empirical risk is only an estimate of the predictor minimizing the true risk. The quality of this estimation depends on the training set size and on the size, or complexity, of the hypothesis class. As we have shown, for a ﬁnite hypothesis class, est increases (logarithmically) with |H| and decreases with m. We can think of the size of H as a measure of its complexity. In future chapters we will deﬁne other complexity measures of hypothesis classes.
Since our goal is to minimize the total risk, we face a tradeoﬀ, called the biascomplexity tradeoﬀ. On one hand, choosing H to be a very rich class decreases the approximation error but at the same time might increase the estimation error, as a rich H might lead to overﬁtting. On the other hand, choosing H to be a very small set reduces the estimation error but might increase the approximation error or, in other words, might lead to underﬁtting. Of course, a great choice for H is the class that contains only one classiﬁer – the Bayes optimal classiﬁer. But the Bayes optimal classiﬁer depends on the underlying distribution D, which we do not know (indeed, learning would have been unnecessary had we known D).
Learning theory studies how rich we can make H while still maintaining reasonable estimation error. In many cases, empirical research focuses on designing good hypothesis classes for a certain domain. Here, “good” means classes for which the approximation error would not be excessively high. The idea is that although we are not experts and do not know how to construct the optimal classiﬁer, we still have some prior knowledge of the speciﬁc problem at hand, which enables us to design hypothesis classes for which both the approximation error and the estimation error are not too large. Getting back to our papayas example, we do not know how exactly the color and hardness of a papaya predict its taste, but we do know that papaya is a fruit and on the basis of previous experience with other fruit we conjecture that a rectangle in the color-hardness space may be a good predictor.
5.3 Summary
The No-Free-Lunch theorem states that there is no universal learner. Every learner has to be speciﬁed to some task, and use some prior knowledge about that task, in order to succeed. So far we have modeled our prior knowledge by restricting our output hypothesis to be a member of a chosen hypothesis class. When choosing this hypothesis class, we face a tradeoﬀ, between a larger, or more complex, class that is more likely to have a small approximation error, and a more restricted class that would guarantee that the estimation error will

66

The Bias-Complexity Tradeoﬀ

be small. In the next chapter we will study in more detail the behavior of the estimation error. In Chapter 7 we will discuss alternative ways to express prior knowledge.

5.4 Bibliographic Remarks
(Wolpert & Macready 1997) proved several no-free-lunch theorems for optimization, but these are rather diﬀerent from the theorem we prove here. The theorem we prove here is closely related to lower bounds in VC theory, as we will study in the next chapter.

5.5 Exercises

1. Prove that Equation (5.2) suﬃces for showing that P[LD(A(S)) ≥ 1/8] ≥ 1/7.

Hint: Let θ be a random variable that receives values in [0, 1] and whose

expectation satisﬁes E[θ] ≥ 1/4. Use Lemma B.1 to show that P[θ ≥ 1/8] ≥

1/7.

2. Assume you are asked to design a learning algorithm to predict whether pa-

tients are going to suﬀer a heart attack. Relevant patient features the al-

gorithm may have access to include blood pressure (BP), body-mass index

(BMI), age (A), level of physical activity (P), and income (I).

You have to choose between two algorithms; the ﬁrst picks an axis aligned

rectangle in the two dimensional space spanned by the features BP and BMI

and the other picks an axis aligned rectangle in the ﬁve dimensional space

spanned by all the preceding features.

1. Explain the pros and cons of each choice.

2. Explain how the number of available labeled training samples will aﬀect

your choice.

3. Prove that if |X | ≥ km for a positive integer k ≥ 2, then we can replace

the

lower

bound

of

1/4

in

the

No-Free-Lunch

theorem

with

k−1 2k

=

1 2

−

1 2k

.

Namely, let A be a learning algorithm for the task of binary classiﬁcation. Let

m be any number smaller than |X |/k, representing a training set size. Then,

there exists a distribution D over X × {0, 1} such that:

• There exists a function f : X → {0, 1} with LD(f ) = 0.

•

ES∼Dm [LD(A(S))]

≥

1 2

−

1 2k

.

6 The VC-Dimension
In the previous chapter, we decomposed the error of the ERMH rule into approximation error and estimation error. The approximation error depends on the ﬁt of our prior knowledge (as reﬂected by the choice of the hypothesis class H) to the underlying unknown distribution. In contrast, the deﬁnition of PAC learnability requires that the estimation error would be bounded uniformly over all distributions.
Our current goal is to ﬁgure out which classes H are PAC learnable, and to characterize exactly the sample complexity of learning a given hypothesis class. So far we have seen that ﬁnite classes are learnable, but that the class of all functions (over an inﬁnite size domain) is not. What makes one class learnable and the other unlearnable? Can inﬁnite-size classes be learnable, and, if so, what determines their sample complexity?
We begin the chapter by showing that inﬁnite classes can indeed be learnable, and thus, ﬁniteness of the hypothesis class is not a necessary condition for learnability. We then present a remarkably crisp characterization of the family of learnable classes in the setup of binary valued classiﬁcation with the zero-one loss. This characterization was ﬁrst discovered by Vladimir Vapnik and Alexey Chervonenkis in 1970 and relies on a combinatorial notion called the VapnikChervonenkis dimension (VC-dimension). We formally deﬁne the VC-dimension, provide several examples, and then state the fundamental theorem of statistical learning theory, which integrates the concepts of learnability, VC-dimension, the ERM rule, and uniform convergence.
6.1 Inﬁnite-Size Classes Can Be Learnable
In Chapter 4 we saw that ﬁnite classes are learnable, and in fact the sample complexity of a hypothesis class is upper bounded by the log of its size. To show that the size of the hypothesis class is not the right characterization of its sample complexity, we ﬁrst present a simple example of an inﬁnite-size hypothesis class that is learnable. Example 6.1 Let H be the set of threshold functions over the real line, namely, H = {ha : a ∈ R}, where ha : R → {0, 1} is a function such that ha(x) = 1[x<a]. To remind the reader, 1[x<a] is 1 if x < a and 0 otherwise. Clearly, H is of inﬁnite
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

68

The VC-Dimension

size. Nevertheless, the following lemma shows that H is learnable in the PAC model using the ERM algorithm.

Lemma 6.1 Let H be the class of thresholds as deﬁned earlier. Then, H is PAC learnable, using the ERM rule, with sample complexity of mH( , δ) ≤ log(2/δ)/ .

Proof Let a be a threshold such that the hypothesis h (x) = 1[x<a ] achieves LD(h ) = 0. Let Dx be the marginal distribution over the domain X and let a0 < a < a1 be such that

P [x ∈ (a0, a )] = P [x ∈ (a , a1)] = .

x∼Dx

x∼Dx

mass

mass

a0

a

a1

(If Dx(−∞, a ) ≤ we set a0 = −∞ and similarly for a1). Given a training set S, let b0 = max{x : (x, 1) ∈ S} and b1 = min{x : (x, 0) ∈ S} (if no example in S is positive we set b0 = −∞ and if no example in S is negative we set b1 = ∞). Let bS be a threshold corresponding to an ERM hypothesis, hS, which implies that bS ∈ (b0, b1). Therefore, a suﬃcient condition for LD(hS) ≤ is that both b0 ≥ a0 and b1 ≤ a1. In other words,

P [LD(hS) > ] ≤ P [b0 < a0 ∨ b1 > a1],

S∼Dm

S∼Dm

and using the union bound we can bound the preceding by

S

P
∼Dm

[LD

(hS

)

>

]

≤

S

P
∼Dm

[b0

<

a0]

+

S

P
∼Dm

[b1

>

a1].

(6.1)

The event b0 < a0 happens if and only if all examples in S are not in the interval (a0, a∗), whose probability mass is deﬁned to be , namely,

S

P
∼Dm

[b0

<

a0]

=

P [∀(x,
S∼Dm

y)

∈

S,

x ∈ (a0, a )] = (1 −

)m ≤ e− m.

Since we assume m > log(2/δ)/ it follows that the equation is at most δ/2. In the same way it is easy to see that PS∼Dm [b1 > a1] ≤ δ/2. Combining with Equation (6.1) we conclude our proof.

6.2 The VC-Dimension
We see, therefore, that while ﬁniteness of H is a suﬃcient condition for learnability, it is not a necessary condition. As we will show, a property called the VC-dimension of a hypothesis class gives the correct characterization of its learnability. To motivate the deﬁnition of the VC-dimension, let us recall the No-FreeLunch theorem (Theorem 5.1) and its proof. There, we have shown that without

6.2 The VC-Dimension

69

restricting the hypothesis class, for any learning algorithm, an adversary can construct a distribution for which the learning algorithm will perform poorly, while there is another learning algorithm that will succeed on the same distribution. To do so, the adversary used a ﬁnite set C ⊂ X and considered a family of distributions that are concentrated on elements of C. Each distribution was derived from a “true” target function from C to {0, 1}. To make any algorithm fail, the adversary used the power of choosing a target function from the set of all possible functions from C to {0, 1}.
When considering PAC learnability of a hypothesis class H, the adversary is restricted to constructing distributions for which some hypothesis h ∈ H achieves a zero risk. Since we are considering distributions that are concentrated on elements of C, we should study how H behaves on C, which leads to the following deﬁnition.
definition 6.2 (Restriction of H to C) Let H be a class of functions from X to {0, 1} and let C = {c1, . . . , cm} ⊂ X . The restriction of H to C is the set of functions from C to {0, 1} that can be derived from H. That is,
HC = {(h(c1), . . . , h(cm)) : h ∈ H},
where we represent each function from C to {0, 1} as a vector in {0, 1}|C|.
If the restriction of H to C is the set of all functions from C to {0, 1}, then we say that H shatters the set C. Formally:
definition 6.3 (Shattering) A hypothesis class H shatters a ﬁnite set C ⊂ X if the restriction of H to C is the set of all functions from C to {0, 1}. That is, |HC | = 2|C|.
Example 6.2 Let H be the class of threshold functions over R. Take a set C = {c1}. Now, if we take a = c1 + 1, then we have ha(c1) = 1, and if we take a = c1 − 1, then we have ha(c1) = 0. Therefore, HC is the set of all functions from C to {0, 1}, and H shatters C. Now take a set C = {c1, c2}, where c1 ≤ c2. No h ∈ H can account for the labeling (0, 1), because any threshold that assigns the label 0 to c1 must assign the label 0 to c2 as well. Therefore not all functions from C to {0, 1} are included in HC; hence C is not shattered by H.
Getting back to the construction of an adversarial distribution as in the proof of the No-Free-Lunch theorem (Theorem 5.1), we see that whenever some set C is shattered by H, the adversary is not restricted by H, as they can construct a distribution over C based on any target function from C to {0, 1}, while still maintaining the realizability assumption. This immediately yields:
corollary 6.4 Let H be a hypothesis class of functions from X to {0, 1}. Let m be a training set size. Assume that there exists a set C ⊂ X of size 2m that is shattered by H. Then, for any learning algorithm, A, there exist a distribution D over X × {0, 1} and a predictor h ∈ H such that LD(h) = 0 but with probability of at least 1/7 over the choice of S ∼ Dm we have that LD(A(S)) ≥ 1/8.

70

The VC-Dimension

Corollary 6.4 tells us that if H shatters some set C of size 2m then we cannot learn H using m examples. Intuitively, if a set C is shattered by H, and we receive a sample containing half the instances of C, the labels of these instances give us no information about the labels of the rest of the instances in C – every possible labeling of the rest of the instances can be explained by some hypothesis in H. Philosophically,
If someone can explain every phenomenon, his explanations are worthless.
This leads us directly to the deﬁnition of the VC dimension.
definition 6.5 (VC-dimension) The VC-dimension of a hypothesis class H, denoted VCdim(H), is the maximal size of a set C ⊂ X that can be shattered by H. If H can shatter sets of arbitrarily large size we say that H has inﬁnite VC-dimension.
A direct consequence of Corollary 6.4 is therefore:
theorem 6.6 Let H be a class of inﬁnite VC-dimension. Then, H is not PAC learnable.
Proof Since H has an inﬁnite VC-dimension, for any training set size m, there exists a shattered set of size 2m, and the claim follows by Corollary 6.4.
We shall see later in this chapter that the converse is also true: A ﬁnite VCdimension guarantees learnability. Hence, the VC-dimension characterizes PAC learnability. But before delving into more theory, we ﬁrst show several examples.

6.3 Examples
In this section we calculate the VC-dimension of several hypothesis classes. To show that VCdim(H) = d we need to show that
1. There exists a set C of size d that is shattered by H. 2. Every set C of size d + 1 is not shattered by H.

6.3.1

Threshold Functions
Let H be the class of threshold functions over R. Recall Example 6.2, where we have shown that for an arbitrary set C = {c1}, H shatters C; therefore VCdim(H) ≥ 1. We have also shown that for an arbitrary set C = {c1, c2} where c1 ≤ c2, H does not shatter C. We therefore conclude that VCdim(H) = 1.

6.3 Examples

71

6.3.2

Intervals
Let H be the class of intervals over R, namely, H = {ha,b : a, b ∈ R, a < b}, where ha,b : R → {0, 1} is a function such that ha,b(x) = 1[x∈(a,b)]. Take the set C = {1, 2}. Then, H shatters C (make sure you understand why) and therefore VCdim(H) ≥ 2. Now take an arbitrary set C = {c1, c2, c3} and assume without loss of generality that c1 ≤ c2 ≤ c3. Then, the labeling (1, 0, 1) cannot be obtained by an interval and therefore H does not shatter C. We therefore conclude that VCdim(H) = 2.

6.3.3

Axis Aligned Rectangles Let H be the class of axis aligned rectangles, formally:

where

H = {h(a1,a2,b1,b2) : a1 ≤ a2 and b1 ≤ b2}

h(a1,a2,b1,b2)(x1, x2) =

1 0

if a1 ≤ x1 ≤ a2 and b1 ≤ x2 ≤ b2 otherwise

(6.2)

We shall show in the following that VCdim(H) = 4. To prove this we need to ﬁnd a set of 4 points that are shattered by H, and show that no set of 5 points can be shattered by H. Finding a set of 4 points that are shattered is easy (see Figure 6.1). Now, consider any set C ⊂ R2 of 5 points. In C, take a leftmost point (whose ﬁrst coordinate is the smallest in C), a rightmost point (ﬁrst coordinate is the largest), a lowest point (second coordinate is the smallest), and a highest point (second coordinate is the largest). Without loss of generality, denote C = {c1, . . . , c5} and let c5 be the point that was not selected. Now, deﬁne the labeling (1, 1, 1, 1, 0). It is impossible to obtain this labeling by an axis aligned rectangle. Indeed, such a rectangle must contain c1, . . . , c4; but in this case the rectangle contains c5 as well, because its coordinates are within the intervals deﬁned by the selected points. So, C is not shattered by H, and therefore VCdim(H) = 4.

c1

c4

c5

c2

c3

Figure 6.1 Left: 4 points that are shattered by axis aligned rectangles. Right: Any axis aligned rectangle cannot label c5 by 0 and the rest of the points by 1.

72

The VC-Dimension

6.3.4 6.3.5

Finite Classes
Let H be a ﬁnite class. Then, clearly, for any set C we have |HC| ≤ |H| and thus C cannot be shattered if |H| < 2|C|. This implies that VCdim(H) ≤ log2(|H|). This shows that the PAC learnability of ﬁnite classes follows from the more general statement of PAC learnability of classes with ﬁnite VC-dimension, which we shall see in the next section. Note, however, that the VC-dimension of a ﬁnite class H can be signiﬁcantly smaller than log2(|H|). For example, let X = {1, . . . , k}, for some integer k, and consider the class of threshold functions (as deﬁned in Example 6.2). Then, |H| = k but VCdim(H) = 1. Since k can be arbitrarily large, the gap between log2(|H|) and VCdim(H) can be arbitrarily large.
VC-Dimension and the Number of Parameters
In the previous examples, the VC-dimension happened to equal the number of parameters deﬁning the hypothesis class. While this is often the case, it is not always true. Consider, for example, the domain X = R, and the hypothesis class H = {hθ : θ ∈ R} where hθ : X → {0, 1} is deﬁned by hθ(x) = 0.5 sin(θx) . It is possible to prove that VCdim(H) = ∞, namely, for every d, one can ﬁnd d points that are shattered by H (see Exercise 8).

6.4 The Fundamental Theorem of PAC learning
We have already shown that a class of inﬁnite VC-dimension is not learnable. The converse statement is also true, leading to the fundamental theorem of statistical learning theory:
theorem 6.7 (The Fundamental Theorem of Statistical Learning) Let H be a hypothesis class of functions from a domain X to {0, 1} and let the loss function be the 0 − 1 loss. Then, the following are equivalent:
1. H has the uniform convergence property. 2. Any ERM rule is a successful agnostic PAC learner for H. 3. H is agnostic PAC learnable. 4. H is PAC learnable. 5. Any ERM rule is a successful PAC learner for H. 6. H has a ﬁnite VC-dimension.
The proof of the theorem is given in the next section. Not only does the VC-dimension characterize PAC learnability; it even determines the sample complexity.
theorem 6.8 (The Fundamental Theorem of Statistical Learning – Quantitative Version) Let H be a hypothesis class of functions from a domain X to {0, 1} and let the loss function be the 0 − 1 loss. Assume that VCdim(H) = d < ∞. Then, there are absolute constants C1, C2 such that:

6.5 Proof of Theorem 6.7

73

1. H has the uniform convergence property with sample complexity

d + log(1/δ)

d + log(1/δ)

C1

2

≤ mHUC( , δ) ≤ C2

2

2. H is agnostic PAC learnable with sample complexity

d + log(1/δ)

d + log(1/δ)

C1

2

≤ mH( , δ) ≤ C2

2

3. H is PAC learnable with sample complexity

d + log(1/δ)

d log(1/ ) + log(1/δ)

C1

≤ mH( , δ) ≤ C2

The proof of this theorem is given in Chapter 28.
Remark 6.3 We stated the fundamental theorem for binary classiﬁcation tasks. A similar result holds for some other learning problems such as regression with the absolute loss or the squared loss. However, the theorem does not hold for all learning tasks. In particular, learnability is sometimes possible even though the uniform convergence property does not hold (we will see an example in Chapter 13, Exercise 2). Furthermore, in some situations, the ERM rule fails but learnability is possible with other learning rules.

6.5
6.5.1

Proof of Theorem 6.7
We have already seen that 1 → 2 in Chapter 4. The implications 2 → 3 and 3 → 4 are trivial and so is 2 → 5. The implications 4 → 6 and 5 → 6 follow from the No-Free-Lunch theorem. The diﬃcult part is to show that 6 → 1. The proof is based on two main claims:
• If VCdim(H) = d, then even though H might be inﬁnite, when restricting it to a ﬁnite set C ⊂ X , its “eﬀective” size, |HC |, is only O(|C|d). That is, the size of HC grows polynomially rather than exponentially with |C|. This claim is often referred to as Sauer’s lemma, but it has also been stated and proved independently by Shelah and by Perles. The formal statement is given in Section 6.5.1 later.
• In Section 4 we have shown that ﬁnite hypothesis classes enjoy the uniform convergence property. In Section 6.5.2 later we generalize this result and show that uniform convergence holds whenever the hypothesis class has a “small eﬀective size.” By “small eﬀective size” we mean classes for which |HC | grows polynomially with |C|.
Sauer’s Lemma and the Growth Function
We deﬁned the notion of shattering, by considering the restriction of H to a ﬁnite set of instances. The growth function measures the maximal “eﬀective” size of H on a set of m examples. Formally:

74

The VC-Dimension

definition 6.9 (Growth Function) Let H be a hypothesis class. Then the growth function of H, denoted τH : N → N, is deﬁned as
τH(m) = max HC .
C⊂X :|C|=m
In words, τH (m) is the number of diﬀerent functions from a set C of size m to {0, 1} that can be obtained by restricting H to C.

Obviously, if VCdim(H) = d then for any m ≤ d we have τH(m) = 2m. In such cases, H induces all possible functions from C to {0, 1}. The following beautiful lemma, proposed independently by Sauer, Shelah, and Perles, shows that when m becomes larger than the VC-dimension, the growth function increases polynomially rather than exponentially with m.

lemma 6.10 (Sauer-Shelah-Perles) Let H be a hypothesis class with VCdim(H) ≤

d < ∞. Then, for all m, τH(m) ≤

d i=0

m i

.

In

particular,

if

m

>

d+1

then

τH(m) ≤ (em/d)d.

Proof of Sauer’s Lemma * To prove the lemma it suﬃces to prove the following stronger claim: For any C = {c1, . . . , cm} we have

∀ H, |HC | ≤ |{B ⊆ C : H shatters B}|.

(6.3)

The reason why Equation (6.3) is suﬃcient to prove the lemma is that if VCdim(H) ≤ d then no set whose size is larger than d is shattered by H and therefore

dm

|{B ⊆ C : H shatters B}| ≤

.

i

i=0

When m > d + 1 the right-hand side of the preceding is at most (em/d)d (see Lemma A.5 in Appendix A).
We are left with proving Equation (6.3) and we do it using an inductive argument. For m = 1, no matter what H is, either both sides of Equation (6.3) equal 1 or both sides equal 2 (the empty set is always considered to be shattered by H). Assume Equation (6.3) holds for sets of size k < m and let us prove it for sets of size m. Fix H and C = {c1, . . . , cm}. Denote C = {c2, . . . , cm} and in addition, deﬁne the following two sets:

Y0 = {(y2, . . . , ym) : (0, y2, . . . , ym) ∈ HC ∨ (1, y2, . . . , ym) ∈ HC }, and

Y1 = {(y2, . . . , ym) : (0, y2, . . . , ym) ∈ HC ∧ (1, y2, . . . , ym) ∈ HC }.
It is easy to verify that |HC | = |Y0| + |Y1|. Additionally, since Y0 = HC , using the induction assumption (applied on H and C ) we have that

|Y0| = |HC | ≤ |{B ⊆ C : H shatters B}| = |{B ⊆ C : c1 ∈ B ∧ H shatters B}|.

6.5 Proof of Theorem 6.7

75

Next, deﬁne H ⊆ H to be
H = {h ∈ H : ∃h ∈ H s.t. (1 − h (c1), h (c2), . . . , h (cm)) = (h(c1), h(c2), . . . , h(cm)},
namely, H contains pairs of hypotheses that agree on C and diﬀer on c1. Using this deﬁnition, it is clear that if H shatters a set B ⊆ C then it also shatters the set B ∪ {c1} and vice versa. Combining this with the fact that Y1 = HC and using the inductive assumption (now applied on H and C ) we obtain that
|Y1| = |HC | ≤ |{B ⊆ C : H shatters B}| = |{B ⊆ C : H shatters B ∪ {c1}}| = |{B ⊆ C : c1 ∈ B ∧ H shatters B}| ≤ |{B ⊆ C : c1 ∈ B ∧ H shatters B}|.
Overall, we have shown that
|HC | = |Y0| + |Y1| ≤ |{B ⊆ C : c1 ∈ B ∧ H shatters B}| + |{B ⊆ C : c1 ∈ B ∧ H shatters B}| = |{B ⊆ C : H shatters B}|,
which concludes our proof.

6.5.2

Uniform Convergence for Classes of Small Eﬀective Size

In this section we prove that if H has small eﬀective size then it enjoys the uniform convergence property. Formally,

theorem 6.11 Let H be a class and let τH be its growth function. Then, for every D and every δ ∈ (0, 1), with probability of at least 1 − δ over the choice of S ∼ Dm we have

4+ |LD(h) − LS(h)| ≤

lo√g(τH(2m)) . δ 2m

Before proving the theorem, let us ﬁrst conclude the proof of Theorem 6.7.

Proof of Theorem 6.7 It suﬃces to prove that if the VC-dimension is ﬁnite then the uniform convergence property holds. We will prove that

16d

16d 16 d log(2e/d)

mHUC(

, δ) ≤ 4 (δ

)2

log

(δ )2

+

(δ )2

.

From Sauer’s lemma we have that for m > d, τH(2m) ≤ (2em/d)d. Combining this with Theorem 6.11 we obtain that with probability of at least 1 − δ,

4 + d log(2em/d)

|LS(h) − LD(h)| ≤

√

.

δ 2m

For simplicity assume that d log(2em/d) ≥ 4; hence,

1 2d log(2em/d)

|LS(h) − LD(h)| ≤ δ

. m

76

The VC-Dimension

To ensure that the preceding is at most we need that

2d log(m) 2 d log(2e/d) m ≥ (δ )2 + (δ )2 .

Standard algebraic manipulations (see Lemma A.2 in Appendix A) show that a suﬃcient condition for the preceding to hold is that

2d

2d

4 d log(2e/d)

m≥4 (δ

)2

log

(δ )2

+

(δ )2 .

Remark 6.4 The upper bound on mHUC we derived in the proof Theorem 6.7 is not the tightest possible. A tighter analysis that yields the bounds given in
Theorem 6.8 can be found in Chapter 28.

Proof of Theorem 6.11 * We will start by showing that

4+

E
S∼Dm

sup |LD(h) − LS(h)|
h∈H

≤

l√og(τH(2m)) . 2m

(6.4)

Since the random variable suph∈H |LD(h) − LS(h)| is nonnegative, the proof of the theorem follows directly from the preceding using Markov’s inequality (see Section B.1).
To bound the left-hand side of Equation (6.4) we ﬁrst note that for every h ∈ H, we can rewrite LD(h) = ES ∼Dm [LS (h)], where S = z1, . . . , zm is an additional i.i.d. sample. Therefore,

E
S∼Dm

sup |LD(h) − LS(h)|
h∈H

=E
S∼Dm

sup
h∈H

S

E
∼Dm

LS

(h) − LS(h)

.

A generalization of the triangle inequality yields

S

E
∼Dm

[LS

(h)

−

LS (h)]

≤

S

E
∼Dm

|LS

(h)

−

LS (h)|,

and the fact that supermum of expectation is smaller than expectation of supremum yields

sup E |LS (h) − LS(h)| ≤ E sup |LS (h) − LS(h)|.

h∈H S ∼Dm

S ∼Dm h∈H

Formally, the previous two inequalities follow from Jensen’s inequality. Combining all we obtain

E
S∼Dm

sup |LD(h) − LS(h)|
h∈H

≤E
S,S ∼Dm
=E
S,S ∼Dm

sup |LS (h) − LS(h)|
h∈H

1m

sup h∈H m

( (h, zi) −
i=1

(h, zi)) . (6.5)

6.5 Proof of Theorem 6.7

77

The expectation on the right-hand side is over a choice of two i.i.d. samples
S = z1, . . . , zm and S = z1, . . . , zm. Since all of these 2m vectors are chosen i.i.d., nothing will change if we replace the name of the random vector zi with the
name of the random vector zi. If we do it, instead of the term ( (h, zi) − (h, zi)) in Equation (6.5) we will have the term −( (h, zi) − (h, zi)). It follows that for every σ ∈ {±1}m we have that Equation (6.5) equals

1m

E
S,S ∼Dm

sup h∈H m

σi( (h, zi) − (h, zi))
i=1

Since this holds for every σ ∈ {±1}m, it also holds if we sample each component

of σ uniformly at random from the uniform distribution over {±1}, denoted U±. Hence, Equation (6.5) also equals

1m

EE
σ∼U±m S,S ∼Dm

sup h∈H m

σi( (h, zi) − (h, zi))
i=1

,

and by the linearity of expectation it also equals

1m

EE
S,S ∼Dm σ∼U±m

sup h∈H m

σi( (h, zi) − (h, zi))
i=1

.

Next, ﬁx S and S , and let C be the instances appearing in S and S . Then, we can take the supremum only over h ∈ HC. Therefore,

1m

E
σ∼U±m

sup h∈H m

σi( (h, zi) − (h, zi))
i=1

1m

=E
σ∼U±m

max h∈HC m

σi( (h, zi) − (h, zi))
i=1

.

Fix

some

h

∈

HC

and

denote

θh

=

1 m

m i=1

σi(

(h, zi) −

(h, zi)). Since E[θh] = 0

and θh is an average of independent variables, each of which takes values in

[−1, 1], we have by Hoeﬀding’s inequality that for every ρ > 0,

P[|θh| > ρ] ≤ 2 exp −2 m ρ2 .

Applying the union bound over h ∈ HC, we obtain that for any ρ > 0,

P max |θh| > ρ ≤ 2 |HC | exp −2 m ρ2 .
h∈HC
Finally, Lemma A.4 in Appendix A tells us that the preceding implies

E

max |θh|
h∈HC

4+ ≤

√log(|HC |) . 2m

Combining all with the deﬁnition of τH, we have shown that

E

sup |LD(h) − LS(h)|

4+ ≤

l√og(τH(2m)) .

S∼Dm h∈H

2m

78

The VC-Dimension

6.6 Summary
The fundamental theorem of learning theory characterizes PAC learnability of classes of binary classiﬁers using VC-dimension. The VC-dimension of a class is a combinatorial property that denotes the maximal sample size that can be shattered by the class. The fundamental theorem states that a class is PAC learnable if and only if its VC-dimension is ﬁnite and speciﬁes the sample complexity required for PAC learning. The theorem also shows that if a problem is at all learnable, then uniform convergence holds and therefore the problem is learnable using the ERM rule.
6.7 Bibliographic remarks
The deﬁnition of VC-dimension and its relation to learnability and to uniform convergence is due to the seminal work of Vapnik & Chervonenkis (1971). The relation to the deﬁnition of PAC learnability is due to Blumer, Ehrenfeucht, Haussler & Warmuth (1989).
Several generalizations of the VC-dimension have been proposed. For example, the fat-shattering dimension characterizes learnability of some regression problems (Kearns, Schapire & Sellie 1994, Alon, Ben-David, Cesa-Bianchi & Haussler 1997, Bartlett, Long & Williamson 1994, Anthony & Bartlet 1999), and the Natarajan dimension characterizes learnability of some multiclass learning problems (Natarajan 1989). However, in general, there is no equivalence between learnability and uniform convergence. See (Shalev-Shwartz, Shamir, Srebro & Sridharan 2010, Daniely, Sabato, Ben-David & Shalev-Shwartz 2011).
Sauer’s lemma has been proved by Sauer in response to a problem of Erdos (Sauer 1972). Shelah (with Perles) proved it as a useful lemma for Shelah’s theory of stable models (Shelah 1972). Gil Kalai tells1 us that at some later time, Benjy Weiss asked Perles about such a result in the context of ergodic theory, and Perles, who forgot that he had proved it once, proved it again. Vapnik and Chervonenkis proved the lemma in the context of statistical learning theory.
6.8 Exercises
1. Show the following monotonicity property of VC-dimension: For every two hypothesis classes if H ⊆ H then VCdim(H ) ≤ VCdim(H).
2. Given some ﬁnite domain set, X , and a number k ≤ |X |, ﬁgure out the VCdimension of each of the following classes (and prove your claims): 1. H=Xk = {h ∈ {0, 1}X : |{x : h(x) = 1}| = k}. That is, the set of all functions that assign the value 1 to exactly k elements of X .
1 http://gilkalai.wordpress.com/2008/09/28/ extremal-combinatorics-iii-some-basic-theorems

6.8 Exercises

79

2. Hat−most−k = {h ∈ {0, 1}X : |{x : h(x) = 1}| ≤ k or |{x : h(x) = 0}| ≤ k}. 3. Let X be the Boolean hypercube {0, 1}n. For a set I ⊆ {1, 2, . . . , n} we deﬁne
a parity function hI as follows. On a binary vector x = (x1, x2, . . . , xn) ∈ {0, 1}n,

hI (x) =

xi mod 2 .

i∈I

(That is, hI computes parity of bits in I.) What is the VC-dimension of the class of all such parity functions, Hn-parity = {hI : I ⊆ {1, 2, . . . , n}}? 4. We proved Sauer’s lemma by proving that for every class H of ﬁnite VCdimension d, and every subset A of the domain,

d |A|

|HA| ≤ |{B ⊆ A : H shatters B}| ≤

. i

i=0

Show that there are cases in which the previous two inequalities are strict
(namely, the ≤ can be replaced by <) and cases in which they can be replaced
by equalities. Demonstrate all four combinations of = and <.
5. VC-dimension of axis aligned rectangles in Rd: Let Hrdec be the class of axis aligned rectangles in Rd. We have already seen that VCdim(Hr2ec) = 4. Prove that in general, VCdim(Hrdec) = 2d.
6. VC-dimension of Boolean conjunctions: Let Hcdon be the class of Boolean conjunctions over the variables x1, . . . , xd (d ≥ 2). We already know that this class is ﬁnite and thus (agnostic) PAC learnable. In this question we calculate VCdim(Hcdon). 1. Show that |Hcdon| ≤ 3d + 1.
2. Conclude that VCdim(H) ≤ d log 3.
3. Show that Hcdon shatters the set of unit vectors {ei : i ≤ d}. 4. (**) Show that VCdim(Hcdon) ≤ d.
Hint: Assume by contradiction that there exists a set C = {c1, . . . , cd+1} that is shattered by Hcdon. Let h1, . . . , hd+1 be hypotheses in Hcdon that satisfy

0 i=j ∀i, j ∈ [d + 1], hi(cj) =
1 otherwise

For each i ∈ [d + 1], hi (or more accurately, the conjunction that corresponds to hi) contains some literal i which is false on ci and true on cj for each j = i. Use the Pigeonhole principle to show that there must be a
pair i < j ≤ d + 1 such that i and j use the same xk and use that fact to derive a contradiction to the requirements from the conjunctions hi, hj. 5. Consider the class Hmd con of monotone Boolean conjunctions over {0, 1}d. Monotonicity here means that the conjunctions do not contain negations.

80

The VC-Dimension

As in Hcdon, the empty conjunction is interpreted as the all-positive hypothesis. We augment Hmd con with the all-negative hypothesis h−. Show that VCdim(Hmd con) = d. 7. We have shown that for a ﬁnite hypothesis class H, VCdim(H) ≤ log(|H|) . However, this is just an upper bound. The VC-dimension of a class can be much lower than that:
1. Find an example of a class H of functions over the real interval X = [0, 1] such that H is inﬁnite while VCdim(H) = 1.
2. Give an example of a ﬁnite hypothesis class H over the domain X = [0, 1], where VCdim(H) = log2(|H|) .
8. (*) It is often the case that the VC-dimension of a hypothesis class equals (or can be bounded above by) the number of parameters one needs to set in order to deﬁne each hypothesis in the class. For instance, if H is the class of axis aligned rectangles in Rd, then VCdim(H) = 2d, which is equal to the number of parameters used to deﬁne a rectangle in Rd. Here is an example that shows that this is not always the case. We will see that a hypothesis class might be very complex and even not learnable, although it has a small number of parameters.
Consider the domain X = R, and the hypothesis class

H = {x → sin(θx) : θ ∈ R}

(here, we take −1 = 0). Prove that VCdim(H) = ∞.
Hint: There is more than one way to prove the required result. One option is by applying the following lemma: If 0.x1x2x3 . . ., is the binary expansion of x ∈ (0, 1), then for any natural number m, sin(2mπx) = (1 − xm), provided that ∃k ≥ m s.t. xk = 1. 9. Let H be the class of signed intervals, that is, H = {ha,b,s : a ≤ b, s ∈ {−1, 1}} where

s if x ∈ [a, b] ha,b,s(x) =
−s if x ∈/ [a, b]

Calculate VCdim(H). 10. Let H be a class of functions from X to {0, 1}.
1. Prove that if VCdim(H) ≥ d, for any d, then for some probability distribution D over X × {0, 1}, for every sample size, m,

d−m

E
S∼Dm

[LD

(A(S

))]

≥

min
h∈H

LD (h)

+

2d

Hint: Use Exercise 3 in Chapter 5. 2. Prove that for every H that is PAC learnable, VCdim(H) < ∞. (Note that
this is the implication 3 → 6 in Theorem 6.7.) 11. VC of union: Let H1, . . . , Hr be hypothesis classes over some ﬁxed domain
set X . Let d = maxi VCdim(Hi) and assume for simplicity that d ≥ 3.

6.8 Exercises

81

1. Prove that

VCdim (∪ri=1Hi) ≤ 4d log(2d) + 2 log(r) .

Hint: Take a set of k examples and assume that they are shattered by the union class. Therefore, the union class can produce all 2k possible labelings on these examples. Use Sauer’s lemma to show that the union class cannot produce more than rkd labelings. Therefore, 2k < rkd. Now use Lemma A.2.
2. (*) Prove that for r = 2 it holds that

VCdim (H1 ∪ H2) ≤ 2d + 1.

12. Dudley classes: In this question we discuss an algebraic framework for deﬁning concept classes over Rn and show a connection between the VC dimension of such classes and their algebraic properties. Given a function f : Rn → R we deﬁne the corresponding function, POS (f )(x) = 1[f(x)>0]. For a class F of real valued functions we deﬁne a corresponding class of functions POS (F) = {POS (f ) : f ∈ F}. We say that a family, F, of real valued functions is linearly closed if for all f, g ∈ F and r ∈ R, (f + rg) ∈ F (where addition and scalar multiplication of functions are deﬁned point wise, namely, for all x ∈ Rn, (f + rg)(x) = f (x) + rg(x)). Note that if a family of functions is linearly closed then we can view it as a vector space over the reals. For a function g : Rn → R and a family of functions F , let F +g d=ef {f +g : f ∈ F }. Hypothesis classes that have a representation as POS (F + g) for some vector space of functions F and some function g are called Dudley classes. 1. Show that for every g : Rn → R and every vector space of functions F as deﬁned earlier, VCdim(POS (F + g)) = VCdim(POS (F)).
2. (**) For every linearly closed family of real valued functions F, the VCdimension of the corresponding class POS (F) equals the linear dimension of F (as a vector space). Hint: Let f1, . . . , fd be a basis for the vector space F . Consider the mapping x → (f1(x), . . . , fd(x)) (from Rn to Rd). Note that this mapping induces a matching between functions over Rn of the form POS (f ) and homogeneous linear halfspaces in Rd (the VC-dimension of the class of homogeneous linear halfspaces is analyzed in Chapter 9).
3. Show that each of the following classes can be represented as a Dudley class: 1. The class HSn of halfspaces over Rn (see Chapter 9). 2. The class HHSn of all homogeneous halfspaces over Rn (see Chapter 9). 3. The class Bd of all functions deﬁned by (open) balls in Rd. Use the Dudley representation to ﬁgure out the VC-dimension of this class. 4. Let Pnd denote the class of functions deﬁned by polynomial inequalities of degree ≤ d, namely,
Pnd = {hp : p is a polynomial of degree ≤ d in the variables x1, . . . , xn},

82

The VC-Dimension

where, for x = (x1. . . . , xn), hp(x) = 1[p(x)≥0] (the degree of a multivariable polynomial is the maximal sum of variable exponents over all of its terms. For example, the degree of p(x) = 3x13x22 + 4x3x72 is 5). 1. Use the Dudley representation to ﬁgure out the VC-dimension of the
class P1d – the class of all d-degree polynomials over R. 2. Prove that the class of all polynomial classiﬁers over R has inﬁnite
VC-dimension.
3. Use the Dudley representation to ﬁgure out the VC-dimension of the class Pnd (as a function of d and n).

7 Nonuniform Learnability
The notions of PAC learnability discussed so far in the book allow the sample sizes to depend on the accuracy and conﬁdence parameters, but they are uniform with respect to the labeling rule and the underlying data distribution. Consequently, classes that are learnable in that respect are limited (they must have a ﬁnite VC-dimension, as stated by Theorem 6.7). In this chapter we consider more relaxed, weaker notions of learnability. We discuss the usefulness of such notions and provide characterization of the concept classes that are learnable using these deﬁnitions.
We begin this discussion by deﬁning a notion of “nonuniform learnability” that allows the sample size to depend on the hypothesis to which the learner is compared. We then provide a characterization of nonuniform learnability and show that nonuniform learnability is a strict relaxation of agnostic PAC learnability. We also show that a suﬃcient condition for nonuniform learnability is that H is a countable union of hypothesis classes, each of which enjoys the uniform convergence property. These results will be proved in Section 7.2 by introducing a new learning paradigm, which is called Structural Risk Minimization (SRM). In Section 7.3 we specify the SRM paradigm for countable hypothesis classes, which yields the Minimum Description Length (MDL) paradigm. The MDL paradigm gives a formal justiﬁcation to a philosophical principle of induction called Occam’s razor. Next, in Section 7.4 we introduce consistency as an even weaker notion of learnability. Finally, we discuss the signiﬁcance and usefulness of the diﬀerent notions of learnability.
7.1 Nonuniform Learnability
“Nonuniform learnability” allows the sample size to be nonuniform with respect to the diﬀerent hypotheses with which the learner is competing. We say that a hypothesis h is ( , δ)-competitive with another hypothesis h if, with probability higher than (1 − δ),
LD(h) ≤ LD(h ) + .
In PAC learnability, this notion of “competitiveness” is not very useful, as we are looking for a hypothesis with an absolute low risk (in the realizable case) or
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

84

Nonuniform Learnability

with a low risk compared to the minimal risk achieved by hypotheses in our class (in the agnostic case). Therefore, the sample size depends only on the accuracy and conﬁdence parameters. In nonuniform learnability, however, we allow the sample size to be of the form mH( , δ, h); namely, it depends also on the h with which we are competing. Formally,

definition 7.1 A hypothesis class H is nonuniformly learnable if there exist a

learning

algorithm,

A,

and

a

function

mNUL
H

:

(0,

1)2 ×H

→

N

such

that,

for

every

,δ

∈

(0, 1)

and

for

every

h

∈

H,

if

m

≥

mNUL
H

(

, δ, h)

then

for

every

distribution

D, with probability of at least 1 − δ over the choice of S ∼ Dm, it holds that

LD(A(S)) ≤ LD(h) + .
At this point it might be useful to recall the deﬁnition of agnostic PAC learnability (Deﬁnition 3.3): A hypothesis class H is agnostically PAC learnable if there exist a learning algorithm, A, and a function mH : (0, 1)2 → N such that, for every , δ ∈ (0, 1) and for every distribution D, if m ≥ mH( , δ), then with probability of at least 1 − δ over the choice of S ∼ Dm it holds that

LD(A(S)) ≤ min LD(h ) + .
h ∈H
Note that this implies that for every h ∈ H

LD(A(S)) ≤ LD(h) + .

In both types of learnability, we require that the output hypothesis will be ( , δ)-competitive with every other hypothesis in the class. But the diﬀerence between these two notions of learnability is the question of whether the sample size m may depend on the hypothesis h to which the error of A(S) is compared. Note that that nonuniform learnability is a relaxation of agnostic PAC learnability. That is, if a class is agnostic PAC learnable then it is also nonuniformly learnable.

7.1.1

Characterizing Nonuniform Learnability
Our goal now is to characterize nonuniform learnability. In the previous chapter we have found a crisp characterization of PAC learnable classes, by showing that a class of binary classiﬁers is agnostic PAC learnable if and only if its VCdimension is ﬁnite. In the following theorem we ﬁnd a diﬀerent characterization for nonuniform learnable classes for the task of binary classiﬁcation.
theorem 7.2 A hypothesis class H of binary classiﬁers is nonuniformly learnable if and only if it is a countable union of agnostic PAC learnable hypothesis classes.
The proof of Theorem 7.2 relies on the following result of independent interest:

7.2 Structural Risk Minimization

85

theorem 7.3 Let H be a hypothesis class that can be written as a countable
union of hypothesis classes, H = n∈N Hn, where each Hn enjoys the uniform convergence property. Then, H is nonuniformly learnable.

Recall that in Chapter 4 we have shown that uniform convergence is suﬃcient for agnostic PAC learnability. Theorem 7.3 generalizes this result to nonuniform learnability. The proof of this theorem will be given in the next section by introducing a new learning paradigm. We now turn to proving Theorem 7.2.

Proof of Theorem 7.2 First assume that H = n∈N Hn where each Hn is agnostic PAC learnable. Using the fundamental theorem of statistical learning, it

follows that each Hn has the uniform convergence property. Therefore, using

Theorem 7.3 we obtain that H is nonuniform learnable.

For the other direction, assume that H is nonuniform learnable using some

algorithm

A.

For

every

n

∈

N,

let

Hn

=

{h

∈

H

:

mNUL
H

(1/8,

1/7,

h)

≤

n}.

Clearly,

H

=

∪n∈NHn.

In

addition,

using

the

deﬁnition

of

mNUL
H

we

know

that

for any distribution D that satisﬁes the realizability assumption with respect to

Hn, with probability of at least 6/7 over S ∼ Dn we have that LD(A(S)) ≤ 1/8.

Using the fundamental theorem of statistical learning, this implies that the VC-

dimension of Hn must be ﬁnite, and therefore Hn is agnostic PAC learnable.

The following example shows that nonuniform learnability is a strict relaxation of agnostic PAC learnability; namely, there are hypothesis classes that are nonuniform learnable but are not agnostic PAC learnable.
Example 7.1 Consider a binary classiﬁcation problem with the instance domain being X = R. For every n ∈ N let Hn be the class of polynomial classiﬁers of degree n; namely, Hn is the set of all classiﬁers of the form h(x) = sign(p(x)) where p : R → R is a polynomial of degree n. Let H = n∈N Hn. Therefore, H is the class of all polynomial classiﬁers over R. It is easy to verify that VCdim(H) = ∞ while VCdim(Hn) = n + 1 (see Exercise 12). Hence, H is not PAC learnable, while on the basis of Theorem 7.3, H is nonuniformly learnable.

7.2 Structural Risk Minimization
So far, we have encoded our prior knowledge by specifying a hypothesis class H, which we believe includes a good predictor for the learning task at hand. Yet another way to express our prior knowledge is by specifying preferences over hypotheses within H. In the Structural Risk Minimization (SRM) paradigm, we do so by ﬁrst assuming that H can be written as H = n∈N Hn and then specifying a weight function, w : N → [0, 1], which assigns a weight to each hypothesis class, Hn, such that a higher weight reﬂects a stronger preference for the hypothesis class. In this section we discuss how to learn with such prior knowledge. In the next section we describe a couple of important weighting schemes, including Minimum Description Length.

86

Nonuniform Learnability

Concretely, let H be a hypothesis class that can be written as H = n∈N Hn. For example, H may be the class of all polynomial classiﬁers where each Hn is

the class of polynomial classiﬁers of degree n (see Example 7.1). Assume that for

each n, the class Hn enjoys the uniform convergence property (see Deﬁnition 4.3

in

Chapter

4)

with

a

sample

complexity

function

mUC
Hn

(

, δ).

Let

us

also

deﬁne

the function n : N × (0, 1) → (0, 1) by

n(m, δ) = min{

∈

(0, 1)

:

mUC
Hn

(

, δ)

≤

m}.

(7.1)

In words, we have a ﬁxed sample size m, and we are interested in the lowest possible upper bound on the gap between empirical and true risks achievable by using a sample of m examples.
From the deﬁnitions of uniform convergence and n, it follows that for every m and δ, with probability of at least 1 − δ over the choice of S ∼ Dm we have that

∀h ∈ Hn, |LD(h) − LS(h)| ≤ n(m, δ).

(7.2)

Let w : N → [0, 1] be a function such that

∞ n=1

w(n)

≤

1.

We

refer

to

w

as

a weight function over the hypothesis classes H1, H2, . . .. Such a weight function

can reﬂect the importance that the learner attributes to each hypothesis class,

or some measure of the complexity of diﬀerent hypothesis classes. If H is a ﬁnite

union of N hypothesis classes, one can simply assign the same weight of 1/N to

all hypothesis classes. This equal weighting corresponds to no a priori preference

to any hypothesis class. Of course, if one believes (as prior knowledge) that a

certain hypothesis class is more likely to contain the correct target function,

then it should be assigned a larger weight, reﬂecting this prior knowledge. When

H is a (countable) inﬁnite union of hypothesis classes, a uniform weighting is

not possible but many other weighting schemes may work. For example, one can

choose

w(n)

=

6 π 2 n2

or

w(n)

=

2−n. Later

in

this chapter

we

will

provide another

convenient way to deﬁne weighting functions using description languages.

The SRM rule follows a “bound minimization” approach. This means that

the goal of the paradigm is to ﬁnd a hypothesis that minimizes a certain upper

bound on the true risk. The bound that the SRM rule wishes to minimize is

given in the following theorem.

theorem 7.4 Let w : N → [0, 1] be a function such that

∞ n=1

w(n)

≤

1.

Let

H be a hypothesis class that can be written as H = n∈N Hn, where for each n, Hn satisﬁes the uniform convergence property with a sample complexity function

mUC
Hn

.

Let

n be as deﬁned in Equation (7.1). Then, for every δ ∈ (0, 1) and

distribution D, with probability of at least 1 − δ over the choice of S ∼ Dm, the

following bound holds (simultaneously) for every n ∈ N and h ∈ Hn.

|LD(h) − LS(h)| ≤ n(m, w(n) · δ).

Therefore, for every δ ∈ (0, 1) and distribution D, with probability of at least

7.2 Structural Risk Minimization

87

1 − δ it holds that

∀h ∈ H, LD(h) ≤ LS(h) + min n(m, w(n) · δ).
n:h∈Hn

(7.3)

Proof For each n deﬁne δn = w(n)δ. Applying the assumption that uniform convergence holds for all n with the rate given in Equation (7.2), we obtain that
if we ﬁx n in advance, then with probability of at least 1 − δn over the choice of S ∼ Dm,

∀h ∈ Hn, |LD(h) − LS(h)| ≤ n(m, δn).

Applying the union bound over n = 1, 2, . . ., we obtain that with probability of
at least 1 − n δn = 1 − δ n w(n) ≥ 1 − δ, the preceding holds for all n, which concludes our proof.

Denote n(h) = min{n : h ∈ Hn},
and then Equation (7.3) implies that

(7.4)

LD(h) ≤ LS(h) + n(h)(m, w(n(h)) · δ).
The SRM paradigm searches for h that minimizes this bound, as formalized in the following pseudocode:

Structural Risk Minimization (SRM)

prior knowledge:

H=

n

Hn

where

Hn

has

uniform

convergence

with

mUC
Hn

w : N → [0, 1] where n w(n) ≤ 1

deﬁne: n as in Equation (7.1) ; n(h) as in Equation (7.4)

input: training set S ∼ Dm, conﬁdence δ

output: h ∈ argminh∈H LS(h) + n(h)(m, w(n(h)) · δ)

Unlike the ERM paradigm discussed in previous chapters, we no longer just care about the empirical risk, LS(h), but we are willing to trade some of our bias toward low empirical risk with a bias toward classes for which n(h)(m, w(n(h))·δ) is smaller, for the sake of a smaller estimation error.
Next we show that the SRM paradigm can be used for nonuniform learning of every class, which is a countable union of uniformly converging hypothesis classes.

theorem 7.5 Let H be a hypothesis class such that H = n∈N Hn, where

each

Hn

has

the

uniform

convergence

property

with

sample

complexity

mUC
Hn

.

Let

w

:

N

→

[0, 1]

be

such

that

w(n)

=

6 n2 π 2

.

Then,

H

is

nonuniformly

learnable

using the SRM rule with rate

mNUL
H

(

, δ, h)

≤

mUC
Hn(h)

/2 ,

6δ (πn(h))2

.

88

Nonuniform Learnability

Proof Let A be the SRM algorithm with respect to the weighting function w.

For every h ∈ H,

,

and

δ,

let

m

≥

m ( UC
Hn(h)

, w(n(h))δ).

Using

the

fact

that

n w(n) = 1, we can apply Theorem 7.4 to get that, with probability of at least

1 − δ over the choice of S ∼ Dm, we have that for every h ∈ H,

LD(h ) ≤ LS(h ) + n(h )(m, w(n(h ))δ).
The preceding holds in particular for the hypothesis A(S) returned by the SRM rule. By the deﬁnition of SRM we obtain that

LD(A(S)) ≤ min LS(h ) + n(h )(m, w(n(h ))δ) ≤ LS(h) + n(h)(m, w(n(h))δ).
h

Finally,

if

m

≥

m ( UC
Hn(h)

/2, w(n(h))δ)

then

clearly

n(h)(m, w(n(h))δ) ≤

/2. In

addition, from the uniform convergence property of each Hn we have that with

probability of more than 1 − δ,

LS(h) ≤ LD(h) + /2.
Combining all the preceding we obtain that LD(A(S)) ≤ LD(h) + , which concludes our proof.

Note that the previous theorem also proves Theorem 7.3.

Remark 7.2 (No-Free-Lunch for Nonuniform Learnability) We have shown that any countable union of classes of ﬁnite VC-dimension is nonuniformly learnable. It turns out that, for any inﬁnite domain set, X , the class of all binary valued functions over X is not a countable union of classes of ﬁnite VC-dimension. We leave the proof of this claim as a (nontrivial) exercise (see Exercise 5). It follows that, in some sense, the no free lunch theorem holds for nonuniform learning as well: namely, whenever the domain is not ﬁnite, there exists no nonuniform learner with respect to the class of all deterministic binary classiﬁers (although for each such classiﬁer there exists a trivial algorithm that learns it – ERM with respect to the hypothesis class that contains only this classiﬁer).

It is interesting to compare the nonuniform learnability result given in The-

orem 7.5 to the task of agnostic PAC learning any speciﬁc Hn separately. The

prior knowledge, or bias, of a nonuniform learner for H is weaker – it is searching

for a model throughout the entire class H, rather than being focused on one spe-

ciﬁc Hn. The cost of this weakening of prior knowledge is the increase in sample

complexity needed to compete with any speciﬁc h ∈ Hn. For a concrete evalua-

tion of this gap, consider the task of binary classiﬁcation with the zero-one loss.

Assume

that

for

all

n,

VCdim(Hn)

=

n.

Since

mUC
Hn

(

, δ)

=

C

n+log(1/δ)
2

(where

C is the contant appearing in Theorem 6.8), a straightforward calculation shows

that

2 log(2n)

mNUL
H

(

,

δ,

h)

−

mUC
Hn

(

/2, δ)

≤

4C

2.

That is, the cost of relaxing the learner’s prior knowledge from a speciﬁc Hn that contains the target h to a countable union of classes depends on the log of

7.3 Minimum Description Length and Occam’s Razor

89

the index of the ﬁrst class in which h resides. That cost increases with the index of the class, which can be interpreted as reﬂecting the value of knowing a good priority order on the hypotheses in H.

7.3 Minimum Description Length and Occam’s Razor

Let H be a countable hypothesis class. Then, we can write H as a countable

union of singleton classes, namely, H = n∈N{hn}. By Hoeﬀding’s inequality (Lemma 4.5), each singleton class has the uniform convergence property with

rate

mUC( , δ)

=

log(2/δ) 22

.

Therefore,

the

function

n given in Equation (7.1)

becomes n(m, δ) =

log(2/δ) 2m

and

the

SRM

rule

becomes

− log(w(n)) + log(2/δ)

argmin LS(h) +
hn ∈H

. 2m

Equivalently, we can think of w as a function from H to [0, 1], and then the SRM rule becomes

− log(w(h)) + log(2/δ)

argmin LS(h) +
h∈H

. 2m

It follows that in this case, the prior knowledge is solely determined by the weight we assign to each hypothesis. We assign higher weights to hypotheses that we believe are more likely to be the correct one, and in the learning algorithm we prefer hypotheses that have higher weights.
In this section we discuss a particular convenient way to deﬁne a weight function over H, which is derived from the length of descriptions given to hypotheses. Having a hypothesis class, one can wonder about how we describe, or represent, each hypothesis in the class. We naturally ﬁx some description language. This can be English, or a programming language, or some set of mathematical formulas. In any of these languages, a description consists of ﬁnite strings of symbols (or characters) drawn from some ﬁxed alphabet. We shall now formalize these notions.
Let H be the hypothesis class we wish to describe. Fix some ﬁnite set Σ of symbols (or “characters”), which we call the alphabet. For concreteness, we let Σ = {0, 1}. A string is a ﬁnite sequence of symbols from Σ; for example, σ = (0, 1, 1, 1, 0) is a string of length 5. We denote by |σ| the length of a string. The set of all ﬁnite length strings is denoted Σ∗. A description language for H is a function d : H → Σ∗, mapping each member h of H to a string d(h). d(h) is called “the description of h,” and its length is denoted by |h|.
We shall require that description languages be preﬁx-free; namely, for every distinct h, h , d(h) is not a preﬁx of d(h ). That is, we do not allow that any string d(h) is exactly the ﬁrst |h| symbols of any longer string d(h ). Preﬁx-free collections of strings enjoy the following combinatorial property:

90

Nonuniform Learnability

lemma 7.6 (Kraft Inequality) If S ⊆ {0, 1}∗ is a preﬁx-free set of strings, then

1 ≤ 1.
2|σ|
σ∈S

Proof Deﬁne a probability distribution over the members of S as follows: Re-

peatedly toss an unbiased coin, with faces labeled 0 and 1, until the sequence

of outcomes is a member of S; at that point, stop. For each σ ∈ S, let P (σ)

be the probability that this process generates the string σ. Note that since S is

preﬁx-free, for every σ ∈ S, if the coin toss outcomes follow the bits of σ then

we will stop only once the sequence of outcomes equals σ. We therefore get that,

for

every

σ

∈

S,

P (σ)

=

. 1
2|σ|

Since

probabilities

add

up

to

at

most

1,

our

proof

is concluded.

In light of Kraft’s inequality, any preﬁx-free description language of a hypoth-

esis class, H, gives rise to a weighting function w over that hypothesis class – we

will

simply

set

w(h)

=

. 1
2|h|

This

observation

immediately

yields

the

following:

theorem 7.7 Let H be a hypothesis class and let d : H → {0, 1}∗ be a preﬁxfree description language for H. Then, for every sample size, m, every conﬁdence

parameter, δ > 0, and every probability distribution, D, with probability greater than 1 − δ over the choice of S ∼ Dm we have that,

∀h ∈ H, LD(h) ≤ LS(h) + where |h| is the length of d(h).

|h| + ln(2/δ) ,
2m

Proof Choose w(h) = 1/2|h|, apply Theorem 7.4 with n(m, δ) =

ln(2/δ) 2m

,

and

note that ln(2|h|) = |h| ln(2) < |h|.

As was the case with Theorem 7.4, this result suggests a learning paradigm

for H – given a training set, S, search for a hypothesis h ∈ H that minimizes

the bound, LS(h) +

|h|+ln(2/δ) 2m

.

In

particular,

it

suggests

trading

oﬀ

empirical

risk for saving description length. This yields the Minimum Description Length

learning paradigm.

Minimum Description Length (MDL)

prior knowledge: H is a countable hypothesis class H is described by a preﬁx-free language over {0, 1} For every h ∈ H, |h| is the length of the representation of h
input: A training set S ∼ Dm, conﬁdence δ

output: h ∈ argminh∈H LS(h) +

|h|+ln(2/δ) 2m

Example 7.3 Let H be the class of all predictors that can be implemented using some programming language, say, C++. Let us represent each program using the

7.3 Minimum Description Length and Occam’s Razor

91

binary string obtained by running the gzip command on the program (this yields a preﬁx-free description language over the alphabet {0, 1}). Then, |h| is simply the length (in bits) of the output of gzip when running on the C++ program corresponding to h.

7.3.1

Occam’s Razor
Theorem 7.7 suggests that, having two hypotheses sharing the same empirical risk, the true risk of the one that has shorter description can be bounded by a lower value. Thus, this result can be viewed as conveying a philosophical message:

A short explanation (that is, a hypothesis that has a short length) tends to be more valid than a long explanation.

This is a well known principle, called Occam’s razor, after William of Ockham,

a 14th-century English logician, who is believed to have been the ﬁrst to phrase

it explicitly. Here, we provide one possible justiﬁcation to this principle. The

inequality of Theorem 7.7 shows that the more complex a hypothesis h is (in the

sense of having a longer description), the larger the sample size it has to ﬁt to

guarantee that it has a small true risk, LD(h).

At a second glance, our Occam razor claim might seem somewhat problematic.

In the context in which the Occam razor principle is usually invoked in science,

the language according to which complexity is measured is a natural language,

whereas here we may consider any arbitrary abstract description language. As-

sume that we have two hypotheses such that |h | is much smaller than |h|. By

the preceding result, if both have the same error on a given training set, S, then

the true error of h may be much higher than the true error of h , so one should

prefer h over h. However, we could have chosen a diﬀerent description language,

say, one that assigns a string of length 3 to h and a string of length 100000 to h .

Suddenly it looks as if one should prefer h over h . But these are the same h and

h for which we argued two sentences ago that h should be preferable. Where is

the catch here?

Indeed, there is no inherent generalizability diﬀerence between hypotheses.

The crucial aspect here is the dependency order between the initial choice of

language (or, preference over hypotheses) and the training set. As we know from

the basic Hoeﬀding’s bound (Equation (4.2)), if we commit to any hypothesis be-

fore seeing the data, then we are guaranteed a rather small estimation error term

LD(h) ≤ LS(h) +

ln(2/δ) 2m

.

Choosing

a

description

language

(or,

equivalently,

some weighting of hypotheses) is a weak form of committing to a hypothesis.

Rather than committing to a single hypothesis, we spread out our commitment

among many. As long as it is done independently of the training sample, our gen-

eralization bound holds. Just as the choice of a single hypothesis to be evaluated

by a sample can be arbitrary, so is the choice of description language.

92

Nonuniform Learnability

7.4 Other Notions of Learnability – Consistency

The notion of learnability can be further relaxed by allowing the needed sample sizes to depend not only on , δ, and h but also on the underlying data-generating probability distribution D (that is used to generate the training sample and to determine the risk). This type of performance guarantee is captured by the notion of consistency1 of a learning rule.

definition 7.8 (Consistency) Let Z be a domain set, let P be a set of

probability distributions over Z, and let H be a hypothesis class. A learn-

ing rule A is consistent with respect to H and P if there exists a function

mCON
H

:

(0, 1)2

×

H

×

P

→

N

such

that,

for

every

, δ ∈ (0, 1), every h ∈ H, and

every

D

∈

P,

if

m

≥

mNUL
H

(

, δ, h, D)

then

with

probability

of

at

least

1−

δ

over

the choice of S ∼ Dm it holds that

LD(A(S)) ≤ LD(h) + .
If P is the set of all distributions,2 we say that A is universally consistent with respect to H.

The notion of consistency is, of course, a relaxation of our previous notion of nonuniform learnability. Clearly if an algorithm nonuniformly learns a class H it is also universally consistent for that class. The relaxation is strict in the sense that there are consistent learning rules that are not successful nonuniform learners. For example, the algorithm Memorize deﬁned in Example 7.4 later is universally consistent for the class of all binary classiﬁers over N. However, as we have argued before, this class is not nonuniformly learnable.
Example 7.4 Consider the classiﬁcation prediction algorithm Memorize deﬁned as follows. The algorithm memorizes the training examples, and, given a test point x, it predicts the majority label among all labeled instances of x that exist in the training sample (and some ﬁxed default label if no instance of x appears in the training set). It is possible to show (see Exercise 6) that the Memorize algorithm is universally consistent for every countable domain X and a ﬁnite label set Y (w.r.t. the zero-one loss).
Intuitively, it is not obvious that the Memorize algorithm should be viewed as a learner, since it lacks the aspect of generalization, namely, of using observed data to predict the labels of unseen examples. The fact that Memorize is a consistent algorithm for the class of all functions over any countable domain set therefore raises doubt about the usefulness of consistency guarantees. Furthermore, the sharp-eyed reader may notice that the “bad learner” we introduced in Chapter 2,
1 In the literature, consistency is often deﬁned using the notion of either convergence in probability (corresponding to weak consistency) or almost sure convergence (corresponding to strong consistency).
2 Formally, we assume that Z is endowed with some sigma algebra of subsets Ω, and by “all distributions” we mean all probability distributions that have Ω contained in their associated family of measurable subsets.

7.5 Discussing the Diﬀerent Notions of Learnability

93

which led to overﬁtting, is in fact the Memorize algorithm. In the next section we discuss the signiﬁcance of the diﬀerent notions of learnability and revisit the No-Free-Lunch theorem in light of the diﬀerent deﬁnitions of learnability.
7.5 Discussing the Diﬀerent Notions of Learnability
We have given three deﬁnitions of learnability and we now discuss their usefulness. As is usually the case, the usefulness of a mathematical deﬁnition depends on what we need it for. We therefore list several possible goals that we aim to achieve by deﬁning learnability and discuss the usefulness of the diﬀerent deﬁnitions in light of these goals.
What Is the Risk of the Learned Hypothesis? The ﬁrst possible goal of deriving performance guarantees on a learning algorithm is bounding the risk of the output predictor. Here, both PAC learning and nonuniform learning give us an upper bound on the true risk of the learned hypothesis based on its empirical risk. Consistency guarantees do not provide such a bound. However, it is always possible to estimate the risk of the output predictor using a validation set (as will be described in Chapter 11).
How Many Examples Are Required to Be as Good as the Best Hypothesis in H? When approaching a learning problem, a natural question is how many examples we need to collect in order to learn it. Here, PAC learning gives a crisp answer. However, for both nonuniform learning and consistency, we do not know in advance how many examples are required to learn H. In nonuniform learning this number depends on the best hypothesis in H, and in consistency it also depends on the underlying distribution. In this sense, PAC learning is the only useful deﬁnition of learnability. On the ﬂip side, one should keep in mind that even if the estimation error of the predictor we learn is small, its risk may still be large if H has a large approximation error. So, for the question “How many examples are required to be as good as the Bayes optimal predictor?” even PAC guarantees do not provide us with a crisp answer. This reﬂects the fact that the usefulness of PAC learning relies on the quality of our prior knowledge.
PAC guarantees also help us to understand what we should do next if our learning algorithm returns a hypothesis with a large risk, since we can bound the part of the error that stems from estimation error and therefore know how much of the error is attributed to approximation error. If the approximation error is large, we know that we should use a diﬀerent hypothesis class. Similarly, if a nonuniform algorithm fails, we can consider a diﬀerent weighting function over (subsets of) hypotheses. However, when a consistent algorithm fails, we have no idea whether this is because of the estimation error or the approximation error. Furthermore, even if we are sure we have a problem with the estimation

94

Nonuniform Learnability

error term, we do not know how many more examples are needed to make the estimation error small.

How to Learn? How to Express Prior Knowledge? Maybe the most useful aspect of the theory of learning is in providing an answer to the question of “how to learn.” The deﬁnition of PAC learning yields the limitation of learning (via the No-Free-Lunch theorem) and the necessity of prior knowledge. It gives us a crisp way to encode prior knowledge by choosing a hypothesis class, and once this choice is made, we have a generic learning rule – ERM. The deﬁnition of nonuniform learnability also yields a crisp way to encode prior knowledge by specifying weights over (subsets of) hypotheses of H. Once this choice is made, we again have a generic learning rule – SRM. The SRM rule is also advantageous in model selection tasks, where prior knowledge is partial. We elaborate on model selection in Chapter 11 and here we give a brief example.
Consider the problem of ﬁtting a one dimensional polynomial to data; namely, our goal is to learn a function, h : R → R, and as prior knowledge we consider the hypothesis class of polynomials. However, we might be uncertain regarding which degree d would give the best results for our data set: A small degree might not ﬁt the data well (i.e., it will have a large approximation error), whereas a high degree might lead to overﬁtting (i.e., it will have a large estimation error). In the following we depict the result of ﬁtting a polynomial of degrees 2, 3, and 10 to the same training set.

degree 2

degree 3

degree 10

It is easy to see that the empirical risk decreases as we enlarge the degree. Therefore, if we choose H to be the class of all polynomials up to degree 10 then the ERM rule with respect to this class would output a 10 degree polynomial and would overﬁt. On the other hand, if we choose too small a hypothesis class, say, polynomials up to degree 2, then the ERM would suﬀer from underﬁtting (i.e., a large approximation error). In contrast, we can use the SRM rule on the set of all polynomials, while ordering subsets of H according to their degree, and this will yield a 3rd degree polynomial since the combination of its empirical risk and the bound on its estimation error is the smallest. In other words, the SRM rule enables us to select the right model on the basis of the data itself. The price we pay for this ﬂexibility (besides a slight increase of the estimation error relative to PAC learning w.r.t. the optimal degree) is that we do not know in

7.5 Discussing the Diﬀerent Notions of Learnability

95

advance how many examples are needed to compete with the best hypothesis in H.
Unlike the notions of PAC learnability and nonuniform learnability, the deﬁnition of consistency does not yield a natural learning paradigm or a way to encode prior knowledge. In fact, in many cases there is no need for prior knowledge at all. For example, we saw that even the Memorize algorithm, which intuitively should not be called a learning algorithm, is a consistent algorithm for any class deﬁned over a countable domain and a ﬁnite label set. This hints that consistency is a very weak requirement.
Which Learning Algorithm Should We Prefer? One may argue that even though consistency is a weak requirement, it is desirable that a learning algorithm will be consistent with respect to the set of all functions from X to Y, which gives us a guarantee that for enough training examples, we will always be as good as the Bayes optimal predictor. Therefore, if we have two algorithms, where one is consistent and the other one is not consistent, we should prefer the consistent algorithm. However, this argument is problematic for two reasons. First, maybe it is the case that for most “natural” distributions we will observe in practice that the sample complexity of the consistent algorithm will be so large so that in every practical situation we will not obtain enough examples to enjoy this guarantee. Second, it is not very hard to make any PAC or nonuniform learner consistent with respect to the class of all functions from X to Y. Concretely, consider a countable domain, X , a ﬁnite label set Y, and a hypothesis class, H, of functions from X to Y. We can make any nonuniform learner for H be consistent with respect to the class of all classiﬁers from X to Y using the following simple trick: Upon receiving a training set, we will ﬁrst run the nonuniform learner over the training set, and then we will obtain a bound on the true risk of the learned predictor. If this bound is small enough we are done. Otherwise, we revert to the Memorize algorithm. This simple modiﬁcation makes the algorithm consistent with respect to all functions from X to Y. Since it is easy to make any algorithm consistent, it may not be wise to prefer one algorithm over the other just because of consistency considerations.

7.5.1

The No-Free-Lunch Theorem Revisited
Recall that the No-Free-Lunch theorem (Theorem 5.1 from Chapter 5) implies that no algorithm can learn the class of all classiﬁers over an inﬁnite domain. In contrast, in this chapter we saw that the Memorize algorithm is consistent with respect to the class of all classiﬁers over a countable inﬁnite domain. To understand why these two statements do not contradict each other, let us ﬁrst recall the formal statement of the No-Free-Lunch theorem.
Let X be a countable inﬁnite domain and let Y = {±1}. The No-Free-Lunch theorem implies the following: For any algorithm, A, and a training set size, m, there exist a distribution over X and a function h : X → Y, such that if A

96

Nonuniform Learnability

will get a sample of m i.i.d. training examples, labeled by h , then A is likely to return a classiﬁer with a larger error.
The consistency of Memorize implies the following: For every distribution over X and a labeling function h : X → Y, there exists a training set size m (that depends on the distribution and on h ) such that if Memorize receives at least m examples it is likely to return a classiﬁer with a small error.
We see that in the No-Free-Lunch theorem, we ﬁrst ﬁx the training set size, and then ﬁnd a distribution and a labeling function that are bad for this training set size. In contrast, in consistency guarantees, we ﬁrst ﬁx the distribution and the labeling function, and only then do we ﬁnd a training set size that suﬃces for learning this particular distribution and labeling function.

7.6 Summary

We introduced nonuniform learnability as a relaxation of PAC learnability and consistency as a relaxation of nonuniform learnability. This means that even classes of inﬁnite VC-dimension can be learnable, in some weaker sense of learnability. We discussed the usefulness of the diﬀerent deﬁnitions of learnability.

For hypothesis classes that are countable, we can apply the Minimum Descrip-

tion Length scheme, where hypotheses with shorter descriptions are preferred,

following the principle of Occam’s razor. An interesting example is the hypothe-

sis class of all predictors we can implement in C++ (or any other programming

language), which we can learn (nonuniformly) using the MDL scheme.

Arguably, the class of all predictors we can implement in C++ is a powerful

class of functions and probably contains all that we can hope to learn in prac-

tice. The ability to learn this class is impressive, and, seemingly, this chapter

should have been the last chapter of this book. This is not the case, because of

the computational aspect of learning: that is, the runtime needed to apply the

learning rule. For example, to implement the MDL paradigm with respect to

all C++ programs, we need to perform an exhaustive search over all C++ pro-

grams, which will take forever. Even the implementation of the ERM paradigm

with respect to all C++ programs of description length at most 1000 bits re-

quires an exhaustive search over 21000 hypotheses. While the sample complexity

of

learning

this

class

is

just

1000+log(2/δ)
2

,

the

runtime

is

≥

21000.

This

is

a

huge

number – much larger than the number of atoms in the visible universe. In the

next chapter we formally deﬁne the computational complexity of learning. In the

second part of this book we will study hypothesis classes for which the ERM or

SRM schemes can be implemented eﬃciently.

7.7 Bibliographic Remarks

97

7.7 Bibliographic Remarks
Our deﬁnition of nonuniform learnability is related to the deﬁnition of an Occamalgorithm in Blumer, Ehrenfeucht, Haussler & Warmuth (1987). The concept of SRM is due to (Vapnik & Chervonenkis 1974, Vapnik 1995). The concept of MDL is due to (Rissanen 1978, Rissanen 1983). The relation between SRM and MDL is discussed in Vapnik (1995). These notions are also closely related to the notion of regularization (e.g. Tikhonov (1943)). We will elaborate on regularization in the second part of this book.
The notion of consistency of estimators dates back to Fisher (1922). Our presentation of consistency follows Steinwart & Christmann (2008), who also derived several no-free-lunch theorems.

7.8 Exercises

1. Prove that for any ﬁnite class H, and any description language d : H → {0, 1}∗, the VC-dimension of H is at most 2 sup{|d(h)| : h ∈ H} – the maxi-

mum description length of a predictor in H. Furthermore, if d is a preﬁx-free

description then VCdim(H) ≤ sup{|d(h)| : h ∈ H}.

2. Let H = {hn : n ∈ N} be an inﬁnite countable hypothesis class for binary classiﬁcation. Show that it is impossible to assign weights to the hypotheses

in H such that

• H could be learnt nonuniformly using these weights. That is, the weighting

function w : H → [0, 1] should satisfy the condition h∈H w(h) ≤ 1. • The weights would be monotonically nondecreasing. That is, if i < j, then

w(hi) ≤ w(hj).

3. • Consider a hypothesis class H =

∞ n=1

Hn

,

where

for

every

n

∈

N,

Hn

is

ﬁnite. Find a weighting function w : H → [0, 1] such that h∈H w(h) ≤

1 and so that for all h ∈ H, w(h) is determined by n(h) = min{n : h ∈

Hn} and by |Hn(h)|. • (*) Deﬁne such a function w when for all n Hn is countable (possibly
inﬁnite).

4. Let H be some hypothesis class. For any h ∈ H, let |h| denote the description

length of h, according to some ﬁxed description language. Consider the MDL

learning paradigm in which the algorithm returns:

|h| + ln(2/δ)

hS ∈ arg min LS(h) +
h∈H

, 2m

where S is a sample of size m. For any B > 0, let HB = {h ∈ H : |h| ≤ B}, and deﬁne

h∗B

=

arg

min
h∈HB

LD (h).

98

Nonuniform Learnability

Prove a bound on LD(hS)−LD(hB∗ ) in terms of B, the conﬁdence parameter δ, and the size of the training set m.

• Note: Such bounds are known as oracle inequalities in the literature: We

wish to estimate how good we are compared to a reference classiﬁer (or “oracle”) hB∗ . 5. In this question we wish to show a No-Free-Lunch result for nonuniform learn-

ability: namely, that, over any inﬁnite domain, the class of all functions is not

learnable even under the relaxed nonuniform variation of learning.

Recall that an algorithm, A, nonuniformly learns a hypothesis class H if

there

exists

a

function

mNUL
H

:

(0,

1)2

×H

→

N

such

that,

for

every

, δ ∈ (0, 1)

and

for

every

h

∈

H,

if

m

≥

mNUL
H

(

, δ, h)

then

for

every

distribution

D,

with

probability of at least 1 − δ over the choice of S ∼ Dm, it holds that

LD(A(S)) ≤ LD(h) + .

If such an algorithm exists then we say that H is nonuniformly learnable. 1. Let A be a nonuniform learner for a class H. For each n ∈ N deﬁne HnA =
{h ∈ H : mNUL(0.1, 0.1, h) ≤ n}. Prove that each such class Hn has a ﬁnite VC-dimension.
2. Prove that if a class H is nonuniformly learnable then there are classes Hn so that H = n∈N Hn and, for every n ∈ N, VCdim(Hn) is ﬁnite.
3. Let H be a class that shatters an inﬁnite set. Then, for every sequence of classes (Hn : n ∈ N) such that H = n∈N Hn, there exists some n for which VCdim(Hn) = ∞. Hint: Given a class H that shatters some inﬁnite set K, and a sequence of classes (Hn : n ∈ N), each having a ﬁnite VC-dimension, start by deﬁning subsets Kn ⊆ K such that, for all n, |Kn| > VCdim(Hn) and for any n = m, Kn ∩ Km = ∅. Now, pick for each such Kn a function fn : Kn → {0, 1} so that no h ∈ Hn agrees with fn on the domain Kn. Finally, deﬁne f : X → {0, 1} by combining these fn’s and prove that f ∈ H \ n∈N Hn .
4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that is nonuniformly learnable but not PAC learnable.
5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that is not nonuniformly learnable.
6. In this question we wish to show that the algorithm Memorize is a consistent learner for every class of (binary-valued) functions over any countable domain. Let X be a countable domain and let D be a probability distribution over X .
1. Let {xi : i ∈ N} be an enumeration of the elements of X so that for all i ≤ j, D({xi}) ≤ D({xj}). Prove that

lim
n→∞

D({xi}) = 0.

i≥n

2. Given any > 0 prove that there exists D > 0 such that

D({x ∈ X : D({x}) < D}) < .

7.8 Exercises

99

3. Prove that for every η > 0, if n is such that D({xi}) < η for all i > n, then for every m ∈ N,
P [∃xi : (D({xi}) > η and xi ∈/ S)] ≤ ne−ηm.
S∼Dm
4. Conclude that if X is countable then for every probability distribution D over X there exists a function mD : (0, 1) × (0, 1) → N such that for every , δ > 0 if m > mD( , δ) then
P [D({x : x ∈/ S}) > ] < δ.
S∼Dm
5. Prove that Memorize is a consistent learner for every class of (binaryvalued) functions over any countable domain.

8 The Runtime of Learning
So far in the book we have studied the statistical perspective of learning, namely, how many samples are needed for learning. In other words, we focused on the amount of information learning requires. However, when considering automated learning, computational resources also play a major role in determining the complexity of a task: that is, how much computation is involved in carrying out a learning task. Once a suﬃcient training sample is available to the learner, there is some computation to be done to extract a hypothesis or ﬁgure out the label of a given test instance. These computational resources are crucial in any practical application of machine learning. We refer to these two types of resources as the sample complexity and the computational complexity. In this chapter, we turn our attention to the computational complexity of learning.
The computational complexity of learning should be viewed in the wider context of the computational complexity of general algorithmic tasks. This area has been extensively investigated; see, for example, (Sipser 2006). The introductory comments that follow summarize the basic ideas of that general theory that are most relevant to our discussion.
The actual runtime (in seconds) of an algorithm depends on the speciﬁc machine the algorithm is being implemented on (e.g., what the clock rate of the machine’s CPU is). To avoid dependence on the speciﬁc machine, it is common to analyze the runtime of algorithms in an asymptotic sense. For example, we say that the computational complexity of the merge-sort algorithm, which sorts a list of n items, is O(n log(n)). This implies that we can implement the algorithm on any machine that satisﬁes the requirements of some accepted abstract model of computation, and the actual runtime in seconds will satisfy the following: there exist constants c and n0, which can depend on the actual machine, such that, for any value of n > n0, the runtime in seconds of sorting any n items will be at most c n log(n). It is common to use the term feasible or eﬃciently computable for tasks that can be performed by an algorithm whose running time is O(p(n)) for some polynomial function p. One should note that this type of analysis depends on deﬁning what is the input size n of any instance to which the algorithm is expected to be applied. For “purely algorithmic” tasks, as discussed in the common computational complexity literature, this input size is clearly deﬁned; the algorithm gets an input instance, say, a list to be sorted, or an arithmetic operation to be calculated, which has a well deﬁned size (say, the
Understanding Machine Learning, c 2014 by Shai Shalev-Shwartz and Shai Ben-David Published 2014 by Cambridge University Press. Personal use only. Not for distribution. Do not post. Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning

