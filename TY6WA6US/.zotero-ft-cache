R a dRi caadli cCaolnCs tornuscttriuvcits mi v i s m
SubsLyesaternminFgobrmy aEtxipoenrDiernivceinngbvyeDrsouusblLeeCaronnitninggbeynRcyegBisetrenrdinPgoOrrli&viePraLo.loGeDoirPgreoodni

the social system analysis but is still required so that agents receive predictable sensor signals that are not disrupted by multiple collisions.
« 82 »  As mentioned before, ICO learning is used to generate the steering of the left and right motors by using the anticipatory and reflex inputs:

ICOavoid, L (t)= wavoid, reflex, L · uavoid, reflex, L (t) + wavoid, pred, L (t) · uavoid, pred, L (t) + wself, L (t) · ICOavoid, self, L (t – 1) + wR2 L (t) · ICOavoid, R2 L (t)
ICOavoid, R (t)= wavoid, reflex, R · uavoid, reflex, R (t) + wavoid, pred, R (t) · uavoid, pred, R (t) + wself, R (t) · ICOavoid, self, R (t – 1) + wL2R (t) · ICOavoid, L2R (t)

(25) (26)

with wavoid, reflex, L = −0.9, wavoid, reflex, R = −1.0 so that hitting an obstacle causes a retraction reaction. The slight differences in the weight cause the robot to turn slightly when hitting an object dead on. Here, the ICO neurons have recurrent synaptic connections and a push pull mechanism between left and right motor neuron as wR2L = −0.42, wL2R = −0.3, wself, R = 0.4, wself, L = 0.4 to implement a hysteresis effect. This effect causes the controller not to follow signals with a slight delay, as shown in Wischman Pasemann & Hülse (2004) and Hülse & Pasemann (2002). It means reactions on an incoming signal are time shifted. This is useful to enable agents to escape from corners: if there were no hysteresis mechanism, an agent would get stuck in a corner forever, turning left and right alternately.

« 83 »  Learning generates anticipatory actions by using the information from the long range sensors xavoid, pred, L∕R and updating their corresponding weights in such a way that the agent steers away from a wall before it crashes into it. This is achieved by ICO learning, which updates the weights wavoid, pred, L∕R:

dωavoid,pred,L dt

=

μ $ uavoid,pred,L $

duavoid,reflex,L dt

dωavoid,pred,R dt

=

μ $ uavoid,pred,R

$

du avoid, reflex, R dt

Received: 5 October 2012 Accepted: 16 January 2014

Open Peer Commentaries
on Bernd Porr & Paolo Di Prodi’s “Subsystem Formation Driven by Double Contingency”

Learning by Experiencing

broader conceptual framework than cy- agents by the relation between their input

versus Learning by Registering bernetic control theory for studying the data and their environment. As illustrated 211 double contingency problem, and may by Bernd Porr and Paolo Di Prodi’s imple-

Olivier L. Georgeon
Université de Lyon, France olivier.georgeon/at/liris.cnrs.fr

yield more progress in constructivist agent design.
« 1 »  Ernst von Glasersfeld differentiat-

mentation, in cybernetic theory, the agent’s input (called perturbation) does not hold an “iconic correspondence” with the environment but rather consists of feedback

ed radical constructivism from realist epis- from the agent’s output (called action). In
> Upshot • Agents that learn from per- temology by the relation between knowl- contrast, as we shall develop below, most

turbations of closed control loops are edge and reality:

machine-learning algorithms implement

considered constructivist by virtue of

this iconic correspondence because they

“  the fact that their input (the perturba- Whereas in the traditional view of epistemol- implement and exploit the agent’s input as
tion) does not convey ontological infor- ogy, as well as of cognitive psychology, that rela- if it directly characterized the environment,

mation about the environment. That is, tion is always seen as a more or less picture-like thus representing a direct access to the onto-

they learn by actively experiencing their (iconic) correspondence or match, radical con- logical essence of reality.

environment through interaction, as op- structivism sees it as an adaptation in the func-

« 2 »  Here, we call learning by experi-

” posed to learning by registering directly tional sense. (Glasersfeld 1984: 20)
input data characterizing the environ-

encing those learning mechanisms that implement and exploit input data as feedback

ment. Generalizing this idea, the notion This suggests differentiating constructiv- from the agent’s output, and learning by

of learning by experiencing provides a ist artificial agents from realist artificial registering those learning mechanisms that

http://www.univie.ac.at/constructivism/journal/9/2/199.porr

implement and exploit input data as a direct action, thus involving a form of active per- & Gay 2013). It is true that our agent’s set

observation of the environment (either a ception (e.g., McCallum 1996). However, of possibilities of experience (the relational

simulated environment or the real world, in the observation still reflects the state of the domain defined by the coupling between the

the case of robots). This formulation com- environment, as if the environment was ob- agent and the environment, e.g., Froese &

plies, for example, with Etienne Roesch et served through a filter that varied with the Ziemke 2009) is discrete, but this does not

al.’s formulation that constructivist episte- action.

prevent the agent from learning interesting

mology considers knowledge as resulting

« 6 »  In contrast, mechanisms of learn- behaviors in continuous space.

from experience of interaction with the en- ing by experiencing implement the agent’s

« 9 »  Since learning-by-experiencing

vironment, as opposed to existing “in an on- input such that it conveys information about (LbE) agents do not directly access the state

tic reality […] available to registration from the effect of an “experiment” performed by of the environment, they incorporate no

the physical world” (Roesch et al. 2013: 26). the agent. In the case of a single input bit, reward function or heuristics defined as a

« 3 »  Partially Observable Markov Deci- this bit indicates one out of two possible out- function of the state of the environment.

sion Process models (POMDP; Kaelbling, comes of the experiment. The same particu- This places LbE agents in sharp contrast

Computational Synthesis in Radical Constructivism

Littman & Cassandra 1998) well exemplify lar state of the environment induces differ- with reinforcement-learning agents and

learning by registering because they typical- ent input bits depending on the experiment problem solving agents. Notably, LbE agents

ly formalize the agent’s input as a function initiated by the agent. No partitioning of the even differ from reinforcement-learning

of the environment’s state only. A similar set of states S can be made according to the agents with an intrinsic reward (e.g., Singh,

argumentation can show that many other input bit because all states may induce input Barto & Chentanez 2005), which consider

machine-learning approaches learn by reg- “0” or “1,” depending on the experiment. In some elements of the state of the world to be

istering, even supposedly constructivist ap- this case, the learning algorithm must not internal to the agent. As a generality, an LbE

proaches based upon schema mechanisms exploit the agent’s input as if it statistically agent gives value to the mere fact of enact-

(e.g., Drescher 1991)1 and many approaches and partially corresponded to the state of ing interactive behaviors rather than to the

based upon multi-agent systems, such as reality, because it does not. In contrast with state resulting from behaviors. We expect

Roesch et al.’s (2013) agents, as we discussed learning by registering, there exist single- LbE agents to demonstrate that they learn to

in our open peer commentary (Georgeon & input-bit learning-by-experiencing agents “master the laws of sensorimotor contingen-

Hassas 2013).

that exhibit interesting learning behaviors cies” (O’Regan & Noë 2001). Consequently,

« 4 »  For the sake of argument, consider (e.g., Georgeon & Hassas 2013; Georgeon & as some authors in the domain of intrinsic

a POMDP in which the agent’s input (called Marshall 2013).

motivation also argued (e.g., Oudeyer, Ka-

observation) is reduced to a single bit. A

« 7 »  Besides cybernetic control theory, plan & Hafner 2007), we recommend assess-

subset S0 of the set S of all the environment’s in §68, Porr and Di Prodi mention other ing LbE agent’s learning through behavioral

states are observed as “0”, and the states in examples of learning by experiencing: Rich- analysis rather than through a measure of

the complementary subset S1 are observed ard Sutton et al.’s (2011) Horde architecture, their performance in reaching specific goals.

as “1”. Because of stochastic noise, some ele- and our work. Horde relies on a swarm of

« 10 »  In accordance with our view on

ments of S0 may occasionally be observed as reinforcement-learning agents to learn hier- LbE agent assessment, Porr and Di Prodi

“1” and the other way around. Yet the obser- archical temporal regularities of interaction assess their agent’s learning through behav-

vation statistically reflects the state of the en- through experience. More broadly, learning ioral analysis (Section 4). Their agents are

212 vironment, and the agent’s policy generally by experiencing implements a form of con- motivated to interact with entities present in

exploits this assumption to try to construct ceptual inversion of the perception-action the environment by controlling sensorimo-

an internal model of the agent’s situation. To cycle recommended by some authors (e.g., tor loops (approaching food or other agents,

our knowledge, there is no POMDP imple- Pfeifer & Scheier 1994; Tani & Nolfi 1999). §18). For each sensorimotor loop, Porr and

mentation that would exhibit interesting be- In learning by experiencing, however, call- Di Prodi define Prediction Utilization as a

haviors with as little observation as a single ing the input a perception or an observation measure of the agent’s commitment to con-

bit when the number of states is great. This is misleading because the input does not trol this loop. We wish to support their effort

limitation is known as the perceptual alias- hold a direct correspondence with reality. in specifying this kind of measure. This ef-

ing problem (Whitehead & Ballard 1991),

« 8 »  Concerning our approach, we fort contributes to defining general quanti-

and is inherent to learning by registering. shall clarify that it does not only “act in dis- fiers that could be used with other learning-

« 5 »  Note that some variations of POM- crete space,” as Porr and Di Prodi wrote in by-experiencing approaches to characterize

DPs have been proposed in which the scope §68. Instead, our agents are indifferent to the agent’s engagement in interactive behav-

of the observation depends on the previous the structure of their environment’s space, iors.

which is precisely an advantage of learning

« 11 »  As Porr & Di Prodi noted in §68,

1 |  Gary Drescher (1991) modelled Piagetian by experiencing. We demonstrated that our simple linear control theory does not real-

schemes as triplets – <pre-observation, action, algorithms could control agents in continu- ize “the generation of more complex actions,

post-observation>. The argument that Drescher’s ous two-dimensional simulated environ- the switching of actions and the sequencing

agent’s observation reflects the environment’s ments (Georgeon & Sakellariou 2012) and of actions.” However, other learning by ex-

state is similar to our argument about POMDPs. robots in the real world (Georgeon, Wolf periencing approaches tackle these issues.

Constructivist Foundations vol. 9, N°2

RadiRcaldiCcoanl sCtoruncstrivuicstmivism
LearnAinlignbiyngExHpoemrieeonsctiantgicvaenrsduHseLteearronsitnagticbyPReersgpisetcetriivnegs OPlaivtireicrkL.MG.ePoirlagresokni

Addressing the double contingency problem domain are crucial points that impact how rather driven by intrinsic motivations that

with approaches that generate such learning we interpret the comparisons made in the are defined by desired states.” (§68) After

would allow more sophisticated subsystem paper and the ways its insights may be ap- acknowledging the need and potential for

organization because each subsystem could plied to work in other domains. I use Porr more complex actions and action sequenc-

control more sophisticated interactions than and Di Prodi’s problem setting and agent es, as potentially provided by techniques

a linear control loop. Therefore, we antici- formulation as a starting point for assessing from reinforcement learning, the text of §68

pate that addressing the problem of subsys- some of the key statements made in their continues by stating that the extrinsically

tem formation driven by double contingen- work, and build toward a specific look at defined reward used in standard reinforce-

cy within the general framework of learning prediction utilization as presented by the au- ment learning “usually means that the life of

by experiencing would allow more advances thors. This assessment is supplemented with the animal is just directed toward this single

in constructivist agent design.

comparisons to related work from the recent moment in time but will not code an ongo-

computational and biological literature.

ing intrinsic motivation.”1 This sequence of

Olivier L. Georgeon is currently an associate researcher at the LIRIS Lab, with a fellowship from

Heterostasis and homeostasis

text sets up a natural contrast between extrinsic and intrinsic reward – motivation or

the French Government (ANR-RPDOC program). He

« 2 »  Porr and Di Prodi’s setting of satisfaction derived from the world or from

received a Masters in computer engineering from agents interacting in a reflexive and predic- within the agent, respectively. At the same

Ecole Centrale de Marseille in 1988, and a PhD in tive manner via continuous inputs and out- time, it reinforces a distinction between het-

psychology from the Université de Lyon in 2008. puts is a natural one, albeit one that is often erostatic and homeostatic optimization by

ignored in favour of the perceived clarity an intelligent system.

Received: 19 February 2014 and mathematical benefits of discrete sensa-

« 4 »  Intrinsic motivation is held to be

Accepted: 19 February 2014 tion and action spaces. Their specific setting a powerful way to drive exploration and

is in fact a problem domain that resonates potentially accelerate the learning of predic-

well with other robot-related constructivist tions, control behavior, and better represen-

Aligning Homeostatic and

demonstrations from the machine learning tations (Schmidhuber 1991; Oudeyer, Kapliterature – e.g., learned multi-robot food lan & Hafner 2007). However, much like the

Heterostatic Perspectives

foraging behaviour (Matarić 1997), robot actual boundary between an agent and its learning applications as surveyed by Grond- environment is often less of a boundary and

Patrick M. Pilarski
University of Alberta, Canada pilarski/at/ualberta.ca

man et al. (2012), and robot knowledge acquisition as per Modayil, White & Sutton (2014) and Sutton et al. (2011, as cited by the authors). It is important to note, how-

more an opinion on the part of the system designer (or examiner), boundaries between what are considered intrinsic and extrinsic reward have been placed at different points

ever, that many of these like-minded ex- by different authors. Is the distinction be-
> Upshot • There is merit to the continu- plorations are rooted in a rather different tween these types of feedback actually use-

ous-signal-space homeostatic viewpoint starting point: that of the learning system ful to our discussion of the present paper, or

on subsystem formation presented by or systems attempting to maximize some does it further cloud the understanding of

Bernd Porr and Paolo Di Prodi; many of aspect of its experience – in other words, how Porr and Di Prodi’s agents react to per-

their ideas also align well with a het- an agent seeking to increase its long-term turbations in their sensorimotor streams?

213

erostatic constructivist perspective, and expected reward or learning progress, as in

« 5 »  One high-level view we could be

specifically developments in the field of the intertwined fields of computational and inclined to take based on the statements

reinforcement learning. This commen- biological reinforcement learning (Sutton & made in §68 is that an intrinsic approach

tary therefore aims to identify and clarify Barto 1998). This maximization, or hetero- to motivation allows ongoing, life-long

some of the linkages made by the au- static goal-seeking behaviour (after Harry learning without the need for endpoints or

thors, and highlight ways in which these Klopf ’s The Hedonistic Neuron, 1982) is at imposed valuations of an agent’s stream of

interdisciplinary connections may be lev- first glance in contrast with an agent’s “task experience (e.g., transient or final goals).

eraged to enable future progress.

of restoring its desired state to homeostasis,” However, it is interesting to refer again to

as posed by the authors (§3). However, for the aforementioned text in §68 indicating

« 1 »  Learning to perceive, predict, and our current discussion, it may be beneficial

act based only on continuous-valued sensorimotor inputs and outputs is a challenging and important pursuit that deserves our

to explore the similarities between these viewpoints in terms of the authors’ work, as opposed to the differences.

1 |  This statement seems to assume a terminal or discrete reward, and passes over the way that standard reinforcement learning often uti-

focused attention. While subsystem forma-

« 3 »  Let us first examine the statements lizes temporally extended expectations of future

tion and evaluation are the principal listed made in the authors’ concluding remarks, reward (e.g., discounted future return; Sutton &

contributions of Bernd Porr and Paolo Di suggesting that the homeostatic linear con- Barto 1998) or average reward (discussed below).

Prodi’s target article, the nature of the sig- trol approach in the paper “establishes an However, a detailed discussion of all these points

nals and predictions in the paper’s problem ongoing process that has no final goal but is is best left outside the present commentary.

http://www.univie.ac.at/constructivism/journal/9/2/199.porr

