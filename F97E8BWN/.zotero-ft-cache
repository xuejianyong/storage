Successor representation (SR) and replay
cyril regan May, 2021

1 Introduction
This objective of this report is to sum up the mathematical assumptions of the Successor Representation (Dayan, 1993; Gershman, 2018) with the formalism of Reinforcement Learning, as introduced in (Sutton-Barto, 1998).

1.1 MDP
A reinforcement learning task that satisﬁes the Markov property is called a Markov Decision Process, or MDP. A particular ﬁnite MDP is deﬁned by its state and action sets and by the one-step dynamics of the environment. Hence, the probability of transitioning to st+1 = s , given the previous ‘history’, is fully captured by conditioning only on the current state st = s and action at = a.
In other words, the future is independent of the past, given the present. This is described by the transition function:

T (s |s, a) = Psas = P[st+1 = s |st = s, at = a]

(1)

Similarly, given any current state and action, s and a, together with any next state s , the expected value of the next reward is :

Rass = E[rt+1|st = s, at = a, st+1 = s ]

(2)

This characterization of the reward dynamics of an MDP in terms of Rass is slightly unusual. It is more common in the MDP literature to describe the
reward dynamics in terms of the expected next reward given just the current
state and action, i.e., by

rt+1 = Ras = r(s, a) = E[rt+1|st = s, at = a] = Psas Rass

(3)

s

The 4-tuple (et = st, at, rt, st+1) is called a target memory, a transition, or an experience (et ).

1

1.1.1 Policy
At any given state, the agent undergoes a decision-making process in order to select an action. The policy is a function that maps the state of an agent to an action. This can either be deterministic :

a = π(s)

(4)

or stochastic :

π(a|s) = P[at = a|st = s]

(5)

1.1.2 Value functions
Value functions on policy : Reinforcement learning is concerned with the estimation of the state-value function V (s) or action-value function Q(s, a), the total reward an agent expects to earn in the future, with short-term rewards weighed more highly than long-term rewards. Of course the rewards the agent can expect to receive in the future depends on what actions it will take. Accordingly, value functions are deﬁned with respect to particular policies. The state-value function is deﬁned as the expected discounted future return following the policy π (Sutton-Barto, 1998):

V π(s) = Eπ[rt + γrt+1 + γ2rt+2 + ...|st = s]

+∞
= Eπ[ γkrt+k|st = s]

(6)

k=0

where γ is a discount factor that captures a preference for proximal rewards. Similarly the action-value function is deﬁned by :

+∞

Qπ(s, a) = Eπ[ γkrt+k|st = s, at = a]

(7)

k=0

State-value function can be written recursively with Bellman equation :

+∞

V π(s) = Eπ

γkrt+k|st = s

k=0

+∞
= Eπ[rt+1 + γ γkrt+k+1|st = s]

k=0

+∞

= π(s, a) rt+1 + Psas γ Eπ

γkrt+k+1|st+1 = s ]

(8)

a

s

k=0

= π(s, a) rt+1 + γ Psas V π(s )

a

s

= π(s, a) r(a, s) + γ Psas V π(s )

a

s

2

Value functions on optimal policy : A policy π is deﬁned to be better
than or equal to a policy π if its expected return is greater than or equal to that of π for all states. In other words, π ≥ π if and only if V π(s) ≥ V π (s)
for all state s ∈ S All the optimal policies by denoted by π∗ share the same state-value function
V ∗.

V ∗(s) = max V π(s) ∀s ∈ S

(9)

π

The optimal policies share also the same action-value function Q∗ :

Q∗(s, a) = max Qπ(s) ∀s ∈ S and a ∈ A(s)

(10)

π

Thus, we can write Q∗ in terms of V ∗ as follows:

Q∗(s, a) = E[rt+1 + γV ∗(st+1)|st = s, at = a]

(11)

Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state:

V ∗(s) = max Q∗(s, a)
a

(12)

= max r(a, s) + γ
a

Psas V ∗(s )

s

Q∗(a, s) = r(a, s) + γ

Psas

max Q∗(s
a

,a )

(13)

s

1.2 Model-based algorithms
Model-based decision algorithms are explicitly based on the underlying “model” (i.e., the reward function r and the transition function Psas ), which is either given a priori or learned. As the model is known, an estimate of the value functions can be updated by :

Vˆ ← r(s) + γ

max
a

Psas

Vˆ

(s

)

(14)

s

Qˆ(s, a) ← r(s, a) + γ

Psas

max Qˆ(s , a ),
a

(15)

s

Where r(s) = max r(s, a). Figure 1 illustrates the model-based schema.
a
Drawback : this architecture is computationally expensive, because value estimation are computed by planning (i.e. propagating the known rewards in the graph generated by the transition function) and are used to decide an action in each state.

3

Advantages : Compared to Model-free algorithms (described below) ModelBased algorithm uses a reduced number of interactions with the real environment during the learning phase. Moreover, the agent endowed with a model requires only a small amount of experience to adapt to changes in rewards localisation or in environment structure.

Figure 1: Model-based schema

1.3 Model-free algorithms
Model-free algorithms directly estimate the value function V from experienced transitions and rewards, without explicitly using or learning a model. In a deterministic setting, the estimated value functions Vˆ or Qˆ can be represented by a lookup table storing the estimates for each state (resp state-action) while physically interacting with the environment, through temporal diﬀerence (TD) learning.

Vˆ (s) ← r(s) + γVˆ (s ),

(16)

Qˆ(s, a) ← r(s, a) + γ max Qˆ(s , a ),

(17)

a

Where s corresponds to the successor state following the policy π of the Modelfree algorithms.
The TD error δ is :

δ = r(s) + γVˆ (s ) − Vˆ (s)

(18)

δ = r(s, a) + γ max Qˆ(s , a ) − Qˆ(s, a)

(19)

a

And the update of the value function is done by :

Vˆ new(s) = Vˆ old(s) + α.δ,

(20)

Qˆnew(s, a) = Qˆold(s, a) + α.δ,

(21)

4

where α is the learning rate. Figure 2 illustrates the model-free schema. Drawback : inﬂexibility because a local change in the transition or reward
functions will produce non-local changes in the value function. Model-free are computationally expensive to train.
Advantages: Trained, a model-free model is computationally eﬃcient to maximize the cumulative reward.
Figure 2: Model-free schema
1.4 Dyna
In order to accommodate for the aforementioned duality of both model-free and model-based strategies in the brain, the Dyna architecture (Sutton, 1990) has been used as a RL paradigm to encapsulate both learning systems. It builds upon traditional model-free techniques where the agent learns directly from real online interactions with the environment. In addition to this, the Dyna framework involves the agent learning a model of the world and allows for experiences to be simulated (similar to hippocampal replay) in order to perform model-based ‘planning’ steps while the agent is oﬄine.
However, the traditional Dyna architecture proposes an algorithm involved n oﬄine planning steps after each transition and selecting previously experienced target state-action pairs uniformly at random. This prioritization technique which does not place emphasis is suboptimal, because lots of these replayed experiences contain no value information.
The model based planning can be a construction of the successor representation instead to learn the transition state function. The issue is the prioritization of experiences to replay while avoiding temporal correlated experiences (correlated data) for the learning of the agent. Figure 3 illustrates the model-based schema.
5

Figure 3: Dyna Model schema

2 The successor representation

The successor representation M represent the ”state occupancy”. In fact, one

can also see the successor representation as a ”timeless” state representation.

Indeed, the M matrix is a kind of map representing the occupancy of the actual

state s and the projection of all future state occupancy (discounting by the γ

factor) which could depend also of the action a taken at the actual state s. The

construction of the successor representation M with the transition state matrix

P

a ss

is meaningful :

M (s , s, a) = 1s,s + γPsas + γ2(P2)ass + γ3(P3)ass + ...

(22)

Here, M (s , s, a) represents the occupancy of state s knowing the agent is presently in the state s and takes the action a. Let’s take a look of the ﬁrst term of the equation :

1s,s = 1 if s = s

(23)

1s,s = 0 if s = s

1s,s represents trivially the state occupancy of the ”present time” which is null for all the states diﬀerent from the actual.

Then the state occupancy of the ﬁrst upcoming time is the probability to be

in the state s knowing the agent is in the state s and takes the action a. It is by

deﬁnition the probability expressed by the state transition matrix discounting

by the γ factor :

γ

P

a ss

(24)

But we can go on and project the state occupancy of the next upcoming time :

γ2(P 2)ass

(25)

To sum up, the successor representation is a cumulative probability to be in a state from the actual and all future projected times. To say it diﬀerently,

6

M is deﬁned as the discounted occupancy of state s, averaged over all possible

trajectories initiated in state s. The SR can intuitively be thought of as a

predictive map that encodes each state in terms of the other states that will be

visited in the future.

Of course, the future state occupancy is only a ”projection” of the present

knowledge

of

the

world

expressed

by

the

transition

matrix

P

a ss

.

As

the

manip-

ulation of a sum is not easy in a computational exploitation, one can express

the sum with a straightforward limited development :

1

M (s

, s, a)

=

1

−

γP

a ss

(26)

or in a matrix way :

M (a) = (1 − γPa)−1

(27)

The successor representation is also popular because of its nice property which represents any value function as :

V (s) = M (s, s )r(s )

(28)

s

Q(s, a) = M (s, s , a)r(s , a)

(29)

s

Moreover,

it

is

not

necessarily

to

calculate

directly

the

transition

matrix

P

a ss

to get the successor representation M . An evaluation Mˆ can be learned via

TD-learning for example with the δt error :

δt(s ) = I[st = s ] + γMˆ (st+1, s ) − Mˆ (st, s )

(30)

Notice that unlike the temporal diﬀerence error for value learning, the temporal diﬀerence error for SR learning is vector-valued, with one error for each successor state.
Advantage on ﬂexibility : changes in the reward function will immediately propagate to all the action-value function. Drawback : this is not true of changes in the transition function

3 Replay
A simple way to improve algorithms is to introduce experience replay (Lin, 1992). It consists of storing experiences ek = (sk, ak, rk, sk) while interacting with the environment, and replaying them oﬀ-line to accelerate learning. This was demonstrated in the Deep Q-Network (DQN) algorithm (Mnih, 2013), (Mnih, 2015), and was improved by the use of prioritized experience replay (Schaul, 2016).
Here, we introduce two another type of replay which are simulated by a model based algorithm (like hippocampal replay). They are model-based experience replay.

7

The ﬁrst model presented is based on the Expected Value of a Bellman backup (EVB) (Mattar-Daw, 2018), which considers which experience a rational agent ought to replay. The prioritized rule is computed as the product of a gain and a need term on all possible experiences. The former considers how much the agent could gain by replaying each event, whereas the latter measures how much it need be replayed given its immediate relevance.
The second is a model-based bidirectional search (Khamassi-Girard, 2020) where prioritized sweeping and trajectory sampling are used sequentially. Prioritized sweeping ranks experiences by how ”surprising” they were. Trajectory sampling favors the update of states that have high probability of being visited. It is an heuristic method for prioritized replay, but has the advantage to not compute all possible experiences for each planning step.

3.1 The Expected Value of a Bellman backup (EVB)
The algorithm in (Mattar-Daw, 2018) is based on Dyna architecture (section 1.4) and prioritized Bellman backup. A Bellman backup is the Temporal Diﬀerence learning of the action-value Q as eq. (21) :

Q(sk, ak) ← Q(sk, ak) + α

rk

+

γ

max
a

Q(sk ,

a)

−

Q(sk ,

ak )

(31)

Bellman backups are performed automatically after each transition in real experience and may also be performed nonlocally during simulated experience. More speciﬁcally, the authors suggest that the order of memories to be replayed (in oﬄine learning) should be based upon their ”expected value of a Bellman backup” (EVB) :

EV B(sk, ak) = Gain(sk, ak) × N eed(sk)

(32)

EVB is the improvement in expected return due to a policy change, and can be calculate by the product of the gain accrued each time the agent visits state sk (Gain term) and the expected number of times sk will be visited (Need term). However, EVB is computed on all possible experiences for each planning oﬄine step which is biologically unfeasible, see algorithm 2. The oﬄine learning is imposed arbitrary on the start and the end of each episode with a limited ﬁxed number of planning steps.

3.1.1 Gain term
The gain term quantiﬁes the improvement in expected discounted future rewards as a result of a policy change at the target state-action pair. It is deﬁned as:

Gain(sk, ak) = Qπnew (sk, a)πnew(a|sk) − Qπnew (sk, a)πold(a|sk) (33)

a

a

where πnew(a|sk) represents the probability of selecting action a in state sk after the Bellman backup, and πold(a|sk) is the same quantity before the Bellman

8

backup. From the state s, the agent selects the next action from a with a ﬁxed policy : the probability distribution over actions in state s computed from the Q-values with a softmax function:

eβQ(s,a) π(a|s) = i eβQ(s,i)

(34)

where β is called the inverse temperature which regulates the exploration/exploitation trade-oﬀ.
Gain(sk, ak) is resistant to updates in Q values unless they also result in a policy change. The term is computed in a manner that is reminiscent of prioritized sweeping, although in this case, positive and negative reward prediction errors have asymmetric roles depending on whether there is more or less reward availability in the rest of the environment.

3.1.2 Need term

The need term quantiﬁes the discounted number of times that an agent expects to visit states in the future, given a target state:

∞

N eed(sk) =

γ i .1St+i,sk

i

(35)

It is estimated through the use of the successor representation, which is simply the discounted geometric sum of the transition matrix calculated by eq. (27). Ultimately, the need term prioritizes adjacent states compared to those further into the future, as a result of temporal discounting. The need term is thus obtained directly from the nth row of the SR matrix. Additionally, after learning a policy, the need term maps out probable trajectories that the agent will take, as it focuses on states that are likely to be visited in the future.

3.1.3 Identiﬁcation
Authors classify each individual backup as forward or reverse by examining the next backup in the sequence. When a backed-up action is followed by a backup in that action’s resulting state, it is classiﬁed as forward. In contrast, when the state of a backup corresponds to the outcome of the following backed-up action, it is classiﬁed as reverse.

3.1.4 EVB Algorithm Algorithms 1 and 2 are presented below :

9

Algorithm 1 EVB

1: procedure EVB

— Initialisation —

2: planExp ← [, ]

3: for each (s, a) ∈ S × A do Loop over all state-action of the environment

4:

Get r (reward),s (next state) from the environment

5:

Put (s, a, r, s ) in planExp

planExp is ﬁlled by all the 4-tuples experiences (sk, ak, rk, sk) with

k ∈ range(|S| × |A|)

6: Construct the transition matrix P

7: M = (1 − γP)−1

Compute the Successor Representation Matrix

8: nep = 100

number of episodes

9: pplan = 20

number of planning oﬄine steps

10: while n <= nep do

— Planning at the start of episode —

11:

Q ← EV Bplanning(Q, planExp)

— Online Qlearning —

12:

repeat

13:

at ← argmaxasof tmaxβ(Q(st, a))

policy softmax (Q)

14:

Take action at receive (st+1, rt+1, done)

15:

P[st, st+1] ← P[st, st+1] + α(1 − P[st, st+1]) Update transition

matrix

16:

Q(st, at) ← Q(st, at) + α [rt + γ maxa Q(st, a ) − Q(st, at)]

Online Belman Backup on Qvalue

17:

until done == T rue

— Planning at the end of episode —

18:

Q ← EV Bplanning(Q, planExp)

19:

n←n+1

10

Algorithm 2 EVB planning

procedure EV Bplanning

2: while p <= pplan do

for each (sk, ak, rk, sk) ∈ planExp do

4:

for a ∈ A do

Qold(sk, a) = Q(sk, a)

6:

πold(a|sk) = sof tmax(Qold(sk, a))

Get action probability

from Qold(sk, a)

Qnew(sk, a)

←

Qold(sk, a)

+

α [rk + γ maxa Qold(sk, a ) − Qold(sk, a)] Calcul Qnew with Bellman backup

on Qold(sk, a)

8:

πnew(a|sk) = sof tmax(Qnew(sk, a))

Get action probability

from Qnew(sk, a) Gain(sk, a) = Qnew(sk, a).πnew(a|sk) − Qnew(sk, a).πold(a|sk)

Compute the Gain term

10:

M = (1 − γP)−1

Compute (update) the SR matrix

N eed(sk) = M (sk)

Compute the Need term

12:

EV B(sk, ak) = Gain(sk, a).N eed(sk)

Compute EV B term

km ← argmaxkEV B(sk, ak)

14:

(s, a, r, s ) ← planExp(km)

Get the experience with the max EVB

Q(s, a) ← Q(s, a) + α [r + γ maxa Q(s , a ) − Q(s, a)]

Belman

backup on prioritized replay

16:

p←p+1

11

3.2 Bidirectionnal model-based approach
In (Caz´e-Khamassi, 2018) and (Khamassi-Girard, 2020), the authors postulated a framework for prioritized replay with a heuristic-based approach. Prioritized sweeping (PS) proposes to visit the states starting from those whose Q-value has changed recently, and then to their predecessors, the predecessors of their predecessors, and so on. Trajectory sampling proposes to generate continuous trajectories from the current position until reward is reached, with the idea that it will favor the update of relevant states (avoiding wasting resources on states that have a very low probability of being visited).

3.2.1 Prioritized Sweeping (PS)

Prioritized sweeping (PS) consists of weighing experiences based on how surprising they are. PS also has a tendency to mediate reverse or backward replay. Speciﬁcally, this is measured by the absolute value of the TD error signal

δ = |Qn+1(s, a) − Qn(s, a)|,

(36)

with :

Q(s, a) ← r(s, a) + γ

Psas

max Q(s , a ),
a

(37)

s

described in eq. (15). if δ is higher than a certain threshold, all possible predecessors of the current
state are added to a priority queue P Queue (that will be used during the PS), with a priority equal to δ, attenuated by γ and by the probability to eﬀectively reach state s from the predecessor under consideration.

3.2.2 Trajectory Sampling (TS)
The second component is trajectory sampling (TS) from the current location (i.e., the current online estimated position of the agent), which involves accessing memories sequentially along a trajectory of successor states, and drives forward replay. These method in (Caz´e-Khamassi, 2018) allowed the agent to autonomously decide when, where and for how long it should stop to perform replay within the environment.

3.2.3 Bidirectionnal model-based Algorithm
The online part of the algorithm 3 is quite classical. Given a state s, the softmax function policy choose the action a. Then, given the observed reward r and the new state s , the transition matrix is updated and then the Q − valueQ(s, a) in a model-based manner.
The world model in (Khamassi-Girard, 2020) uses a very basic statistical approach for the transition function T which consists in simply counting how many times (s, a) was followed by s normalized by the total number of times (s, a) was encountered. Besides, updating the reward function R in the world

12

model is here based on the latest feedback only. After this step, we measure how much the absolute value of the Q-value has been changed according to:

δ = |Qt+1(s, a) − Qt(s, a)|

(38)

if δ is higher than a certain threshold, all possible predecessors of the current state are added to a priority queue P Queue = δ attenuated by γ and the probability to eﬀectively reach state s from the predecessor under consideration.
After each action performed by the agent in the environment, an oﬄine reactivation phase starts. It consists on a series of PS followed by series of TS. The model-based mental traveling is stopped when forward and backward search process have reached a connection state. The schematic representation of the operation of the bidirectional inference algorithm is presented in Figure 4 from (Khamassi-Girard, 2020).

13

Figure 4: Prioritized sweeping and trajectory sampling phases alternate until an inference
stop criterion is reached (Q-value convergence, exhaustion of a general budget, etc.). The mouse represents the agent’s position, and R the reward location. Each PS phase stops either when budget is exhausted or when no element in the priority queue has a priority above a ﬁxed threshold. Each TS phase stops either when budget is exhausted or when the trajectory reaches a state stored in the priority queue. Picture from (Khamassi-Girard, 2020) available at https://doi.org/10.6084/m9.ﬁgshare.8306132
14

Algorithm 3 bidirectional algorithm

procedure Online(s0, Q) nbActions ← 0

initial state, Q-values

3: st ← s0 P Queue ← {}

PQueue: empty priority queue

T ←0

T : crude transition statistics storage

6: R ← 0

R: reward function

repeat

at ← draw(sof tmaxβ(Q(st))

policy softmax depending on Qvalue

9:

nbActions ← nbActions + 1

Take action at receive st+1, rt+1 T (st, at, st+1) ← T (st, at, st+1) + 1 pseudo transition matrix update in

counting the (st, at, st+1) occurancy as deterministic way

12:

R(st, at) ← rt+1

Qold ← Q(st, at)

Qold ← Q(st, at+1) in the paper : error ?

Q(st, at) ← R(st, at) + γ

s

T (st,at,s ) i T (st,at,i)

maxk

Q(s

,

k)

1-step

MB update the Qvalue with the pseudo transition matrix T . Q(st, at+1) is updated

in the paper : error ?

15:

δ = |Q(st, at) − Qold| TD error to choose to add the experience in the

P Queue δ = |Q(st−1, at+1) − Qold| is written in the paper : error ? for each (s, a) so that T (s, a, st) = 0 do

iterate on all predecessor

[state-action] couple which get to the state st

p←δ×γ×

T (s,a,st) T (s,a,i)

compute the priority p = error discounted

by gamma and the probability of transition.

18:

if p > ν then

if (s, a) ∈/ P Queue then

Put (s, a) in P Queue with priority p

21:

else

Update priority of (s, a) in P Queue with p

st ← st+1

24:

Q ← bidirectionalInf erence(Q, P Queue, st)

after every online step

until nbActions = nbActionsM ax

oﬄine phase computed

Algorithm 4 Bidirectional planning

procedure bidirectionalInference(Q, P Queue, st) current state in

the real world,n Q-values, priority queue

nbLoops ← 0

repeat

repeat (PS/ TS) until TD error convergence or budget max

4:

Sumδ ← 0

nbLoops ← nbLoops + 1

— Start of Prioritized Sweeping (PS) —

nbP S ← 0 while priority(P Queue[0]) > ν and nbP S < nbP Smax do

P Queue[0] = highest prioritized tuple (s, a) / nbP Smax = budget of PS

8:

nbP S ← nbP S + 1

(s, a) ← P Queue[0]

(s,a) are the ones with the higher priority

Qold ← Q(s, a) Q(s, a) ← R(s, a) + γ
error in the paper ?

s

T (s,a,s ) i T (s,a,i)

maxk

Q(s

,

k)

”maxk Q(s, k)” :

12:

δ ← |Q(s, a) − Qold|

Sumδ ← Sumδ + δ

for each (s , a ) so that T (s , a , s) = 0 do (s , a ) predecessors

update priority of (s, a) )

p←δ×γ×

T (s,a,st) T (s,a,i)

16:

if p > ν then

if (s , a ) ∈/ P Queue then

Put (s , a ) in P Queue with priority p

else

20:

Update priority of (s , a ) in P Queue with p

— Start of Trajectory Sampling (TS) —

nbT S ← 0

s ← st

start from the current location in the real world

while s ∈/ P Queue and nbP S < nbP Smax do loop until a state of

PQueue is reached or until budget expended

24:

nbT S ← nbT S + 1

a ← draw(sof tmaxβR (Q(s)))

”a” taken by the policy : maybe

diﬀerent from ”at” because Q has change with PS

s ← draw(probabilityP roportionateSelection(T (s, a)))

s

is the NEXT step. And it can’t be the return of the environment because we are

oﬄine. So s is a estimation by the transitional state function. R(s, a) is also the

reward estimation.

Qold ← Q(s, a)

28:

Q(s, a) ← R(s, a) + γ

error in the paper ?

s

T (s,a,s ) i T (s,a,i)

maxk

Q(s

,

k)

”maxk Q(s, k)” :

Sumδ ← Sumδ + |Q(s, a) − Qold|

add the error to the

cumulative sum of error

s←s

jump to the next state ! and go on !

until Sumδ < or nbLoops > nbLoopsM ax

no signiﬁcant Q-value

update in the last loop or global budget exhausted return Q

updated Q-values

4 Linear Successor Features Model (LSFM)
State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. Indeed, it may become computationally very diﬃcult to compute decision-making strategies that maximize rewards for high dimensional inputs. This “curse of dimensionality” (Bellman, 1961) can be overcome by compressing high dimensional sensor data into a lower dimensional latent state space. Furthemore, mapping high-dimensional inputs into a lower dimensional latent state space can lead to faster learning because information can be reused across diﬀerent inputs.
Successor features are the generalization of Successor Representation on latent space predicting frequencies of future observations. Previous work presents algorithms that reuse Successor Features (SFs) (Barreto and Dabney, 2017) to initialize learning across tasks with diﬀerent reward speciﬁcations, leading to improvements in learning speed in challenging control tasks (Barreto and Borsa, 2019); (Zhang, 2016); (Gershman, 2016). Nevertheless, the latent space learned by these Successor Features cannot generalize across diﬀerent transition functions.
Here, we introduce the Linear Successor Feature Model (LSFM) (Lehnert, 2020). LSFMs construct latent state space that extract equivalences of a task’s transition dynamics and rewards, and aﬀord transfer across tasks with diﬀerent rewards but also diﬀerent transition dynamics (unlike previous SFs). The extension from (Lehnert, 2020) of this present work is to learn a neural networks mapping inputs to latent feature spaces that are predictive of future reward outcomes. It is presented in section 4.5.

4.1 State Representations

4.1.1 Value-predictive state representation

A latent state space Sφ is constructed using a state representation function φ : S → Sφ. A value-predictive state representation constructs a latent state space that retains enough information to support accurate value or Q-value predictions. Authors in (Barreto and Dabney, 2017) use directly SFs as state representation, the SFs formulation is value-predictive :

Qπ(s, a) = ψπ(s, a).w

(39)

Because value-predictive state representations are only required to be predictive of a particular policy π, they implicitly depend on the policy π. Consequently, this formulation of SFs learns latent state space that is predictive of the optimal decision-making strategy and are thus akin to model-free RL.

4.1.2 Reward-Predictive State Representations
A reward-predictive state representation constructs a latent state space that retains enough information about the original state space to support accurate

predictions of future expected reward outcomes. Which means for any start state and any arbitrary action sequence, both the latent grid world and the original grid world produce equal reward sequences. This property can be formalized using a family of functions {ft}t∈N that predicts the expected reward outcome after executing an action sequence a1, ..., at starting at state s:

ft : (s, a1, ..., at) = Ep[rt|s, a1, ..., at]

(40)

A state representation is reward-predictive if the function ft can be reparameterized in terms of the constructed latent state space and if there exists a family of functions {gt}t∈N such that

ft : (s, a1, ..., at) = gt(φ(s), a1, ..., at)

(41)

4.2 Successor Features (SFs)
SFs combine Successor Representation with arbitrary state representations. Given a state representation φ, the SF is a column vector deﬁned for each state and action pair (Gershman, 2016) and

∞

ψπ(s, a) = Ep,π

γt−1φst s1 = s, a1 = a ,

t=1

(42)

A SF vector ψπ can be understood as a statistic measuring how frequently diﬀerent latent states vectors are encountered following the policy π.
Suppose a state representation φ constructs a latent state space with empirical latent transition probabilities that match the transition probabilities of the original task, this state representation is reward predictive. A reward-predictive state representation can be used in conjunction with a Linear Action Model (Sutton, 2008) to compute expected future reward outcomes.

Deﬁnition 1 — Linear Action Model (LAM) : Given an MDP and a state representation φ : S → Rn a LAM consists of a set of matrices and vectors {M a, wa}a∈A, where M a is of size n × n and the column vector wa is of dimension n.

The transition matrices of a LAM {M a}a∈A model the empirical latent transition probabilities. The vectors wa model a linear map from latent states to expected one-step reward outcomes. The expected reward outcome after follow-
ing the action sequence a1, ..., at starting at state s can then be approximated with

Ep[rt|s, a1, ..., at] ≈ φs M a1...M at−1.wat

(43)

To tie SFs to reward-predictive state representations, we ﬁrst introduce a set of square real-valued matrice {F a}a∈A such that, for every state s and action a,

φs F a ≈ ψπ(s, a),

(44)

where π is deﬁned on the latent state space. A Linear Successor Feature Model (LSFM) is then deﬁned using the matrices {F a}a∈A :
Deﬁnition 2 — Linear Successor Feature Model (LSFM) : Given an MDP, a policy π, and a state representation φ : S → Rn, an LSFM consists of a set of matrices and vectors {F a}a∈A, where F a is of size n × n and the column vector wa is of dimension n. The {F a}a∈A are used to model a linear map from latent state features to SFs as described in eq. (44).
In (Barreto and Dabney, 2017), the latent space is constructed by setting the output of φ to be one-step rewards : r(s, a, s ) = φ(s, a, s ).w. In constrast, LSFMs are used to learn the state-representation function φ that satisﬁes eq. (44) suitable for predicting reward sequences for any arbitrary decision sequence. Thus, LSFM reward-predictive state representations can be understood as form of model-based RL. With LSFM, an agent has to search policies that are deﬁned on its latent state space. These policies are called abstract policies.
Deﬁnition 3 — Abstract Policies : An abstract policy πφ is a function mapping latent state and action pairs to probability values:

∀s ∈ S, a ∈ A, πφ(φs, a) ∈ [0, 1] and πφ(φs, a) = 1

(45)

a

For a ﬁxed state representation φ, the set of all abstract policies is denoted with Πφ.

4.3 LSFM
4.3.1 Bisimulation
Bisimulation Relations express the fact that the empirical latent transition probabilities have to match the transition probabilities in the original task. In (Lehnert, 2020), the author proves that LAMs or LSFMs encode state representations that generalize only across bisimilar states. This aﬃrmation is based on the assumption that the state representation φ : S → {e1, ..., en} is assumed to have a range that consists of all n one-hot bit vectors φ(s) = ei.
Theorem 1 — LSFM one-hot latent states : For an MDP S, A, p, r, γ be a state representation and {F a, wa}a∈A an LSFM. If, for one policy π ∈ Πφ, the representation φ satisﬁes
∀s ∈ S, ∀a ∈ A, φs .wa = Ep[r(s, a, s )|s, a] and φs F a = φs +γEp,π[φs F a |s, a], (46)
then φ generalizes across bisimilar states and any two states s and s˜ are bisimilar if φs = φs˜. If eq. (46) holds for one policy π ∈ Πφ, then eq. (46) also holds every other policy π˜ ∈ Πφ as well.

But the assumption of one-hot bit vectors can be relaxed by a state representation which only approximately satisﬁes the conditions outlined in theorem 1. Moreover, an (approximate) reward-predictive state representation (approximately) generalizes across all abstract policies, because the same state representation can be used to predict the value of every possible abstract policy π ∈ Πφ if prediction errors are low enough.

4.3.2 Learning
To learn the (approximate) reward-predictive state representation with LSFM, the loss objective LLSF M is the sum of three diﬀerent terms: The ﬁrst term Lr computes the one-step reward prediction error and is designed to minimize the reward error r. The second term Lψ computes the SF prediction error and is designed to minimize the SF error ψ. The last term LN is a regularizer constraining the gradient optimizer to ﬁnd a state representation that outputs unit norm vectors.
Given a ﬁnite data set of transitions D = (si, ai, ri, si)Di=1, the formal loss objective is

D
LLSF M =
i=1

2

D

φsi wai − ri +αψ

i=1

2

D

φsi F a − ysi,ai,ri,si

+ αN
2

i=1

2

2

φsi

−1
2

=Lr

=Lψ

=LN
(47)

and αN , αψ > 0 are hyper-parameters. In eq. (47), the prediction target is

ys,a,r,s = φs + γφs F

(48)

with F the LSFM that predicts the SF for a policy that selects actions uni-

formly at random :

1

F= |A|

Fa

(49)

a∈A

Because the matrix F averages across all actions, the LSFM computes the SFs for a policy that selects actions uniformly at random. But the matrix F could be constructed diﬀerently because the proofs of the theoretical results assume that F can only depend on the matrices {F a}a∈A and is not a function of the state s.
For each gradient update, the target ys,a,r,s can be considered a constant (Lehnert, 2020).

4.3.3 Results from (Lehnert, 2020)
Figure 5 presents the puddle-world experiments and the results. To analyze across which states the learned reward-predictive state representation generalizes, all feature vectors were clustered using agglomerative clustering. Two different states that are associated if the distance of there feature vectors φs−φs˜ 2

Figure 5: Puddle-World Experiment from (Lehnert, 2020). 6(a): Map of the puddle-world task in which the agent can move up, down, left, or right to transition to adjacent grid cells. The agent always starts at the blue start cell and once the green reward cell is reached a reward of +1 is given and the interaction sequence is terminated. For each transition that enters the orange puddle, a reward of -1 is received. 6(b), 6(c): Partitioning obtained by merging latent states into clusters by Euclidean distance. 6(d), 6(e), 6(f): Expected reward and predictions for a randomly chosen 200-step action sequence using a randomly chosen representation, a representation learned with a LAM, and a representation learned with an LSFM.
is low. Figure 5 (b) and (c) plot the obtained clustering as a partition map. Only a transition data set D was given as input to the optimization algorithm and the algorithm was not informed about the grid-world topology of the task in any other way.
Figure 5 (d), (e), (f) plot an expected reward rollout and the predictions presented by a random initialization (Figure 5 (d)), the learned representation using a LAM (Figure 5 (e)), and the learned representation using a LSFM (Figure 5 (f)). The blue curve plots the expected reward outcome Ep[rt|s, a1, ..., at] as a function of t for a randomly selected action sequence. While a randomly initialized state representation produces poor predictions of future reward outcomes (Figure 5 (d)), the learned representations produce relatively accurate predictions and follow the expected reward curve (Figure 5 (e) and (f)). Because the optimization process was forced to compress 100 grid cells into a 80 dimensional latent state space, the latent state space cannot preserve the exact grid cell position and thus approximation errors occur.
The plots in Figure 5 suggest that both LSFMs and LAMs can be used to learn approximate reward-predictive state representations. Because both models optimize diﬀerent non-linear and non-convex loss functions, the optimization

process leads to diﬀerent local optima, leading to diﬀerent performance on the puddle-world task. While prediction errors are present, Figure 5 suggests that both LSFM and LAM learn an approximate reward-predictive state representation and empirically the diﬀerences between each model are not signiﬁcant.
4.4 Discussion on LSFM
The schematic in Figure 6 illustrates the diﬀerences between the previous and the presented models. LSFM ties successor features to model-based reinforce-
Figure 6: Comparison of SFs State-Representation Models
ment learning, because an agent that has learned an LSFM can predict expected future reward outcomes for any arbitrary action sequence. LSFMs are a “strict” model-based architecture and are distinct from model-based and model-free hybrid architectures that iteratively search for an optimal policy and adjust their internal representation.
LSFMs only evaluate SFs for a ﬁxed target policy that selects actions uniformly at random. The learned model can then be used to predict the value function of any arbitrary policy, including the optimal policy.
In contrast to the SF framework introduced by (Barreto and Dabney, 2017), the connection between LSFMs and model-based RL is possible because the same state representation φ is used to predict its own SF (Figure 6 ). While the deep learning models presented by (Gershman, 2016) and (Zhang, 2016) use one state representation to predict SFs and one-step rewards, these models are also constrained to predict image frames. LSFMs do not use the state representation to reconstruct actual states. Instead, the state space is explicitly compressed, allowing the agent to generalize across distinct states. LSFMs do not use the state representation to reconstruct actual states. Instead, the state space is

explicitly compressed, allowing the agent to generalize across distinct states. Reward-predictive state representations remove previous limitations of SFs
and generalize across variations in transition functions. This property stands in contrast to previous work on SFs, which demonstrate robustness against changes in the reward function only ((Barreto and Dabney, 2017), (Barreto and Borsa, 2019), (Gershman, 2016), (Zhang, 2016)).
In all cases, including reward-predictive state representations, the learned models can only generalize to changes that approximately preserve the latent state space structure. If the same representation is used on a completely diﬀerent randomly generated ﬁnite MDP, then positive transfer may not be possible because both tasks do not have a latent state structure in common.
The combination of LSFM with eﬃcient exploration algorithms is left to future studies.
4.5 Linear Successor Feature Neural Network (LSFNN)
To exploit the potentiality of LSFM in bigger state space, we construct here a Linear Successor Feature Neural Network (LSFNN). But in a ﬁrst step, we test the LSFNN in simple environment of navigation tasks.
The ﬁrst experiment is made on a small 6 × 6 puddle word described in Figure 7a. The purpose is to compare the cumulative rewards of a linear Q model trained with a reward predictive latent space with a Deep Q Learning model.
The second experiment involve two bigger environments. The ﬁrst one is a bigger puddle word of 11 × 11. A reward predictive latent space and a value predictive latent space are constructed on this ﬁrst environment. These two latent space are then reused to train a linear Q-learning on the second four-rooms environment, described in ﬁg. 10b which diﬀers on rewards and transition.
4.5.1 Small puddle word
The environment is a small puddle word on a 6 × 6 grid presented in Figure 7a. The environment initial state is ﬁxed as the goal state. The Black states return rewards of −1, grey states return rewards of 0.
The architecture of LSFNN presented in Figure 7b contain 30 hidden layers and constructs a 30 dimensional latent state space. The linear reward prediction layer wa such as the output is rˆ(φs, a) = φs wa, and the linear matrix {F a} such as the output is ψˆ(φs, a) = φs F a are dense layers with no bias.
LSFNN is trained on a ﬁxed target policy that selects actions uniformly at random. The loss is exactly calculated following eq. (47). After trained on 200 episodes, the LSFNN losses converge, as presented in the Figure 8.
As the selection of actions is uniformly random, the agent reach the goal on 150 steps in average, so the whole simulation takes 30000 steps. The loss on ψ is the slower because αψ = 0.01 and αN = 1. These coeﬃcients were obtained by ﬁne tuning with αψ ∈ [0.01, 0.1, 1] and αN ∈ [0.01, 0.1, 1].

(a) Simple puddle word 6 × 6

(b) LSFNN Architecture

Figure 7: (a) Simple 6 × 6 puddle word with ﬁx red initial state and green goal state . Black states return rewards of −1, grey states return rewards of 0. The green goal states return a +1 reward. (b) LSFNN architecture with a 30 latent space dimension. The Linear reward prediction layer {wa} and the linear {F a} matrix are dense layer with no bias.

Figure 8: Losses on LSFNN
The learned latent space φs constructed by LSFNN can be used to predict the valuefunction of any arbitrary policy, including the optimal policy. We trained a Linear Q-learning Neural Network (QNN) on the latent space φs and compared with a Linear QNN and a Deep Q-learning (DQN) with 30 hidden layers on the original state space.
The experiment is repeated 10 times for each of the 3 conﬁgurations trained on 200 episodes. The policy is an espilon-greedy with a exponential decrease of epsilon with the number of steps : = min + ( max − min) ∗ exp (−λsteps), with min = 0.01, max = 1, and λ = 0.0005. The results in cumulative gain is presented on the Figure 9.
The drop of cumulative results on the ﬁrst episodes mean the models gather lots of negative rewards in the beginning before starting to learn to avoid them.

Figure 9: Q learnings on latent and original state space (linear and Deep)
The results show clearly the best results on the linear Q learning based on latent space trained with LSFNN.
4.5.2 LSFNN vs DQN latent spaces on two environments
The goal of this section is to conﬁrm experimentally the potentiality of LFSNN to transfer tasks which diﬀer in rewards and transitions.
We ﬁrst construct a reward predictive latent space with LFSNN on a puddle word of 11×11 as described in Figure 10a. The policy and the model parameters to train LSFNN are the same as described in Section 4.5.1.
A value predictive latent space is constructed with a DQN with 30 hidden layers on the original state space. The only change in comparison with the DQN training in section 4.5.1, is the decrease of λ = 0.0001 for the exponential decrease of epsilon greedy.

(a) Big puddle word 11 × 11

(b) 11 × 11 Four rooms environment

Figure 10: (a) Big 11 × 11 puddle word with ﬁx red initial state and green goal state. Black states return rewards of −1, grey states return rewards of 0. The green goal states return a +1 reward. (b) 11 × 11 Four rooms environment with ﬁx red initial state and green goal state. Blue tiles are barriers where the agent can not go through and grey states return rewards of 0.

These two latent space are then reused to train a linear Q-learning on a second four-rooms environment described in Figure 10b which diﬀers on rewards and transition.

Figure 11: Cumulative steps of a linear Q-learning model, trained with latent space constructed on Big puddle word by LSFNN and DQN model.
We compare in Figure 11 the cumulative steps of a linear Q-learning model, trained with latent space constructed on Big puddle word by LSFNN and DQN model. It is clear that latent space constructed by the reward predictive LSFNN

model on the Big puddle word leads to way better results for the learning on the 4-rooms environment, than the value predictive DQN latent space.
We conﬁrm experimentally the capacity of the reward predicitve LSFNN model to transfer representations across diﬀerent tasks which diﬀer in both the rewards and transitions.
4.5.3 Future work
The training time of LSFNN is very long with a uniform random policy. A idea of future work is to improve the training time of LSFNN by ﬁnding a better policy than the selection of actions uniformly at random.
A promising way is the option model presented by (Machado, 2018.).

References

A. Barreto and D. Borsa. Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement. 2019.

A. Barreto and W. Dabney. Successor Features for Transfer in Reinforcement Learning. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA., 2017.

Caz´e-Khamassi. Hippocampal replays under the scrutiny of reinforcement learning models. J Neurophysiol, 2018.

P. Dayan. Improving Generalization for Temporal Diﬀerence Learning: The Successor Representation. Computational Neurobiology Laboratory, The Salk Institute, P.0. Box 85800, Sun Diego, CA 92186-5800 USA, 1993.

S. J. Gershman. The Successor Representation: Its Computational Logic and Neural Substrates. The Journal of Neuroscience, 2018. ISBN 9781417642595.

T. D. K. S. J. Gershman. Deep Successor Reinforcement Learning. ArXiv vol abs/1606.02396, 2016.

Khamassi-Girard. Modeling awake hippocampal reactivations with model-based bidirectional search. Biological Cybernetics (Modeling), Springer Verlag, 2020, 10.1007/s00422-020- 00817-x. hal-02504897, 2020.

L. Lehnert. Successor Features Combine Elements of Model-Free and Modelbased Reinforcement Learning. 2020.

Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3):293–321, 1992, 1992.

Machado. EIGENOPTION DISCOVERY THROUGH THE DEEP SUCCESSOR REPRESENTATION. conference paper at ICLR 2018, 2018.

Mattar-Daw. Prioritized memory access explains planning and hippocampal replay. Nature Neuroscience, 2018.

Mnih.

Playing atari with deep reinforcement learning.

2016arXiv:1312.5602, 2013.Mnih, 2013.

ICLR

Mnih. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015, 2015.

Schaul. Prioritized experience replay. ICLR, 2016.

Sutton. Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming. nips, 1990.

R. S. Sutton. Dyna-style planning with linear function approximation and prioritized sweeping. 24th Conference on Uncertainty in Artiﬁcial Intelligence, 2008.

Sutton-Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998 A Bradford Book, 1998.
J. Zhang. Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments. 2016.

