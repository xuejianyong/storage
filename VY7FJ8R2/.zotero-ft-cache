See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/328213149
Investigating Enactive Learning for Autonomous Intelligent Agents
Preprint · October 2018

CITATIONS
0
1 author: Rafik Hadfi Monash University (Australia) 31 PUBLICATIONS 47 CITATIONS
SEE PROFILE

READS
81

Some of the authors of this publication are also working on these related projects: Automated Negotiating Agents View project Developing Enactive Artificial Agents View project

All content following this page was uploaded by Rafik Hadfi on 14 October 2018.
The user has requested enhancement of the downloaded file.

Investigating Enactive Learning for Autonomous Intelligent Agents
Raﬁk Hadﬁ School of Psychological Sciences,
Monash University, Australia raﬁk.hadﬁ@monash.edu

arXiv:1810.04535v1 [cs.LG] 9 Oct 2018

Abstract—The enactive approach to cognition is typically proposed as a viable alternative to traditional cognitive science. Enactive cognition displaces the explanatory focus from the internal representations of the agent to the direct sensorimotor interaction with its environment. In this paper, we investigate enactive learning through means of artiﬁcial agent simulations. We compare the performances of the enactive agent to an agent operating on classical reinforcement learning in foraging tasks within maze environments. The characteristics of the agents are analysed in terms of the accessibility of the environmental states, goals, and exploration/exploitation tradeoffs. We conﬁrm that the enactive agent can successfully interact with its environment and learn to avoid unfavourable interactions using intrinsically deﬁned goals. The performance of the enactive agent is shown to be limited by the number of affordable actions.
Index Terms—Enactive Learning, Reinforcement Learning, Intrinsic Motivation, Self-motivation, Exploration and Exploitation, Curiosity, Artiﬁcial Intelligence, Simulation
I. INTRODUCTION
The enactive paradigm has originally emerged from embodied cognitive science and particularly from the early work of Maturana and Varela [1], [2]. According to this paradigm, a living organism depends constitutively on its living body and places sensorimotor interactions at the centre of cognition [3]. Cognition becomes therefore aligned with the organisational principles of living organisms while giving a major role to the phenomenology of experience. The paradigm challenges the separation between the internal constituents of a system and its external conditions, and emphasises the interaction between the two. It also perceives an organism as an autonomous and active entity that is able to adaptively maintain itself in its environment [4]. For this type of interactions to happen enactivism posits that the agent must be a part of reality [5], [6].
The paradigm has inﬂuenced a large number of embodied cognition theorists [7], [8] and has contributed to the emergence of a variety of research programs such as evolutionary [9] and epigenetic [10] robotics. In the area of Artiﬁcial Intelligence (AI), it is becoming more and more accepted as a viable alternative to the computationalist approaches in building artiﬁcial agents that can behave in a ﬂexible and robust manner under dynamic conditions [11]. Nevertheless, some concerns have been raised with regard to the sufﬁciency of the current enactive AI for advancing our understanding of artiﬁcial agency and providing accurate models of cognition [12], [13]. The aim of this paper is to provide some initial steps towards

the development of such an understanding. We think that enactive cognitive science can provide the conceptual tools that are needed to diagnose more clearly the limitations of current enactive AI, particularly at a time where Reinforcement Learning (RL) is by far the dominant paradigm [14]–[16]. The development of an enactive AI would provide fuller models of Enactive Learning (EL) and would challenge the RL methodologies that fail at many real-world problems.
There were few attempts to operationalise enactive cognition in the context of autonomous agents and agent learning. For instance, the authors in [17] use the enactive principles to model biological agents. Such agents try to perform rewarding interactions with their environment instead of trying to reach rewarding states as it is the case with RL. In another work, [18] formalise the enactive types of interactions between the agent and its environment using an enactive redeﬁnition of Markov Decision Processes. The framework describes a viable architecture that could be used in designing enactive agents but does not evaluate the paradigm in the face of environmental complexity nor it compares it to RL. Departing from the same theoretical framework, we propose to scrutinise enactive learning by building an artiﬁcial agent that could act based on enactive principles. We compare such agent to a classical RL agent within complex and volatile maze environments. We then analyse and discuss the different behavioural characteristics of both agents in light of limited access to the environmental states, goals, and exploration/exploitation tradeoffs. We show that enactive the agent can successfully learn to interact with its environment and exploit regularities of sensorimotor interactions. Particularly, it learns to avoid unfavourable actions using intrinsically deﬁned goals.
The paper is structured as following. In section 2, we provide the theoretic foundation of enactive cognition. In section 3, we give the formal agent models as well as the process of enactive learning. In section 4, we provide the methodology and the experimental setting. In section 5, we provide the results and discussion. Finally, we conclude and highlight the future work.
II. ENACTIVE COGNITION
Enactive cognition is fundamentally compliant with the constructivist school of thought, which perceives learning as creating meaning from experience [19]. On the contrary of cognitive psychologists who think of the mind as a reference tool to the real world, constructivists view the mind as ﬁltering

input from the world to produce its own unique reality [20]. The key concepts behind the enactive paradigm is that the agent must discover and learn to exploit the regularities in its interactions with the environment [18]. Regularities are patterns of sensorimotor interactions that occur consistently and depend on the active interactions between the agent and its environment.
To better understand how an enactive agent operates, let us contrast it with the type of agent that are often used in RL and rely heavily on internal representations of their environment. Such agent (namely RL agent), passively interprets input data as if it represented the environment. For instance, we could take the situation of a robot exploring a real-world environment. The position of the robot reﬂects its (spatial) state as seen by an external observer, and its internal state as if the robot is keeping track of its own position. The state of the environment is also available to the robot and would for instance account for the percepts that are available within its visual ﬁeld. Moving around implies a change in the perceptual ﬁeld and therefore the state of the environment. Such assumptions do not hold for the enactive agent which is not a passive observer of reality but constructs its perception of the environment through the active experience of interaction [21], [22]. In this case, the states of the environment are not directly available and the agent is actively involved in shaping its perceptions of those states. We illustrate the distinctions between the two types of agents in ﬁgure 1. We particularly look at how interactions are initiated between the agent and the environment as a succession of decision cycles.

is noted as post( it−1, it ). The set of composite interactions known by the agent at time t is deﬁned as Kt and the set Jt = I ∪ Kt is the set of all interactions known to the agent at time t. When enacted, the primitive interaction it activates previously learned composite interactions as it matches their pre-interaction. For example, if it = a and if the composite interaction a, b has been learned before time t, then the composite interaction a, b is activated, which means that it is recalled from memory. Activated composite interactions propose their post-interaction’s experiment, in this case: b’s experiment. If the sequence a, b corresponds to a regularity of interaction, then it is probable that the sequence a, b can be enacted again. Therefore, the agent can anticipate that performing b’s experiment will likely produce b’s result. The agent can thus base its choice of the next experiment on this anticipation.
III. AGENT LEARNING MODELS
In the following, we provide the details of the enactive and reinforcement learning models.
A. Reinforcement Learning
Reinforcement Learning was inspired by behaviourist psychology as it uses feedback (reinforcement) to modify behaviours in the desired direction [19]. In practice, an agent is built as to take actions in an environment while maximising some cumulative reward. This is usually formalised using a Markov Decision Process (MDP) within a fully described environment. The MDP is generally represented as a tuple S, A, R, T where S and A are the state and action spaces deﬁned on the environment. The function R : S × A × S → R is a reward function that determines how much an agent will be rewarded by taking a given action in a given state. The agent has partial control on the outcomes in its model, which is described by the transition probability function (1),

(a) Reinforcement Learning

(b) Enactive Learning

Fig. 1. Interactions between the agent and the environment in reinforcement and enactive learnings.

In ﬁgure 1(a), the interaction cycle starts with an observation ot and ends with an action at on the environment. In ﬁgure 1(b), the cycle starts with the agent performing an experiment et and ends by the agent receiving the result rt of the experiment.
Let us now look at the cycles of ﬁgure 1(b). At the beginning of each decision cycle t of the enactive interactions, the agent
decides on an intended sensorimotor interaction to try to enact
with reference to the reactive part played by the environment. That is, the agent enacts an interaction it = et, rt at time t, with it being an element of the set of primitive interactions I. Enacting it means experimenting et and receiving a result rt. Then, the agent records the two-step sequence it−1, it made by the previously enacted interaction it−1 of it. The sequence of interactions it−1, it is called a composite interaction. The interaction it−1 is called it−1, it ’s pre-interaction, noted as pre( it−1, it ), and it is called it−1, it ’s post-interaction and

T (s, a, s ) = P(st+1 = s |st = s, at = a)

(1)

with s, s ∈ S and a ∈ A. The goal of the agent is to ﬁnd a
policy π : S → A that maximises the discounted summation
of rewards. When the utility of each state converges, we get the optimal policy π∗. The optimal policy is found by iterating using the value function V π described in (2),

V π(s) = T (s, π(s), s ) R(s, π(s), s ) + γV π(s ) (2)
s ∈S
with γ being a discounting factor in [0, 1]. The optimal behaviour of the agent is to select an action at each state according to π∗. An optimal behaviour would be a sequence of actions yielding a sequence of occupied states.
In the following, we choose to restrict certain features of RL in order to allow for fair comparison with the enactive learning. For S = R2, we limit the number of states that are available to the agent by deﬁning its scope as distance δ ∈ R. The new state space becomes Sδ = {s ∈ R2 : s − s ≤ δ} with state s being the location of the agent. We also parametrise the exploratory and exploitative behaviours of the agent by a

parameter α ∈ [0, 1]. α is the probability of using a purely exploratory strategy (random walk) while 1−α is the probability of following the optimal policy π∗. B. Enactive Learning
1) Enactive MDP: The main distinction between RL and the EL resides in the nature of the interactions between the agent and its environment. Such interactions are based on the unique element of actions and results. We can model such interactions using an Enactive Markov Decision Process (EMDP) as shown in ﬁgure 2. An EMDP [17] is deﬁned as a tuple S, I, q, p with S being the set of environment states; I the set of primitive interactions offered by the coupling between the agent and the environment; q a probability distribution such that q(st+1|st, it) gives the probability that the environment transitions to state st+1 ∈ S when the agent chooses interaction it ∈ I in state st ∈ S; and p is a probability distribution such that p(et|st, it) gives the probability that the agent receives input et ∈ I after choosing it in state st.
Fig. 2. Enactive Markov Decision Process

set At of activated interactions deﬁned as At = {a ∈ Kt|pre(a) ∈ Ct}. 3) Proposition: The activated interactions in At propose their post-interaction for enaction, forming the set Pt of proposed interactions deﬁned as Pt = {p ∈ Jt|∃a ∈ At, p = post(a)}. 4) Selection: The intended interaction it is selected from the proposed interactions in Pt based on the proclivity of the interactions. The proclivity of an interaction it is deﬁned as proclivity(it) = r(it) × P(it) and reﬂects the regularity of the interaction based on its probability of occurrence and the motivations of the agent. The selection of the intended interaction is also subject to parameters α ∈ [0, 1] and d ∈ R. α is the probability that it is selected randomly for exploratory purposes, and 1 − α is the probability that it is selected based on proclivity. The parameter d encodes the limited foresight of the agent and speciﬁes how deep it can go in the hierarchy of interactions. In other words, d is the length of the allowable sequences of interactions. 5) Enaction: The agent tries to enact the intended interaction it, which could (or not) result in an enacted interaction et. 6) Learning: New composite interactions are constructed or reinforced with their pre-interaction belonging to the context Ct and their post interaction being et, forming the set of learned or reinforced interactions Lt to be included in Kt+1. The set Lt is deﬁned as Lt = { pre(i), et }. 7) Construction: A new context Ct+1 is constructed to include the stabilized interactions in et and post(et): Ct+1 ← Lηt ∪ {et} ∪ {post(et)}.
In the following section, we propose to test the enactive and reinforced mechanisms within artiﬁcial agents.

In the EMDP, it is called the intended interaction as it represents the sensorimotor scheme that the agent intends to enact at the beginning of step t; and constitutes the agent’s output that is sent to the environment. We call et the enacted interaction because it represents the sensorimotor scheme that the agent records as actually enacted at the end of step t; et constitutes the agent’s input received from the environment. If the enacted interaction equals the intended interaction (et = it) then the attempted enaction of it is considered a success, otherwise, it is considered a failure.
2) The Learning Process: The mechanism underlying the EMDP could be implemented as a sequential learning process that relies on the interactions between the agent and its environment [17]. The enactive learning process operates at every decision step t according to the following 7 phases.
1) Preparation: The agent is initially presented with a set of interactions Ct ⊂ Jt referred to as the context, with C0 = ∅ and J0 = I.
2) Activation: The agent takes the previously learned composite interactions whose pre-interaction belongs to the current context and activates them, forming the

IV. METHODOLOGY AND EXPERIMENTAL SCENARIO To evaluate the two paradigms we set up an experimental scenario in which two agents are expected to perform a foraging task in a 2D maze environment as illustrated in ﬁgure 3. Each agent is tested on its own and both agents start from the same position.
Fig. 3. Maze with one agent and 18 food units
A. The environment The environment is a 8 × 10 maze and is deﬁned in terms of
its structure and behaviour. The structure of the maze reﬂects

the difﬁculty of the problem based on the existence of obstacles and how they limit the access to the food units. The obstacles and their distribution are important when we evaluate the exploratory behaviours of the agents. The dynamic aspects of the environment reside in uniformly adding 20 food units every 200 ticks for an overall period of 1000 ticks per trial.

B. The agents

An agent is an entity that moves in a 2D space according to a

predeﬁned list of actions: moving forward by one step, turning right by 90◦, turning left by 90◦. The enactive agents possess few additional actions that correspond to failures of interactions. For instance, the action “step” has another action “step fail” that corresponds to a failed “step” action and happens when the agent is attempting to move forward despite the existence of an obstacle. In the following simulations, we consider two

types of agents.

1) An enactive agent that uses EL and interacts with the environment according to an EMDP.

2) A RL agent that interacts with the observable environment according to an MDP. The agent uses a Q-Learning mechanism by updating the reward states based on any dynamically added food unit.

The rewards of the RL agent are deﬁned as in (3).

 +5 

if st = food cell

r(st) = 0.04 if st = empty cell

(3)

 −9 if st = obstacle cell

Since the enactive agent does not possess an extrinsically deﬁned utility function that maps states to rewards [23], [24], we need to deﬁne an intrinsic valuation of its actions.

Intrinsic motivational values are the main driver of the agent’s spontaneous exploratory behaviour [18], [25], [26]. In particular, we want the rewarding mechanism to account for the adaptive behavioural strategies of the agent in the face

of constantly changing environments. We deﬁne the agent’s intrinsic motivations using a reward function r : I → R that maps primitive interactions to motivational states (4).

 +10 

if it = step

 
r(it) =

−1 −0.3



if it = step fails if it = turn right

(4)

 

−0.3

if it = turn left

The feedback values (4) should in principle be chosen as to encode the expected behaviour that would lead to the desired goals. In RL, the goal is extrinsically speciﬁed as scalar reward values. In EL, goal-oriented behaviours are speciﬁed as intrinsic

motivational values [17]. For instance, the step action has a ﬁxed positive valence that does not change over time, which means that the agent has the same level of motivation to walk forward. Similarly, turning right and left have low negative valences, which means that the agent would often avoid turning.

V. EXPERIMENTAL RESULTS A. The Enactive Learning

Quantifying the enactive learning requires at ﬁrst an evaluation of the agents ability to avoid negative-valence enactions.

Variance of valences Vt : t Δ Δt

The valence Vt is the value that the agent assigns to an interaction it, based on its intrinsic motivational value and its probability of occurrence. The enactive learning translates to learning to choose interactions that have positive valence and avoid the interactions that have negative valence.
Figure 4 shows the reduction in negative valence counts for 15 runs of the EL agent with δ = 10 and α = 0. We started by dividing the 1000-tick period into 10 time windows of length ∆t = 100 each. Then, we calculated the means of all the standard deviations across the 15 trials of each time window. The reduction of variance reﬂects a stabilisation in
1.2
1.0
0.8
0.6
0.4
0.2
0.0
−0.2 0123456789 Time window, Δt = 100
Fig. 4. Reduction in negative valence
the negative counts, and that actions with negative valence are becoming less and less frequent as time goes on. This also means that the agent has mastered the enactions and their consequences, and in our case, learnt to avoid bumping into the wall while maintaining the combinations of actions (turning left/right) that do have negative valence but are required to explore the environment. Such balance reﬂects the trade off between exploitation and exploration in the sense that the agent learns to maximise valences.
B. Comparing the learning paradigms In the following, we compare the foraging performances
of the agents for exploratory behaviours α ∈ {0, 0.5} across foresights d ∈ [2, 20] and δ ∈ [21, 211].
Since the notion of space is not explicitly deﬁned for EL, we need to interpret how d and δ map to each other. For RL, an increment in δ corresponds to an increment in space coverage, for instance, moving from position x to position y happens with x − y ≤ δ. Limiting the choice of interactions with motivational values (4), makes the EL agent more likely to pick the “step” interaction instead of the others. The interaction “step” corresponds in reality to a change in space. Therefore, an increment in d corresponds to an increment in the space

coverage. For instance, let us take the composite interaction it, it+1, it+2 composed of successful “step” actions: et = et+1 = et+2 = “step” and rt = rt+1 = rt+2 = “Success”. This sequence causes the agent to change position from st to st+3 with st−st+3 = 3. This type of mapping illustrates how internal motivational models map to behaviours. Programming the enactive agent corresponds to ﬁnding the right motivational model that would reproduce the desired behaviours.

food units

100 80

α=0 α = 0.5

60

40

20

0

2

4

6

8

10

12

14

16

18

20

Scope (δ)

(a) Reinforcement Agent

70

α=0

α = 0.5

60

Despite the poor overall performance shown in ﬁgure 5(b), the EL agent did actually start well for δ = 2 with a performance within [50,60] compared to that of the RL agent (20), but then its performance went down with the increase of d. The drop of performance of α = 0 for high d values is mainly due to the usage of long sequences of sensorimotor interactions, which take long to enact, evaluate and learn. Complex enactions impair the agents ability to exploit the space and traps him in suboptimal areas. This is true in our case even with 4 primitive interactions and should grow intractably if we add more interactions. A possible solution to reduce this complexity is to interrupt the activation cycle of long sequences of interactions by randomly picking one primitive interaction instead. The use of such an exploratory behaviour is visible for α = 0.5 and yields more gain.
Being trapped in complex interactions does not necessarily translate to a goal-oriented behaviour for an external observer but accounts for the spontaneity and the self-motivation that drives the EL agent [27]. The goal-driven behaviour becomes an emergent property of the constructs (4). The challenge is therefore to ﬁnd the appropriate mapping from rewards (3) to motivational values (4) while balancing the exploration/exploitation tradeoff with an optimal α.
Enactive learning could fail at problems that cannot be deﬁned in terms of states either for the lack of any formal description of the state space or when the state space is too large to be encoded in a reward function. While reinforcement learning is more performant when the state space is well deﬁned, it still lacks the ability to maintain its scalability for large spaces. Enactive learning on the other hand has the ability to construct its own map of the state-space and exploit it based on its intrinsic model of behaviour. We summarise the main difference between the two learning paradigms in table I.

50

VI. CONCLUSION AND FUTURE WORK

food units

40

30

20

10

0

2

8

32

128

512

2048

Depth (d)

(b) Enactive Agent

Fig. 5. Foraging for different exploratory strategies

The performances of the agents are shown in ﬁgure 5. In ﬁgure 5(a), the RL agent exhibits a systematic, deterministic behaviour for α = 0 since it is solely governed by its policy. Adding and explorative behaviour (α = 0.5) improves the gain when the foresight is limited (scope [2-10]) but goes up for δ = 10, which means that the scope covers most of the 8 × 20 maze. When the scope covers all the space (δ 20), the agent’s gain is mainly driven by exploitation.

Enactive Learning is an interesting alternative to RL given its capability to operate without prior knowledge of the states of the environment. We developed an artiﬁcial agent that learns based on the enactive principles, and compared its behaviours and performances to an agent that runs on RL. The agents are tested in foraging tasks within complex and volatile environments. We show that the enactive agent can successfully interact with its environment, and learn to avoid unfavourable interactions using intrinsically deﬁned goals. The enactive agent is shown to be limited by the number of affordable actions that it could enact at a time. Limiting the size of its memory of interactions or relying on exploratory strategies increases its performance.
As future directions, we would like to change the way we deﬁned the intrinsic motivational values as scalar values, and instead use expectations of rewards as it is done in the predictive coding framework of [28]. Moreover, we think of investigating the structures of hierarchies of interactions and whether they could be used to build spatial representations of the environment and of the agent itself.

TABLE I COMPARING ENACTIVE AND REINFORCEMENT LEARNING

Criteria
Theoretical framework Formal model Mechanism Reward Scope Goal Representation Perception Input/Output Action cycle

Enactive Learning
Constructivism Enactive MDP (EMDP)
Sequential Learning Intrinsic, not a function of the environment (self-motivation)
Depth in hierarchies of interactions Emergent, mainly driven by self-motivation
Gradient of actions Percepts are internally constructed
Result/Experiment Starts from the agent

Reinforcement Learning
Behaviourism MDP
Value Iteration Extrinsic, function of environment
Distance on the state space Predeﬁned, using a reward function
Hierarchies of interactions Percepts are function of the environment
Observation/Action Starts from the environment

REFERENCES
[1] F. J. Valera, E. Thompson, and E. Rosch, “The embodied mind: Cognitive science and human experience,” 1991.
[2] H. R. Maturana and F. J. Varela, Autopoiesis and cognition: The realization of the living. Springer Science & Business Media, 1991, vol. 42.
[3] O. L. Georgeon and D. Aha, “The radical interactionism conceptual commitment,” Journal of Artiﬁcial General Intelligence, vol. 4, no. 2, pp. 31–36, 2013.
[4] E. Abramova, M. Slors, and I. van Rooij, “Enactive mechanistic explanation of social cognition,” in Proceedings of the 39th Annual Meeting of the Cognitive Science Society, 2017, pp. 26–29.
[5] S. Gallagher and M. Bower, “Making enactivism even more embodied,” 2013.
[6] X. E. Barandiaran, “Autonomy and enactivism: towards a theory of sensorimotor autonomous agency,” Topoi, vol. 36, no. 3, pp. 409–430, 2017.
[7] A. Clark, Being there, 1997. [8] D. D. Hutto and E. Myin, Radicalizing enactivism: Basic minds without
content, 2012. [9] I. Harvey, E. D. Paolo, R. Wood, M. Quinn, and E. Tuci, “Evolutionary
robotics: A new scientiﬁc tool for studying cognition,” Artiﬁcial life, vol. 11, no. 1-2, pp. 79–98, 2005. [10] L. Berthouze and T. Ziemke, “Epigenetic roboticsmodelling cognitive development in robotic systems,” 2003. [11] E. Di Paolo and E. Thompson, “The enactive approach,” 2014. [12] T. Froese and T. Ziemke, “Enactive artiﬁcial intelligence: Investigating the systemic organization of life and mind,” Artiﬁcial Intelligence, vol. 173, no. 3-4, pp. 466–500, 2009. [13] S. Torrance and T. Froese, “An inter-enactive approach to agency: Participatory sense-making, dynamics, and sociality,” Agency, p. 21. [14] S. J. Russell and P. Norvig, Artiﬁcial intelligence: a modern approach. Malaysia; Pearson Education Limited,, 2016. [15] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016. [16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, p. 529, 2015. [17] O. L. Georgeon, R. C. Casado, and L. A. Matignon, “Modeling biological agents beyond the reinforcement-learning paradigm,” Procedia Computer Science, vol. 71, pp. 17–22, 2015. [18] O. L. Georgeon, C. Wolf, and S. Gay, “An enactive approach to autonomous agent and robot learning,” in Development and Learning and Epigenetic Robotics (ICDL), 2013 IEEE Third Joint International Conference on. IEEE, 2013, pp. 1–6. [19] P. A. Ertmer and T. J. Newby, “Behaviorism, cognitivism, constructivism: Comparing critical features from an instructional design perspective,” Performance Improvement Quarterly, vol. 26, no. 2, pp. 43–71, 2013. [20] D. H. Jonassen, “Evaluating constructivistic learning,” Educational technology, vol. 31, no. 9, pp. 28–33, 1991. [21] O. L. Georgeon, “Little ai: Playing a constructivist robot,” SoftwareX, vol. 6, pp. 161–164, 2017.

[22] O. L. Georgeon, F. J. Bernard, and A. Cordier, “Constructing phenomenal knowledge in an unknown noumenal reality,” Procedia Computer Science, vol. 71, pp. 11–16, 2015.
[23] R. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,” Artiﬁcial intelligence, vol. 112, no. 1-2, pp. 181–211, 1999.
[24] A. G. Barto, S. Singh, and N. Chentanez, “Intrinsically motivated learning of hierarchical collections of skills.” Citeseer.
[25] P.-Y. Oudeyer and F. Kaplan, “What is intrinsic motivation? a typology of computational approaches,” Frontiers in neurorobotics, vol. 1, p. 6, 2009.
[26] ——, “How can we deﬁne intrinsic motivation?” in Proceedings of the 8th International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems, Lund University Cognitive Studies, Lund: LUCS, Brighton. Lund University Cognitive Studies, Lund: LUCS, Brighton, 2008.
[27] S. Gay, A. Mille, and A. Cordier, “Autonomous object modeling and exploiting: a new approach based on affordances from continual interaction with environment,” in The Seventh Joint IEEE International Conference on Developmental Learning and Epigenetic Robotics (ICDLEpirob), 2017.
[28] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and G. Pezzulo, “Discussion paper active inference and epistemic value,” 2015.

View publication stats

