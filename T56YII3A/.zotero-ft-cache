ARTICLES

COMPUTATION AND SYSTEMS

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control
Nathaniel D Daw1, Yael Niv1,2 & Peter Dayan1
A broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including one associated with prefrontal cortex and another with dorsolateral striatum. However, such a surfeit of control raises an additional choice problem: how to arbitrate between the systems when they disagree. Here, we consider dual-action choice systems from a normative perspective, using the computational theory of reinforcement learning. We identify a key trade-off pitting computational simplicity against the ﬂexible and statistically efﬁcient use of experience. The trade-off is realized in a competition between the dorsolateral striatal and prefrontal systems. We suggest a Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate. This provides a unifying account of a wealth of experimental evidence about the factors favoring dominance by either system.

Diverse neural systems, notably prefrontal cortex, the striatum and their dopaminergic afferents, are thought to contribute to the selection of actions. Their differential and integrative roles are under active examination, and an important hypothesis is that subparts of these regions subserve two largely distinct and parallel routes to action. Such a division is the neurobiological scaffolding for an equivalent hypothesis about dual controllers that is prominent in psychological accounts of a range of behavioral phenomena in economic, social and animal-conditioning contexts1–5.
The conventional idea is that the dorsolateral striatum and its dopaminergic afferents support habitual or reﬂexive control6, whereas prefrontal cortex is associated with more reﬂective or cognitive action planning7. (Following this convention, we will refer to the cognitive circuit as ‘prefrontal’, although it likely involves a number of additional regions, potentially including more medial striatal territories8.) This suggested dissociation is consistent with a range of electrophysiological9–11, functional magnetic resonance imaging (fMRI)12,13 and lesion studies14–17. The last are based on a clever behavioral approach to differentiating dual control strategies: namely, conditioning studies in which the values of rewards are unexpectedly changed. Outcome re-valuation affects the two styles of control differently and allows investigation of the characteristics of each controller, its neural substrates and the circumstances under which it dominates.
Despite the wealth of evidence, there are few answers to two key normative questions: why should the brain use multiple action controllers, and how should action choice be determined when they disagree? For a framework for answers, we turn to reinforcement learning18, the computational theory of learned optimal action control. In reinforcement learning, candidate actions are assessed through

predictions of their values, deﬁned in terms of the amount of reward they are expected eventually to bring about. Such predictions pose statistical and computational challenges when reward is contingent on performing a sequence of actions, and thus early action choices incur only deferred rewards. Approximations are essential in the face of these challenges; there are two major classes of reinforcement learning, which make different approximations, and so are differentially accurate in particular circumstances. One class involves ‘modelfree’ approaches such as temporal-difference learning, which underpin existing popular accounts of the activity of dopamine neurons and their (notably dorsolateral) striatal projections19,20. The other class involves ‘model-based’ methods18, which we identify with the second, prefrontal cortex system.
We propose that the difference in the accuracy proﬁles of different reinforcement learning methods both justiﬁes the plurality of control and underpins arbitration. To make the best decisions, the brain should rely on a controller of each class in circumstances in which its predictions tend to be most accurate. Here we suggest how the brain might estimate this accuracy for the purpose of arbitration by tracking the relative uncertainty of the predictions made by each controller. We show that this accounts for a range of factors shown in behavioral studies to favor either controller. To isolate our hypothesis, we develop the bulk of our account assuming strict separation between the systems; other aspects of their integration, particularly through learning21, are certainly also important.
We interpret the two controllers as representing opposite extremes in a trade-off between the statistically efﬁcient use of experience and computational tractability. Temporal-difference learning18 is a modelfree reinforcement learning method, which offers a compelling account of the activity of dopamine neurons in classical and instrumental

1Gatsby Computational Neuroscience Unit, University College London, Alexandra House, 17 Queen Square, London WC1N 3AR, UK. 2Interdisciplinary Center for Neural Computation, Hebrew University. P.O. Box 1255, Jerusalem 91904, Israel. Correspondence should be addressed to N.D.D. (daw@gatsby.ucl.ac.uk).
Received 15 April; accepted 12 September; published online 6 November 2005; doi:10.1038/nn1560

1704

[ [ VOLUME 8 NUMBER 12 DECEMBER 2005 NATURE NEUROSCIENCE

ARTICLES

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

a Tree System

b Cache system

Figure 1 Task representations used by tree-search and caching reinforcement learning methods in

S0 Initial state

S0 Initial state

a discrete-choice, discrete-trial representation of a standard instrumental conditioning task.

Press lever

Enter magazine

Press lever Q = 1

Enter magazine
Q = 0

(a) Structure of the task as represented by a tree-search controller. S0–S3 are the four possible states within the task; R ¼ {1, 0} represents

whether or not reward was attained. (b) A caching

S1 Food delivered

S2 No reward

S1 Food delivered

Press lever Q = 0

Enter magazine
Q = 1

reinforcement learning controller represents only the scalar expected future value (‘Q’) for each action in each state, divorced from the actual sequence and identity of future consequences.

Press lever

Enter magazine

R = 0

S2 No reward

tree of possible situations (states) that the subject can face in the task, the transitions between those states engendered by the possi-

Q = 0

ble actions and the reward that is available

given an appropriate sequence of actions.

S2 No reward

S3 Food obtained

S3 Food obtained

Acquiring this arboreal representation of the task from experience, and using it to choose

R = 0

R = 1

appropriate actions, are exactly the goals of

Q = 1

the tree-search controller.

In the next phase of the experiment, the

value of the food pellets is reduced, for

learning tasks19,20. The foundation of this method is what we refer to as instance by prefeeding the animal with them or by pairing them with

‘caching’: namely, the association of an action or situation with a illness to induce aversion. Then, animals are tested to see if they will

scalar summary of its long-run future value. A hallmark of this is the continue to perform the actions previously associated with the newly

ubiquitous transfer of the dopaminergic response from rewards to the devalued outcome. The test is performed without delivering outcomes

stimuli that predict them20. Working with cached values is computa- (formally, in extinction) to prevent new learning about the value of the

tionally simple but comes at the cost of inﬂexibility: the values are outcome during these trials.

divorced from the outcomes themselves and so do not immediately Outcome devaluation exploits a key distinction between tree search

change with the re-valuation of the outcome. This is also the deﬁning (Fig. 1a) and caching (Fig. 1b). Only tree search enumerates the

behavioral characteristic of habitual control.

speciﬁc consequences expected for some course of action, such as the

By contrast, we suggest that the prefrontal circuit subserves a model- identity of the food reward expected. The cached value of an action is,

based reinforcement learning method. This constructs predictions of by its nature, independent of any such speciﬁc outcome information.

long-run outcomes, not through cached storage, but rather on the ﬂy, Thus, if an animal acts based on a cached value, it will continue to

by chaining together short-term predictions about the immediate do so even after the outcome has been devalued. In psychology,

consequences of each action in a sequence. Because this involves such outcome-insensitive behavior is known as ‘habitual’5,22. If,

exploring a branching set of possible future situations, such methods however, a behavior is determined by tree search, its propensity should

are also known as ‘tree search’. Search in deep trees can be expensive in be sharply reduced following devaluation. Psychologists term such

terms of memory and time and can also be error-prone. However, outcome-sensitive behavior ‘goal-directed’5,22, as it changes when

that the predictions are constructed on the ﬂy allows them to react ‘goals’ are re-valued.

more nimbly to changed circumstances, as when outcomes are Behavioral experiments (summarized in Fig. 2) demonstrate that,

re-valued. This, in turn, is the behavioral hallmark of cognitive (or under different circumstances, animals show both proﬁles of devalua-

‘goal-directed’) control.

tion sensitivity. Moderately trained lever presses are indeed sensitive to

Here we develop these ideas in a formal, computational model and outcome devaluation (Fig. 2a, left)—suggesting control by a tree-

present simulation results that demonstrate the model’s ability to search system. After extensive training, though, lever pressing becomes

capture a body of animal conditioning data concerning the trade-off insensitive to devaluation (Fig. 2a, middle)—suggesting a transition to

between controllers. Our results suggest that principles of sound, caching control23. Lesions or depletions of dopaminergic input to

approximate, statistical reasoning may explain why organisms use dorsolateral areas of the striatum evidently block this transfer of control

multiple decision-making strategies and also provide a solution to to a caching system14,24. Such animals display relatively normal learn-

the problem of arbitrating between them.

ing of the task, but despite over-training, their lever pressing is

persistently sensitive to devaluation. This is consistent with choice

RESULTS

relying only on an intact tree-search controller.

Post-training reinforcer devaluation

The transition to caching with over-training is also tempered by two

We begin by discussing key experimental results suggesting the factors—the complexity of action choice and the proximity of the

circumstances under which each controller dominates. Behavioral action to reward. In more complex tasks, in which an animal may, for

psychologists have investigated this issue extensively by post-training instance, execute either of two different actions to obtain two different

reinforcer devaluation (see a recent review5 for references). In a typical rewards, extensively trained actions remain sensitive to outcome

experiment, hungry rats are trained to perform a sequence of actions, devaluation (Fig. 2a, right), indicating a dominance of tree-search

usually a lever press followed by entry to a food magazine, to obtain a control25,26. Finally, though the evidence is perhaps less persuasive,

reward such as a food pellet. We formally depict this task (Fig. 1a) as a actions closer to the reward are more sensitive to devaluation than

[ [ NATURE NEUROSCIENCE VOLUME 8 NUMBER 12 DECEMBER 2005

1705

ARTICLES

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience
Actions per min Actions per min

a
10
5

Distal action (lever press or chain pull)

b
10
5

Proximal action (magazine entry)
Non-devalued Devalued

0 Moderate training

Extensive training, one action, one outcome

Extensive training, two actions, two outcomes

0 Moderate training

Extensive training

Figure 2 Behavioral results from reward devaluation experiments in rats. Actions per minute in an extinction test after devaluation of the outcome (black) or without devaluation (white). (a) Actions distal from the outcome (lever pressing and chain pulling) after moderate or extensive training and with one or two actions and outcomes, adapted from ref. 26, experiment 2. (b) Magazine entries (more proximal to the outcome), adapted from ref. 17. Data and error bars reproduced here are for a control group; differences were signiﬁcant when collapsed with two additional lesion groups. Error bars: s.e.m.

actions further away. For instance, when animals must press a lever and chosen simply by comparing their values. The collection of

then enter a food magazine to obtain reward, the action more proximal values is called a ‘state-action value function’ or, for simplicity, a

to reward—magazine entry—remains devaluation-sensitive even after value function.

extensive training17 (Fig. 2b, right). In the experiment depicted here, The two classes of reinforcement learning methods can produce

this effect was signiﬁcant only when collapsed over multiple lesion different, and differentially accurate, estimates of the value function. As

groups (which did not differ signiﬁcantly among themselves), and there in other cases of evidence reconciliation in neuroscience, such as

are only few other published reports of over-trained magazine behavior. multisensory integration32, we suggest that arbitration between values

However, actions more proximal to reward are more readily sensitive to is based on the uncertainty or expected inaccuracy of each. Uncertainty devaluation in ‘incentive learning’ studies27, and extensively trained quantiﬁes ignorance about the true values (for example, about the

magazine responses remain devaluation-sensitive in a Pavlovian task probabilities of different payoffs); it should be distinguished from risk

without lever pressing28.

(which generically arises when payoffs are stochastic, but their prob-

The counterpart to lesions affecting the caching system is that lesions abilities may be known). For simplicity, we assume that the estimated

to a wide network of structures—including prelimbic cortex (a sub- value of each action is taken to be that derived from the controller that

area of rat prefrontal cortex)15–17, prefrontal-associated regions is more certain about the value. (Though most reliable, this estimate is

of dorsomedial striatum8, basolateral amygdala29, gustatory insular not necessarily the largest.) The probability of choosing an action for

cortex30 and, in a monkey study, orbitofrontal cortex31—seem to execution is then proportional to this value. In addition to controlling

interfere with tree-search control. That is, they eliminate devaluation estimation, uncertainty about an action’s value might, in principle,

sensitivity even for moderately trained behaviors.

inﬂuence choice directly, as by promoting exploration to seek undis-

covered rewards.

Theory sketch

In general, both controllers are uncertain about the values because

The lesion studies indicate that each controller can substitute for they begin ignorant and have only a limited amount of noisy experi-

the other even under circumstances when it would not normally ence. Even given inﬁnite experience, uncertainty persists, due to the

dominate. This suggests a theory combining separate and parallel possibility that the task itself (and hence the long-term values) can

reinforcement learners (the implementation

is detailed in Methods and Supplementary

Methods online).

As in previous applications of reinforcement learning to neural and behavioral data20,

S0 Initial state

we work with a stylized description of the experimental tasks (Figs. 1a and 3). This

Press Pull

Enter

lever chain magazine

allows us to expose a unifying, normative

interpretation of the pattern of experimental

results discussed above, rather than focusing

on a quantitative ﬁt to rather qualitative data. Here, the goal of optimal control is to choose

S1 Food A delivered

S2 Food B delivered

S3 No reward

actions that maximize the probability of the ultimate receipt of a valued outcome

Press Pull

Enter

lever chain magazine

Press Pull

Enter

lever chain magazine

(although it would be straightforward to

include additional factors into the optimiza-

tion, such as risk-sensitivity in the case of

stochastic rewards). Optimization can be accomplished by calculating or learning the value of taking each action at each state,

S3 No reward

S4 Food A obtained

S3 No reward

S5 Food B obtained

deﬁned in terms of the probability that

reward will later be received when starting

from that action in that state. Given such Figure 3 Stylized tree representation of an instrumental conditioning task with two actions (a lever press

information, advantageous actions can be and a chain pull) for two rewards. Note the additional states compared with Figure 1a.

1706

[ [ VOLUME 8 NUMBER 12 DECEMBER 2005 NATURE NEUROSCIENCE

ARTICLES

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

Press lever
Enter magazine

After 1 reward: S0

S1

S2

After 50 rewards: S0

S1

S2

S2

S3

S2

S3

Figure 4 Tree estimation at two stages of learning by the tree-search system on the task of Figure 1a. States are as in that ﬁgure; arrows represent the state transitions expected following a lever press or a magazine entry, with the width of each proportional to the estimated probability of the transition (averaged over 250 runs; s.e.m. error bars negligible). Before learning, all transitions were equally likely (data not shown).

change unexpectedly. We quantify uncertainty using approximate Bayesian versions of each reinforcement learning algorithm33,34. The differing methods of value estimation of the two systems give rise to differing uncertainty proﬁles.
A (prefrontal) tree-search system uses experience with the task to estimate the nature of the state transitions and rewards (essentially, reconstructing the ‘trees’ of Figs. 1a and 3). Long-term reward probabilities are estimated by iteratively searching through this tree; uncertainty about which tree describes the task makes the value estimates uncertain. Furthermore, such tree search is computationally demanding in realistic (wide or deep) trees. Thus, in practice, approximations must be introduced at each iteration, such as ‘pruning’ or exploring only a subset of paths. We model the resulting inaccuracy or ‘computational noise’ in derived value estimates as an additional source of uncertainty that accumulates with each search step.
A (dorsolateral striatal) caching system such as temporal-difference learning18 estimates the long-term values directly from experience, without explicitly constructing a tree. This relies on a different approximation: ‘bootstrapping’, or using the value estimates cached for subsequently encountered states as stand-ins for the actual longterm values at predecessor states. Initial ignorance and continual change make these values potentially inaccurate, and thus cause uncertainty. By contrast, though, the cached values make calculation straightforward, so there is little computational ‘noise’ associated with its output.
To summarize, both the value predictions and the estimated uncertainties will differ between the tree-search and caching systems. Our account of action choice is based on an analysis of these two sets of quantities.
Figure 5 Simulation of the dual-controller reinforcement learning model in the task of Figure 1a. (a) Distal action (lever press); (b) Proximal action (magazine entry). The topmost graphs show uncertainties (posterior variances) in the value estimates for different actions according to the cache (blue line) and tree (gold line), as a function of the number of rewarded training trials. The middle graphs show the value estimates themselves (posterior means); diamonds indicate the value estimates that would result after reward devaluation at various stages of training. Beneath the graphs are bar plots comparing the probability of choosing actions before and after their consequences were devalued, normalized to the non-devalued level. Bar color denotes which system (cache: blue; tree: gold) controlled the action in a majority of the 250 runs. All data reported are means over 250 runs; error bars (s.e.m.) are negligible.

Simulations We simulated the two-controller reinforcement learning model on the action choice tasks, beginning with the task with one lever press for one outcome (Fig. 1a). The quantitative results conformed with the qualitative expectations adduced above. The prefrontal tree system learned, over experience, the structure of action-induced state transitions in the task, assigning high probability to the actual transitions (Fig. 4; the system additionally tracked uncertainty about its estimates of the transition probabilities, which is not illustrated).
We studied each system’s action values (here, posterior means), along with its uncertainty (posterior variances) about those values, as functions of the amount of training and the position of the action in the behavioral sequence relative to the reward (Fig. 5a,b). Each system’s prior ignorance gradually resolved with experience. In all simulations, model-based reinforcement learning was more conﬁdent early in training, even though both systems had matched initial uncertainty. This is because under prefrontal tree search, any particular morsel of experience immediately propagates to inﬂuence the estimates of action values at all states; the effect of bootstrapping in dorsolateral striatal temporal-difference learning is to delay such propagation, making the system less data-efﬁcient.
Because the systems incorporate the expectation that actions’ values may change, past observations gradually become less relevant to present value estimates. This effective time horizon on the data implies that the uncertainties asymptote at ﬁnite levels for both systems. For the same reason, the value predictions can asymptote well short of the true payoffs. This asymptotic uncertainty has a harsher effect on the datainefﬁcient cache. Thus, for the action proximal to reward (the magazine response), enhanced data efﬁciency allowed the tree-search system to be more certain, even asymptotically (Fig. 5b). However, an extra iteration of tree search was required to evaluate the action more distal from reward (the lever press), incurring additional uncertainty asymptotically due to the assumption of computational noise outlined above. The effect of this, asymptotically (Fig. 5a), was to favor the cache system, which suffers no such computational noise because it recalls values rather than computing them.
We saw different results for a version of the task with two actions for two outcomes (Fig. 6a,b). Here, the agent’s experience was spread between more states and actions. Given the expectation of task change and the resulting temporal horizon on past experience, fewer relevant

a
0.04
0.02

Distal action (lever press)
Cache Tree

b
0.04

Proximal action (magazine entry)

0.02

Uncertainty

Uncertainty

0

0

1

1

Expected value

Expected value

Response rate relative to non-dev.

0.5

Non-dev.

Dev.

0

25 50 75 100

Rewarded trials

1

1

0.5

0.5

Non- Dev. dev.

Non- Dev. dev.

Response rate relative to non-dev.

0.5

0

25 50 75 100

Rewarded trials

1

1

0.5

0.5

Non- Dev. dev.

Non- Dev. dev.

[ [ NATURE NEUROSCIENCE VOLUME 8 NUMBER 12 DECEMBER 2005

1707

ARTICLES

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

a
0.04
0.02

Distal action (lever press)

Cache Tree

b
0.04

Proximal action (magazine entry)

0.02

Figure 6 Simulation of the dual-controller reinforcement learning model in the task of Figure 3, in which two different actions produced two different rewards. One of the rewards was devalued in probe trials. (a) Distal action (lever press). (b) Proximal action (magazine entry). The same conventions and results are shown as in Figure 5, except that data reported are means over 1,000 runs; error bars (s.e.m.) are again negligible.

Uncertainty

Uncertainty

0

0

1

1

Expected value

Expected value

0.5

Non-dev. Dev.

0

25

50

75 100

Rewarded trials

1

1

0.5

0.5

0.5

0 25 50 75 100 Rewarded trials

1

1

0.5

0.5

Response rate relative to non-dev.

Response rate relative to non-dev.

Non- Dev. dev.

Non- Dev. dev.

Non- Dev. dev.

Non- Dev. dev.

data were available to constrain any particular action value. The effect was asymptotically to preserve the tree system’s uncertainty advantage from the early training, low-data situation, even for the distal lever press (Fig. 6a).
Whenever the tree system dominated, the overall system’s action choices were sensitive to outcome devaluation, whereas when the caching system dominated, they were not (Figs. 5 and 6, bar plots). This is because the underlying value predictions were sensitive to devaluation only in the tree system. The simulations, then, reproduced and explained the pattern seen in the behavioral experiments: overtraining promoted devaluation insensitivity, unless opposed by the countervailing effects of proximity to reward or task complexity. The results support the underlying hypothesis that the brain appropriately deploys each controller under those circumstances in which it is expected to be most accurate.
DISCUSSION Our account builds on and extends existing ideas in several key ways. In contrast to the somewhat descriptive animal learning theories that are its foundation4,5,22, we have adopted a normative view, unifying the body of results on controller competition by appealing to uncertainty. This stance also contrasts with accounts of human behavioral data1,3: notably, ideas in economics2 suggesting that irrational, impulsive or emotional limbic inﬂuences (in our terms, the caching system) interfere with a more rational prefrontal controller. Under our account, both controllers are pursuing identical rational ends; in appropriate circumstances, the caching controller can more effectively accomplish the same functions as the prefrontal controller.
Among reinforcement learning theories, there are various precedents for the idea of combining several controllers, including multiple caching controllers35–37 and also (partly cerebellar) model-based and model-free controllers38. However, normative, competitive interaction has not hitherto been investigated. Most other reinforcement learning theories that contemplate model-based control either completely replace caching with search39,40, or envision a hybrid blending features of both41,42. Such theories founder on lesion results indicating

a dissociation between the neural substrates for tree-like and cache-like choice8,14–17,24.
Of course, normativity only extends so far for us. The true reason for multiple controllers in our theory is the computational intractability of the complete Bayesian solution (roughly speaking, the tree-search system unencumbered by computational incapacity) and the resulting need for approximations. The cache system is an extreme example of an approximation that embraces potential inaccuracy to gain computational simplicity.
Neural substrates
We built on the classic idea that habitual control is associated with dopamine and dorsolateral striatum, and more cognitive search with prefrontal cortex. Because behavioral and lesion studies suggest these controllers can operate independently, for the purpose of modeling we made the simplifying approximation that they are strictly separate. However, their neural substrates are clearly intertwined—prefrontal cortex is itself dopaminergically innervated, and cortex and striatum are famously interconnected in ‘loops’43, including one that joins prefrontal areas with dorsomedial subregions of striatum. Indeed, recent pharmacological and lesion results implicate those prefrontalassociated striatal areas in tree-search control8. Competition between model-based and model-free control might, therefore, best be viewed as between dorsomedial and dorsolateral corticostriatal loops, rather than between cortex and striatum per se, a view that extends previous ideas about multiple caching controllers coexisting in different loops35,36. Although dopamine is hypothesized to support learning in the caching system, the role of dopamine in the tree-search controller remains wholly unresolved.
Computational considerations also suggest that the systems should interact. Owing to computational costs in tasks involving deep trees, it is commonplace in reinforcement learning to search partway along some paths, then use cached values to substitute for unexplored subtrees18. Uncertainties can be compared at each stage to decide whether to expand the tree or to fall back on the cache44, trading off the likely costs (for example, time or calories) of additional search against its expected beneﬁts (more accurate valuations allowing better reward harvesting). The essentials of our account would be preserved in a model incorporating such partial evaluation, and the resulting improvement in the tree system’s valuations due to learning in the cache system echoes other suggestions that learning in the basal ganglia might train or inform cortex11,21.
There is limited evidence about the substrate for the uncertaintybased arbitration that has been our key focus. First, along with direct, population-code representations of uncertainty45, cholinergic and noradrenergic neuromodulation have often been implicated46. Second, two candidates for arbitration are the infralimbic cortex (IL; part of the prefrontal cortex) and the anterior cingulate cortex (ACC). Lesions to the IL reinstate tree-search from previously caching control16,17; however, because this area is not classically part of the habitual system, it has been suggested that it might support controller competition17. The involvement of the ACC in the competition-related functions of monitoring and resolving response error and conﬂict has been suggested in experimental and theoretical studies47,48.

1708

[ [ VOLUME 8 NUMBER 12 DECEMBER 2005 NATURE NEUROSCIENCE

ARTICLES

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

Complementary evidence about dual control arises from spatial tasks in both humans and animals37,49. Navigational decisions can arise from a ﬂexible ‘cognitive map’ that supports latent learning and is associated with the hippocampus; with practice, they become habitized and evidently under dorsal striatal control.
Experimental considerations One route to test our framework is neuronal recordings. We expect activity in areas associated with each controller to reﬂect its decision preferences, even when (as a result of arbitration) the other is actually directing behavior. Behavior should thus be better correlated with activity in whichever system is producing it. By manipulating factors such as the amount of training or the proximity of response to reward, it should be possible to transfer control between the systems and thereby to switch the behavioral-neural correlations.
Researchers have recently recorded from striatum and prefrontal cortex (interpreted as parts of the cache and tree systems, respectively) in monkeys over-trained on an associative learning task with reversals11. Various features of this task could promote the dominance of either system—extreme over-training and task simplicity favor cache control, but action-reward proximity and frequent reversals promote tree search. The neural recordings are also inconclusive. A direct interpretation supports striatal control: neurons there are more strongly selective for the animal’s choices, earlier in trials, and more robustly after reversals. However, an alternative interpretation instead supports prefrontal dominance, because change in the prefrontal representation correlates with behavioral re-learning following reversal. A devaluation challenge or recordings under different task circumstances (over-training levels, etc.) could help to distinguish these possibilities.
Because, in these recordings, representational changes occur faster in striatum, the authors suggest11 that relearning the correct responses following reversal might be more rapid in the striatum, and that this knowledge subsequently transfers to cortex21. This contrasts with some habitization models in which learning progresses in the other order, though our theory makes no speciﬁc claim about the relative ‘learning rates’ of the two systems. In any case, subsequent analysis of error trials shows that the striatal ﬁring reﬂects the animal’s actual (and not the correct) choices (A. Pasupathy & E.K. Miller, Comput. Syst. Neurosci. Abstr., p. 38, 2005). Finally, because the striatal region recorded (caudate) includes areas likely corresponding to dorsomedial striatum in rats, it may be that this area too is part of the tree system and not the cache8, in which case properly interpreting the results will require a more ﬁnely fractionated understanding of the neural organization of tree search.
Our theory provides additional testable factors likely to inﬂuence the trade-off between systems. Computational pressures might be increased, and tree search discouraged, in tasks that pose more strenuous cognitive demands (for example, delayed match to sample; such a strategy has been used with humans2 but not in animal devaluation studies). Introducing unexpected changes in task contingencies should also favor the data-efﬁcient tree system, because relevant data thereby become more scarce. Further, although task complexity favors goal-directed control, the details of task structure may have subtler effects. It has long been known in reinforcement learning that caching is relatively advantageous in tasks with a fan-out structure (in which a state might be followed randomly by any of several others); conversely, tasks with linear or fan-in structure (several states leading to one) should favor search.
Finally, our theory is applicable to several other phenomena in animal behavior. Stimuli can signal reinforcement that is available irrespective of the animal’s actions, and these ‘Pavlovian’ associations can affect

behavior. Such stimulus-reward predictions might originate from both cache and tree systems, with rich interactions and consequences. In ‘conditioned reinforcement’, animals learn to work to receive a stimulus that had previously been paired with reinforcement. Such learning might occur in either of our reinforcement learning systems. However, it is also a plausible test case for their potential interaction through partial evaluation, as the tree system might explore the consequences of the (new) response but defer to the cache’s evaluation of the (familiar) subsequent stimulus. Animals can acquire a new conditioned response even for a stimulus whose associated reinforcer had been devalued50, suggesting at least the value of the stimulus was cached. The hypothesized involvement of both systems might be investigated with lesions disabling each.
Our theory also casts the phenomenon of ‘incentive learning’27 in a new light. In this, for some actions to be sensitive to outcome devaluation, the animal must previously have experienced the reinforcer in the devalued state. The predominant account of incentive learning5 holds that such experience is necessary for the goal-directed system (our tree) to learn about the new value of the reinforcer. We suggest instead that experience with the outcome decreases the tree system’s uncertainties (by conﬁrming existing knowledge about the outcome’s value). This tends to promote its dominance over the cache, explaining interactions between outcome exposure and other factors such as over-training and reward proximity27. Because outcome exposure allows the tree system to overcome caching control, our theory makes the strong prediction (contrary to the standard account) that the need for such experience should vanish in animals with lesions disabling the caching system.
METHODS Background. For simplicity, we modeled conditioning tasks using absorbing Markov decision processes (MDPs)18 (Figs. 1a and 3)—ones in which experience is structured as a set of trials, with a set of terminal states at which an episode can end. We assumed that outcomes were delivered only (if at all) in terminal states and identiﬁed particular terminal states with particular outcomes (for instance, different foods).
Key to our account are two complicating factors. First, the agent started without knowing the exact MDP, which, furthermore, could change over time. These were the major sources of uncertainty. Second, although MDPs traditionally treat rewards with static, scalar utilities, here devaluation treatments explicitly changed some outcomes’ utilities. For convenience, we assumed that rewards were binary (0 or 1) and used the probability that the reward was 1 in a particular terminal state as a surrogate for the associated reward’s utility.
Choice in both cache and tree systems depended on scalar values— predictions of the future utility of executing a particular action at a particular state. If an outcome was devalued, both could learn by experiencing it that its corresponding state had lower utility. However, only the tree system used that information to guide subsequent action choice at distal states, as it derived action values by considering what future states would result. The cache system’s values were stored scalars and were thus insensitive even to known changes in outcome value, absent new experience of the action actually producing the outcome.
Fully optimal choice in unknown MDPs is radically computationally intractable. Tree and cache reinforcement learning methods therefore each rely on approximations, and we tracked uncertainties about the values produced by such systems to determine for what circumstances each method is best suited.
Formal model. An absorbing MDP comprises sets S of states and A of actions, a ‘transition function’ T(s, a, s¢)  P(s(t + 1) ¼ s¢ | s(t) ¼ s, a(t) ¼ a) specifying the probability that state s¢ A S will follow state s A S given action a A A, and (in our version) a ‘reward function’ R(s)  P(reward(t) ¼ 1 | s(t) ¼ s) specifying the probability that reward is received in terminal state s.
Here, the state-action value function Q(s, a) is the expected probability that reward will ultimately be received, given that the agent takes action a in state s

[ [ NATURE NEUROSCIENCE VOLUME 8 NUMBER 12 DECEMBER 2005

1709

ARTICLES

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

and chooses optimally thereafter. The formal deﬁnition is recursive:

(

Qðs; aÞ 

P

Tðs;

a;

RðsÞ s0Þ Á max½Qðs0; a0Þ

s is terminal ða ¼ Þ otherwise

s0

a0

Standard reinforcement learning methods18 do not track uncertainty in their estimates of Q. We consider Bayesian variations33,34, which estimate not simply the expected value Q(s, a) but a posterior distribution Qs,a(q)  P(Q(s, a) ¼ q | data) that measures, for any 0 r q r 1, how likely it is that the true optimal probability of future reward (compounded over different paths through the states) equals q, given the evidence, ‘data’, about transitions and outcomes so far observed. A Bayesian tree-search (‘value iteration’) system34 uses experience to estimate a posterior distribution over the MDP (functions T and R) and explores it to derive distributions over Q(s, a) (Supplementary Fig. 1 online). A Bayesian caching (‘Q-learning’) system33 instead stores a distribution over Q(s, a) for each action and state and updates it for consistency with the stored value distributions of subsequently encountered states (Supplementary Fig. 2 online). Full equations appear in Supplementary Methods.
If, for a particular controller, state and action, the distribution Qs,a is sharply peaked at some q, then the controller is fairly certain of the value; if it is instead spread out over a range of possible q’s, then the controller cannot identify the value with certainty. We thus arbitrated between the controllers’ estimates on the basis of their variance (mean squared error, ‘uncertainty’): given distributions Qtsr;aee from the tree anDd Qcs;aEache from the cache, we took the winning value Q(s, a) to be the mean Qtsr;aee iDf the vaEriance of Qtsr;aee was smaller than the variance of Qcs;aache, and the mean Qcs;aache otherwise. (Softer integration schemes, such as a certainty-weighted average, are also possible.) Given winning estimates Q(s, a) for each action available in the current state, we chose an action stochastically using softmax probabilities, P(a(t) ¼ a | s(t) ¼ s) p ebQ(s,a) where the parameter b controlled the tendency of the system to choose exclusively the action deemed best. Experimentally, the effect of devaluation can be assessed either within or between animals (by comparing to another action or group for which the outcome was not devalued). In our simulations, we compared the probabilities of choosing the same action a in the relevant state s, with or without devaluation (similar to the between-group approach); softmax action selection ensured that a reduction in Q(s, a) for an action will reduce the probability that the action is chosen.
Note that posterior uncertainty quantiﬁes ignorance about the true probability of reward, not inherent stochasticity in reward delivery. For instance, reward may follow from some state randomly with 50% probability—but if the controller can precisely identify that the true probability is 50% rather than some other number, the value is not uncertain.

Note: Supplementary information is available on the Nature Neuroscience website.

ACKNOWLEDGMENTS We are grateful to B. Balleine, A. Courville, A. Dickinson, P. Holland, D. Joel, S. McClure and M. Sahani for discussions. The authors are supported by the Gatsby Foundation, the EU Bayesian Inspired Brain and Artefacts (BIBA) project (P.D., N.D.), a Royal Society USA Research Fellowship (N.D.) and a Dan David Fellowship (Y.N.).

COMPETING INTERESTS STATEMENT The authors declare that they have no competing ﬁnancial interests.

Published online at http://www.nature.com/natureneuroscience/ Reprints and permissions information is available online at http://npg.nature.com/ reprintsandpermissions/

1. Kahneman, D. & Frederick, S. Representativeness revisited: attribute substitution in intuitive judgment. in Heuristics and Biases: the Psychology of Intuitive Judgment (eds. T. Gilovich, D.G. & Kahneman, D.) 49–81 (Cambridge University Press, New York, 2002).
2. Loewenstein, G. & O’Donoghue, T. Animal spirits: affective and deliberative processes in economic behavior. Working Paper 04–14, Center for Analytic Economics, Cornell University (2004).
3. Lieberman, M.D. Reﬂective and reﬂexive judgment processes: a social cognitive neuroscience approach. in Social Judgments: Implicit and Explicit Processes (eds. Forgas, J., Williams, K. & von Hippel, W.) 44–67 (Cambridge University Press, New York, 2003).

4. Killcross, S. & Blundell, P. Associative representations of emotionally signiﬁcant outcomes. in Emotional Cognition: from Brain to Behaviour (eds. Moore, S. & Oaksford, M.) 35–73 (John Benjamins, Amsterdam, 2002).
5. Dickinson, A. & Balleine, B. The role of learning in motivation. in Stevens’ Handbook of Experimental Psychology Vol. 3: Learning, Motivation and Emotion 3rd edn. (ed. Gallistel, C.R.) 497–533 (Wiley, New York, 2002).
6. Packard, M.G. & Knowlton, B.J. Learning and memory functions of the basal ganglia. Annu. Rev. Neurosci. 25, 563–593 (2002).
7. Owen, A.M. Cognitive planning in humans: neuropsychological, neuroanatomical and neuropharmacological perspectives. Prog. Neurobiol. 53, 431–450 (1997).
8. Yin, H.H., Ostlund, S.B., Knowlton, B.J. & Balleine, B.W. The role of the dorsomedial striatum in instrumental conditioning. Eur. J. Neurosci. 22, 513–523 (2005).
9. Jog, M.S., Kubota, Y., Connolly, C.I., Hillegaart, V. & Graybiel, A.M. Building neural representations of habits. Science 286, 1745–1749 (1999).
10. Holland, P.C. & Gallagher, M. Amygdala-frontal interactions and reward expectancy. Curr. Opin. Neurobiol. 14, 148–155 (2004).
11. Pasupathy, A. & Miller, E.K. Different time courses of learning-related activity in the prefrontal cortex and striatum. Nature 433, 873–876 (2005).
12. McClure, S.M., Laibson, D.I., Loewenstein, G. & Cohen, J.D. Separate neural systems value immediate and delayed monetary rewards. Science 306, 503–507 (2004).
13. O’Doherty, J. et al. Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science 304, 452–454 (2004).
14. Yin, H.H., Knowlton, B.J. & Balleine, B.W. Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning. Eur. J. Neurosci. 19, 181–189 (2004).
15. Balleine, B.W. & Dickinson, A. Goal-directed instrumental action: contingency and incentive learning and their cortical substrates. Neuropharmacology 37, 407–419 (1998).
16. Coutureau, E. & Killcross, S. Inactivation of the infralimbic prefrontal cortex reinstates goal-directed responding in overtrained rats. Behav. Brain Res. 146, 167–174 (2003).
17. Killcross, S. & Coutureau, E. Coordination of actions and habits in the medial prefrontal cortex of rats. Cereb. Cortex 13, 400–408 (2003).
18. Sutton, R.S. & Barto, A.G. Reinforcement Learning: an Introduction (MIT Press, Cambridge, Massachusetts, 1998).
19. Houk, J.C., Adams, J.L. & Barto, A.G. A model of how the basal ganglia generate and use neural signals that predict reinforcement. in Models of Information Processing in the Basal Ganglia (eds. Houk, J.C., Davis, J.L. & Beiser, D.G.) 249–270 (MIT Press, Cambridge, Massachusetts, 1995).
20. Schultz, W., Dayan, P. & Montague, P.R. A neural substrate of prediction and reward. Science 275, 1593–1599 (1997).
21. Houk, J.C. & Wise, S.P. Distributed modular architectures linking basal ganglia, cerebellum, and cerebral cortex: their role in planning and controlling action. Cereb. Cortex 5, 95–110 (1995).
22. Dickinson, A. Actions and habits—the development of behavioural autonomy. Phil. Trans. R. Soc. Lond. B 308, 67–78 (1985).
23. Adams, C.D. Variations in the sensitivity of instrumental responding to reinforcer devaluation. Q. J. Exp. Psychol. 34B, 77–98 (1982).
24. Faure, A., Haberland, U., Conde´, F. & Massioui, N.E. Lesion to the nigrostriatal dopamine system disrupts stimulus-response habit formation. J. Neurosci. 25, 2771–2780 (2005).
25. Colwill, R.M. & Rescorla, R.A. Instrumental responding remains sensitive to reinforcer devaluation after extensive training. J. Exp. Psychol. Anim. Behav. Process. 11, 520–536 (1985).
26. Holland, P.C. Relations between Pavlovian-instrumental transfer and reinforcer devaluation. J. Exp. Psychol. Anim. Behav. Process. 30, 104–117 (2004).
27. Balleine, B.W., Garner, C., Gonzalez, F. & Dickinson, A. Motivational control of heterogeneous instrumental chains. J. Exp. Psychol. Anim. Behav. Process. 21, 203–217 (1995).
28. Holland, P. Amount of training affects associatively-activated event representation. Neuropharmacology 37, 461–469 (1998).
29. Blundell, P., Hall, G. & Killcross, S. Preserved sensitivity to outcome value after lesions of the basolateral amygdala. J. Neurosci. 23, 7702–7709 (2003).
30. Balleine, B.W. & Dickinson, A. The effect of lesions of the insular cortex on instrumental conditioning: evidence for a role in incentive memory. J. Neurosci. 20, 8954–8964 (2000).
31. Izquierdo, A., Suda, R.K. & Murray, E.A. Bilateral orbital prefrontal cortex lesions in rhesus monkeys disrupt choices guided by both reward value and reward contingency. J. Neurosci. 24, 7540–7548 (2004).
32. Deneve, S. & Pouget, A. Bayesian multisensory integration and cross-modal spatial links. J. Physiol. (Paris) 98, 249–258 (2004).
33. Dearden, R., Friedman, N. & Russell, S.J. Bayesian Q-learning. in Proceedings of the 15th National Conference on Artiﬁcial Intelligence (AAAI) 761–768 (1998).
34. Mannor, S., Simester, D., Sun, P. & Tsitsiklis, J.N. Bias and variance in value function estimation. in Proceedings of the 21st International Conference on Machine Learning (ICML) 568–575 (2004).
35. Nakahara, H., Doya, K. & Hikosaka, O. Parallel cortico-basal ganglia mechanisms for acquisition and execution of visuomotor sequences - a computational approach. J. Cogn. Neurosci. 13, 626–647 (2001).
36. Tanaka, S.C. et al. Prediction of immediate and future rewards differentially recruits cortico-basal ganglia loops. Nat. Neurosci. 7, 887–893 (2004).

1710

[ [ VOLUME 8 NUMBER 12 DECEMBER 2005 NATURE NEUROSCIENCE

ARTICLES

37. Chavarriaga, R., Strosslin, T., Sheynikhovich, D. & Gerstner, W. A computational model of parallel navigation systems in rodents. Neuroinformatics 3, 223–242 (2005).
38. Doya, K. What are the computations in the cerebellum, the basal ganglia, and the cerebral cortex. Neural Netw. 12, 961–974 (1999).
39. Suri, R.E. Anticipatory responses of dopamine neurons and cortical neurons reproduced by internal model. Exp. Brain Res. 140, 234–240 (2001).
40. Smith, A.J., Becker, S. & Kapur, S. A computational model of the functional role of the ventral-striatal D2 receptor in the expression of previously acquired behaviors. Neural Comput. 17, 361–395 (2005).
41. Dayan, P. & Balleine, B.W. Reward, motivation and reinforcement learning. Neuron 36, 285–298 (2002).
42. Daw, N.D., Courville, A.C. & Touretzky, D.S. Timing and partial observability in the dopamine system. in Advances in Neural Information Processing Systems 15, 99–106 (MIT Press, Cambridge, Massachusetts, 2003).
43. Alexander, G.E., Delong, M.R. & Strick, P.L. Parallel organization of functionally segregated circuits linking basal ganglia and cortex. Annu. Rev. Neurosci. 9, 357–381 (1986).

44. Baum, E.B. & Smith, W.D. A Bayesian approach to relevance in game playing. Artiﬁcial Intelligence 97, 195–242 (1997).
45. Pouget, A., Dayan, P. & Zemel, R.S. Inference and computation with population codes. Annu. Rev. Neurosci. 26, 381–410 (2003).
46. Yu, A.J. & Dayan, P. Uncertainty, neuromodulation, and attention. Neuron 46, 681–692 (2005).
47. Holroyd, C.B. & Coles, M.G. The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity. Psychol. Rev. 109, 679–709 (2002).
48. Botvinick, M.M., Cohen, J.D. & Carter, C.S. Conﬂict monitoring and anterior cingulate cortex: an update. Trends Cogn. Sci. 8, 539–546 (2004).
49. Hartley, T. & Burgess, N. Complementary memory systems: competition, cooperation and compensation. Trends Neurosci. 28, 169–170 (2005).
50. Parkinson, J.A., Roberts, A.C., Everitt, B.J. & Di Ciano, P. Acquisition of instrumental conditioned reinforcement is resistant to the devaluation of the unconditioned stimulus. Q. J. Exp. Psychol. B 58, 19–30 (2005).

© 2005 Nature Publishing Group http://www.nature.com/natureneuroscience

[ [ NATURE NEUROSCIENCE VOLUME 8 NUMBER 12 DECEMBER 2005

1711

