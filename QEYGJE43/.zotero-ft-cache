Reinforcement Learning Approaches in Dynamic Environments
Miyoung Han
To cite this version:
Miyoung Han. Reinforcement Learning Approaches in Dynamic Environments. Databases [cs.DB]. Télécom ParisTech, 2018. English. ￿tel-01891805￿

HAL Id: tel-01891805 https://hal.inria.fr/tel-01891805
Submitted on 10 Oct 2018

HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.

2018-ENST- ? ? ? ?

EDITE - ED 130

Doctorat ParisTech

T H E` S E
pour obtenir le grade de docteur d´elivr´e par

TE´LE´COM ParisTech
Sp´ecialit´e Informatique

pr´esent´ee et soutenue publiquement par
Miyoung HAN
le 19 juillet 2018
Approches d’apprentissage par renforcement dans les environnements dynamiques

Directeur de the` se : Pierre Senellart

T

Jury

H

Mme AMER-YAHIA Sihem, Directrice de Recherche, CNRS

Examinatrice

È

M. CAUTIS Bogdan, Professeur, Universite´ Paris-Sud M. GROSS-AMBLARD David, Professeur, Universite´ de Rennes 1

Rapporteur Rapporteur

S

M. SENELLART Pierre, Professeur, ENS, Universite´ PSL M. WUILLEMIN Pierre-Henri, Maˆıtre de Confe´ rences, Sorbonne University

Directeur de the` se Examinateur

E

TE´LE´COM ParisTech e´ cole de l’Institut Mines-Te´ le´ com - membre de ParisTech
46 rue Barrault 75013 Paris - (+33) 1 45 81 77 77 - www.telecom-paristech.fr

2

Contents

1 Introduction

5

1.1 Motivation and Objective . . . . . . . . . . . . . . . . . . . . . . . . 5

1.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . 6

1.3 Overview and Contributions . . . . . . . . . . . . . . . . . . . . . . . 7

I Literature Review

9

2 Reinforcement Learning

11

2.1 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . 11

2.2 Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . 13

2.2.1 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 13

2.2.2 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.2.3 Interaction between Policy Evaluation and Policy Improvement. 16

2.3 Temporal-Diﬀerence Methods . . . . . . . . . . . . . . . . . . . . . . 16

2.4 Function Approximation Methods . . . . . . . . . . . . . . . . . . . . 19

2.5 Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

2.6 Model-Based and Model-Free Methods . . . . . . . . . . . . . . . . . 24

2.7 Priority-Based Value Iteration . . . . . . . . . . . . . . . . . . . . . . 26

2.8 Non-Stationary Environment . . . . . . . . . . . . . . . . . . . . . . . 29

II Applications

33

3 Model-Free and Model-Based Methods

35

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

3.2 Learning without Models . . . . . . . . . . . . . . . . . . . . . . . . . 35

3.2.1 Background and Related Work . . . . . . . . . . . . . . . . . 36

3.2.2 Q-learning for Taxi Routing . . . . . . . . . . . . . . . . . . . 36

3.2.3 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . 38

3.2.4 Demonstration Scenario . . . . . . . . . . . . . . . . . . . . . 40

3.3 Learning Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

3.3.1 Background: Factored MDP . . . . . . . . . . . . . . . . . . . 42

3.3.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

3.3.3 Algorithm for Structure Learning . . . . . . . . . . . . . . . . 45

3

3.3.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.4 Discussion and Future Research . . . . . . . . . . . . . . . . . . . . . 50 3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

4 Focused Crawling

53

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

4.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

4.3 Focused Crawling and Reinforcement Learning . . . . . . . . . . . . . 55

4.3.1 Markov Decision Processes (MDPs) in Crawling . . . . . . . . 56

4.3.2 MDPs with Prioritizing Updates . . . . . . . . . . . . . . . . . 59

4.3.3 Linear Function Approximation with Prioritizing Updates . . 59

4.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

4.5 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

4.6 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

4.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

5 Inﬂuence Maximization

73

5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

5.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

5.3 Topic-Based Inﬂuence Maximization Algorithm for Unknown Graphs 76

5.3.1 Problem Statement and our Method . . . . . . . . . . . . . . 76

5.3.2 Modeling and Algorithm . . . . . . . . . . . . . . . . . . . . . 77

5.4 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

5.5 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

5.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

6 Conclusion

87

6.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

6.2 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

Bibliography

91

Appendices

101

A R´esum´e en fran¸cais

103

4

Chapter 1
Introduction
In this thesis, we apply reinforcement learning to sequential decision making problems in dynamic environments. This chapter presents the motivation, objective, and an overview of this thesis. We begin by presenting the motivation and objective of this thesis. Then we introduce brieﬂy reinforcement learning that is used as a fundamental framework throughout this thesis. Finally we provide a preview of each chapter including contributions.
1.1 Motivation and Objective
Reinforcement learning [94] is based on the idea of trial-and-error learning and it has been commonly used in robotics, with applications such as robot soccer [88], robot helicopters [1], etc.
It has also been used in various applications that concern sequential decision making problems in dynamic environments such as power management [95], channel allocation [91], traﬃc light control problems [19], etc. Power management in data centers is a rapidly growing concern in economic and environmental issues. In [95], a reinforcement learning approach is presented to learn eﬀective management policies of both performance and power consumption in web application servers. In cellular telephone systems, an important problem is to dynamically allocate the communication channels to maximize the service provided to mobile callers. This problem is tackled in [91] using a reinforcement learning method to allocate the available channels to calls in order to minimize the number of blocked calls and the number of calls that are dropped when they are handed oﬀ to a busy call. A reinforcement learning method is also applied to the traﬃc lights control problem [19] that adjusts traﬃc signal according to real-time traﬃc in order to reduce traﬃc congestion. The agent learns a traﬃc signal control policy in which vehicles do not wait too long for passing through the intersection.
These problems have explicit goals to achieve and they require making an optimal decision for a given environment in order to achieve the goals. Environments change in reaction to some control behaviors. However, it is diﬃcult to design optimal policies in advance because environment models are not available. In such problems, reinforcement learning can be used to ﬁnd the optimal policies. It learns the policies by interacting with the environment in order to achieve a goal. The learned policies
5

take into account long-term consequences of individual decisions. In this thesis, we solve several sequential decision making problems using rein-
forcement learning methods. For example, in a focused crawling problem, a crawler has to collect as many Web pages as possible that are relevant to a predeﬁned topic while avoiding irrelevant pages. Many crawling methods use classiﬁcation for unvisited links to estimate if the links point to relevant pages but these methods do not take into account long-term eﬀects of selecting a link. In the inﬂuence maximization problem, the agent aims to choose the most inﬂuential seeds to maximize inﬂuence under a certain information diﬀusion model. The problem already takes into account long-term values but not necessarily the planning dimension that reinforcement learning introduces.
To solve such sequential decision making problems, we ﬁrst formulate the problems as Markov decision processes (MDPs), a general problem formulation of reinforcement learning. Then we solve these problems using appropriate reinforcement learning methods for corresponding problems and demonstrate that reinforcement learning methods ﬁnd stochastic optimal policies for each problem that are close to the optimal.
1.2 Reinforcement Learning
Reinforcement learning is similar to the way of learning of humans and animals. In fact, many of the algorithms of reinforcement learning are inspired by biological learning systems [94].
In reinforcement learning, an agent learns from continuing interaction with an environment in order to achieve a goal. Such interaction produces lots of information about the consequences of the behavior, that helps to improve its performance. Whenever the learning agent does an action, the environment responds to its action by giving a reward and presenting a new state. The agent’s objective is to maximize the total amount of reward it receives. Through experience in its environment, it discovers which actions stochastically produce the greatest reward and uses such experience to improve its performance for subsequent trials. That is, the agent learns how to behave in order to achieve goals. In reinforcement learning, all agents have explicit goals and learn decisions by interacting with their environment in order to achieve the goals.
Reinforcement learning focuses on learning how good it is for the agent to be in a state over the long run, called a value of state, or how good it is to take an action in a given state over the long run, called a value of action. A reward is given immediately by an environment as a response of the agent’s action and a learning agent uses the reward to evaluate the value of a state or action. The best action is selected by values of states or actions because the highest value brings about the greatest amount of reward over the long run. Then the learning agent can maximize the cumulative reward it receives.
A model represents the environment’s dynamics. A learning agent learns value functions with or without a model. When a reinforcement learning algorithm constructs a model of the environment and learns value functions from the model, it is called a model-based method. Reinforcement learning algorithms can learn value
6

functions directly from experience without any environment models. If an algorithm learns values of states or actions from trial-and-error without a model, we call it a model-free method. Since a model mimics the behavior of the environment, it allows to estimate how the environments will change in response to what the agent does. However, learning a complete and accurate model requires more complex computation than model-free methods. We study a model-free method and a model-based method in Chapter 3.
These value functions can be represented using tabular forms but, in large and complicated problems, tabular forms cannot eﬃciently store all value functions. In this case, the functions must be approximated using parameterized function representation for large problems. In Chapters 4 and 5, we study a focused crawling problem and an inﬂuence maximization problem using a function approximation method.
1.3 Overview and Contributions
In Chapter 2, we review the main concepts of reinforcement learning that we have used as a fundamental framework throughout this thesis. We start with the notion of Markov decision process (MDP), that is the general problem formulation of reinforcement learning. Then, we describe the fundamental methods for solving MDP problems, such as dynamic programming (DP) and temporal-diﬀerence (TD) methods. These methods based on tabular forms can be extended into function approximation methods that can be applied to much larger-scale problems. We also present some important topics or improvements presented in the reinforcement learning literature.
In Chapter 3, we study two main approaches for solving reinforcement learning problems: model-free and model-based methods. First, we study a model-free method that learns directly from observed experiences without a model. We present a Q-learning [98] based algorithm with a customized exploration and exploitation strategy to solve a real taxi routing problem. We demonstrate that a reinforcement learning algorithm is able to progressively learn optimal actions for routing an autonomous taxi to passenger pick-up points. In experiments, we quantify the inﬂuence of two important parameters of Q-learning – the step size and discount rate – on eﬀectiveness. We also investigate the inﬂuence of trade-oﬀ between exploration and exploitation on learning. We published that work in the industry track of the CIKM 2016 conference [50].
Then, we turn to a model-based method that learns transition and reward models of the environment. We address the factored MDP problem [7] where a state is represented by a vector of n variables, in a non-deterministic setting. Most modelbased methods are based on Dynamic Bayesian Network (DBN) transition models. We propose an algorithm that learns the DBN structures of state transitions including synchronic parents. Decision trees are used to represent transition functions. In experiments, we show the eﬃciency of our algorithm by comparing with other algorithms. We also demonstrate that factorization methods allow to learn eﬀectively complete and correct models to obtain the optimal policies and through the learned models the agent can accrue more cumulative rewards.
7

In Chapter 4, we extend our discussion to a very large and continuous domain, in particular, a focused crawling problem. Focused crawling aims at collecting as many Web pages relevant to a target topic as possible while avoiding irrelevant pages, reﬂecting limited resources available to a Web crawler. We improve on the eﬃciency of focused crawling by proposing an approach based on reinforcement learning that learns link scores in an online manner. Our algorithm evaluates hyperlinks most proﬁtable to follow over the long run, and selects the most promising link based on this estimation. To properly model the crawling environment as an MDP, we propose new feature representations of states (Web pages) and actions (next link selection) considering both content information and the link structure. A number of pages and links are generalized with the proposed features. Based on this generalization, we use a linear function approximation with gradient descent to estimate value functions, i.e., link scores. We investigate the trade-oﬀ between synchronous and asynchronous methods to maintain action values (link scores) in the frontier that are computed at diﬀerent time steps. As an improved asynchronous method, we propose moderated update to reach a balance between action-values updated at diﬀerent time steps. We compare the performance of a crawling task with and without learning. Crawlers based on reinforcement learning show better performance for various target topics. Our experiments demonstrate that reinforcement learning allows to estimate longterm link scores and to eﬃciently crawl relevant pages. The work presented in that chapter is published at the ICWE 2018 conference [51].
In Chapter 5, we continue our discussion with another very large domain, an inﬂuence maximization problem. Given a social network, the inﬂuence maximization problem is to choose an optimal initial seed set of a given size to maximize inﬂuence under a certain information diﬀusion model such as the independent cascade (IC) model, the linear threshold (LT) model, etc. We extend the classical IM problem with incomplete knowledge of graph structure and topic-based user’s interest. Assuming that the graph structure is incomplete or can change dynamically, we address a topic-based inﬂuence maximization problem for an unknown graph. In order to know a part of the graph structure and discover potentially promising nodes, we probe nodes that may have a big audience group. Then, we ﬁnd the most inﬂuential seeds to maximize topic-based inﬂuence by using reinforcement learning. As we select seeds with a long-term impact in the inﬂuence maximization problem, action values in the reinforcement learning signify how good it is to take an action in a given state over the long run. Thus we learn action values of nodes from interaction with the environment by reinforcement learning. For this, nodes are generalized with some features that represent a node’s proper information and relation information with respect to surrounding nodes. We deﬁne states and actions based on these features, and we evaluate action value for each probed node and select a node with the highest action value to activate.
Finally, in Chapter 6, we discuss various interesting directions for future work. Then, we conclude this thesis with some additional remarks.
8

Part I Literature Review
9

Chapter 2
Reinforcement Learning
Reinforcement learning [94] is learning from interaction with an environment to achieve a goal. It is a powerful framework to solve sequential decision-making problems. The agent discovers which actions produce the greatest reward by experiencing actions and learns how good it is for the agent to be in a state over the long run, called the value of state, or how good it is to take a certain action in a given state over the long-term, quantiﬁed by the value of action. Reinforcement learning aims to maximize the total reward in the long run. Rewards are given immediately by selecting an action but values of states (or actions) must be estimated from an agent’s experience. Since states (or actions) with the highest values can bring about the greatest amount of reward over the long run, we are most concerned with the value of state (or action) when making decisions.
In this chapter, we start with Markov decision processes (MDPs) which are a key formalism for reinforcement learning. Then, we describe the fundamental methods for solving MDP problems, such as dynamic programming (DP) and temporaldiﬀerence (TD) methods. These methods based on tabular forms can be extended into function approximation methods that can be applied to much larger problems. The remaining sections present some important topics or improvements from the literature.
2.1 Markov Decision Processes
The notion of Markov Decision Process (MDP) underlies much of the work on reinforcement learning. An MDP is deﬁned as a 4-tuple M = S, A, R, T where S is a set of states, A is a set of actions, R : S × A → R is a reward function, and T : S × A × S → [0, 1] is a transition function. The reward function returns a single number, a reward, for an action selected in a given state. The transition function speciﬁes the probability of transition from state s to state s on taking action a (denoted T (s, a, s ) or, simply, Pr(s | s, a)). A ﬁnite MDP is an MDP in which the sets of states S, actions A, and rewards R have a ﬁnite number of elements.
An entity that learns and makes decisions is called the agent and everything outside the agent is called the environment. The agent learns from continual interaction with an environment to achieve a goal. The agent selects actions and the environment responds to these actions by giving a reward and presenting new state.
11

The objective of the agent is to maximize the total amount of reward it receives in the long run.
We usually describe the interaction between the agent and the environment with a sequence of discrete time steps. At time step t, the agent is given a state st and selects an action at on the basis of the current state. At time step t + 1, the agent receives a reward rt+1 and new state st+1 as a result of taking action at.
In the MDP environment, a state should retain all relevant information, though we are not concerned with the complete history of states that led to it. We say that the state has the Markov property: if the state is a Markov state, then the environment’s response at time t + 1 depends only on the state and action representations at time t. This Markov property enables to predict the next state and reward given the current state and action. Relevant information about states and actions are typically summarized in a compact form.
A policy π : S × A → [0, 1] maps states to probabilities of selecting an action. The policy π represents the agent’s action selection in a certain state s. In any MDP, there is a policy that is better than or equal to all other policies for all states. This is called an optimal policy, denoted π∗. The goal of the agent is to ﬁnd an optimal policy π∗ that maximizes the total reward in the long run.

Rewards and Values. At each time step, the agent receives a reward from the environment as a consequence of its behavior. The objective of the agents is to maximize the total amount of reward it receives in the long run. To achieve the objective, the agent has to estimate the expected total amount of reward starting from a state, called the value of state. While a reward characterizes how good the action is in an immediate sense, a value function of state measures how good it is for the agent to be in a given state over the long run. A reward is given directly from the environment but a value function must be learned from the agent’s experience. In reinforcement learning, when making decisions, the agent does not focus on immediate rewards but on values, i.e., cumulative reward in the long run. A state might yield a low immediate reward but it does not mean that the following state also brings about a low reward. If a state with a low reward is followed by some states that yield high rewards, it has a high value. Thus, the agent has to follow states with the highest values not the highest immediate rewards because those states bring about the greatest amount of reward over the long run.

Value Functions. For each time step, the agent selects an action and then learns from its experience. By repeated action-selection behavior, the agent learns the value of being in a state and taking an action in a state. The value functions are deﬁned with respect to policies. The value of a state s under a policy π, denoted vπ(s), is deﬁned as the expected future rewards when starting from state s and following policy π, using a discount factor 0 ≤ γ ≤ 1 (usually, 0 < γ < 1):

∞

vπ(s) =. Eπ

γkrt+k+1 | st = s

k=0

(2.1)

The discount factor γ determines the present value of future rewards. If γ = 0, the agent is only concerned with the immediate reward. The agent’s action inﬂuences

12

only the current reward. If γ approaches 1, the agent considers future rewards more strongly on its action.
The value function can be computed recursively:

vπ(s) = Eπ [rt+1 + γvπ(st+1) | st = s]

= π(a | s) Pr(s | s, a) [R(s, a) + γvπ(s )]

a

s

Similarly, the optimal state-value function, denoted v∗, is deﬁned as:

(2.2) (2.3)

v∗(s) = max Pr(s | s, a) [R(s, a) + γv∗(s )] a s

(2.4)

The value of taking an action a in a state s under a policy π, denoted qπ(s, a), is also deﬁned in the same way and computed recursively:

∞

qπ(s, a) =. Eπ

γkrt+k+1 | st = s, at = a

k=0

= Eπ [rt+1 + γqπ(st+1, at+1) | st = s, at = a]

(2.5) (2.6)

= Pr(s | s, a) R(s, a) + γ π(s , a )qπ(s , a )

s

a

(2.7)

The optimal action-value function, denoted q∗, is similarly described with a recursive deﬁnition:

q∗(s, a) = Pr(s | s, a) R(s, a) + γ max q∗(s , a ) a s

(2.8)

2.2 Dynamic Programming
Dynamic programming (DP) is a collection of algorithms that assume a complete and perfect model of the environment’s dynamics as an MDP and compute optimal policies using the model. The model of the environment’s dynamics means the transition function and the reward function. Since DP requires a prior knowledge of a complete model and a great computational expense, DP is of limited applicability but it is an essential foundation of reinforcement learning. DP algorithms use value functions to obtain optimal policies. We present two fundamental DP methods, policy iteration and value iteration, in the following subsections.

2.2.1 Policy Iteration
Policy iteration consists of two processes, policy evaluation and policy improvement. Policy evaluation computes the value functions consistent with a given policy and policy improvement makes the policy greedy with respect to the value function obtained in policy evaluation.
13

Policy Evaluation In policy evaluation, the state-value function vπ is computed for an arbitrary policy π. We assume that the environment’s dynamics are completely known. The state-value function vπ can be obtained iteratively by using the Bellman equation.

vk+1(s) =. Eπ [rt+1 + γvk(st+1) | st = s]

= π(a | s) Pr(s | s, a) [R(s, a) + γvk(s )]

a

s

(2.9) (2.10)

where π(a | s) is the probability of taking action a in state s under policy π. The sequence (vk) converges to vπ as k → ∞ if γ < 1 or eventual termination is guaranteed from all states under the policy π [94]. The iteration is stopped when the changes of value functions are lower than a threshold θ. The iterative policy evaluation algorithm is shown as follows:

Algorithm 1 Iterative policy evaluation

1: Input: π, the policy to be evaluated

2: Initialize an array V (s) = 0, for all s ∈ S

3: repeat

4: ∆ ← 0

5: for each s ∈ S do

6:

v ← V (s)

7:

V (s) ← a π(a | s) s Pr(s | s, a) [R(s, a) + γV (s )]

8:

∆ ← max(∆, |v − V (s)|)

9: end for

10: until ∆ < θ (a small positive number)

11: Output V ≈ vπ

Policy Improvement We obtained the value function vπ under an arbitrary deterministic policy π from the policy evaluation step. In the policy improvement method, we consider whether it is better to change the current policy π to the new policy π . For example, for some state s, we keep following the current policy π(s) or select an action a = π(s). If it is better to select a in s and thereafter follow the existing policy π than it would be to follow π all the time, we have to change the current policy π to new policy π with π (s) = π(s). This is generalized in the policy improvement theorem.
Theorem 1 (policy improvement theorem). Let π and π be any pair of deterministic policies such that, for all s ∈ S, qπ(s, π (s)) ≥ vπ(s). Then the policy π must be as good as, or better than, π. That is, it must obtain greater or equal expected return from all states s ∈ S: vπ(s) ≥ vπ(s).
According to the policy improvement theorem, we can deﬁne the new greedy
14

policy π by

π (s) =. arg max qπ(s, a)
a
= arg max E [rt+1 + γvπ(st+1) | st = s, at = a]
a

= arg max Pr(s | s, a) [R(s, a) + γvπ(s )]

a

s

(2.11) (2.12) (2.13)

The new policy π improves on an original policy π by greedily taking actions according to value function vπ. The deterministic policy π we have seen above can be extended to the general form, a stochastic policy that is speciﬁed by probabilities π(a | s) for taking each action a in each state s.

Policy Iteration In the policy iteration method, we repeat policy evaluation and
policy improvement until convergence to an optimal policy and optimal value func-
tion. Given an arbitrary policy π, we compute value function vπ and improve the policy π with respect to value function vπ to yield a better policy π . Then we compute again vπ and improve π to get a better policy π and so on. A complete algorithm is as follows:

Algorithm 2 Policy Iteration (using iterative policy evaluation)

1: V (s) ∈ R and π(s) ∈ A(s) arbitrarily for all s ∈ S

2:

3: // 1. Policy Evaluation

4: repeat

5: ∆ ← 0

6: for each s ∈ S do

7:

v ← V (s)

8:

V (s) ← s Pr(s | s, π(s)) [R(s, a) + γV (s )]

9:

∆ ← max(∆, | v − V (s) |)

10: end for

11: until ∆ < θ (a small positive number)

12:

13: // 2. Policy Improvement

14: policy-stable ← true

15: for each s ∈ S do

16: old-action ← π(s)

17: π(s) ← arg max s Pr(s | s, a) [R(s, a) + γV (s )] a
18: If old-action = π(s), then policy-stable ← f alse

19: end for

20: If policy-stable, then stop and return V ≈ v∗ and π ≈ π∗; else go to 4

2.2.2 Value Iteration
In policy iteration, each iteration involves policy evaluation that repeatedly sweeps through the state space. The policy evaluation step can be truncated without losing
15

the convergence guarantees of policy iteration [94]. When policy evaluation is used just once, we call this value iteration. Then we combine the policy evaluation and the policy improvement steps in a simple update operation:

vk+1(s) =. max E [rt+1 + γvk(st+1) | st = s, at = a] a = max Pr(s | s, a) [R(s, a) + γvk(s )] a s

(2.14) (2.15)

In Eq. (2.15), an action is greedily selected with respect to the current value function and the selected greedy action is used to update the value function. A complete algorithm of value iteration is as follows:

Algorithm 3 Value Iteration

1: Initialize array V arbitrarily (e.g., V (s) = 0 for all s ∈ S)

2: repeat

3: ∆ ← 0

4: for each s ∈ S do

5:

v ← V (s)

6:

V (s) ← maxa s Pr(s | s, a) [R(s, a) + γV (s )]

7:

∆ ← max(∆, | v − V (s) |)

8: end for

9: until ∆ < θ (a small positive number)

10: Output a deterministic policy, π ≈ π∗, such that 11: π(s) ← arg max s Pr(s | s, a) [R(s, a) + γV (s )]
a

2.2.3 Interaction between Policy Evaluation and Policy Improvement.
In policy iteration, two processes, policy evaluation and policy improvement, alternate. When one process completes then the other process begins. Policy evaluation makes the value function consistent with the current policy. Then the updated value function is used to improve the policy. In policy improvement, the policy is changed with respect to the current value function. As the interaction of two processes continues, the policy and the value function move toward the optimal. When the policy and the value function are not changed by either process then they are optimal. The interaction between the policy evaluation and policy improvement processes underlies almost all reinforcement learning methods [94]. Whereas policy iteration separates them as two diﬀerent processes, value iteration merges them in one process.
2.3 Temporal-Diﬀerence Methods
While dynamic programming (DP) in section 2.2 needs a complete and accurate model of the environment, temporal-diﬀerence (TD) methods do not require prior
16

knowledge about the environment’s dynamics. They compute value functions using raw experience in an on-line, fully incremental manner. For example, at time t, if the agent takes action at in state st under policy π, the action causes a transition to st+1 with reward rt+1. With this experience, the TD method updates the value function of st by:

V (st) ← V (st) + α [rt+1 + γV (st+1) − V (st)]

(2.16)

The quantity in brackets in the update is called the TD error. δt =. rt+1 + γV (st+1) − V (st)

(2.17)

It is the diﬀerence between the current estimated value of st and the better estimated value based on the actual observed reward and the estimated value of the next state, st+1, i.e., rt+1 + γV (st+1). As value functions are repeatedly updated, the errors are reduced. Here, α is a positive fraction such that 0 < α ≤ 1, the step-size parameter that inﬂuences the rate of learning. When α = 1, the agent considers only the most recent information for learning. If α is properly reduced over time, the function converges [94].
TD methods are the most widely used methods due to their several advantages such as computational simplicity, on-line learning approach, and learning directly from experience generated from interaction with an environment.
In TD methods, each iteration of value updates is based on an episode, a sequence of state transitions from a start state to the terminal state (or, in some cases, till some other condition has been reached, for example, the limited number of time steps, etc.). For example, at time t, in state s, the agent takes an action a according to its policy, which results in a transition to state s . At time t + 1 in the successor state of s, state s , the agent takes its best action a followed by a transition to state s and so on until the terminal state. Each episode starts in a starting state or any randomly selected state and ends in the terminal state.
A complete algorithm for the TD method is shown in Algorithm 4.

Algorithm 4 Tabular TD for estimating vπ

1: Input: the policy π to be evaluated

2: Initialize V (s) arbitrarily (e.g., V (s) = 0, for all s ∈ S)

3: repeat for each episode

4: Initialize s

5: repeat for each step of episode

6:

a ← action given by π for s

7:

Take action a, observe r, s

8:

V (s) ← V (s) + α [r + γV (s ) − V (s)]

9:

s←s

10: until s is terminal

11: until

We now turn to action-value functions rather than state-value functions. There are two main approaches for learning qπ in TD methods: on-policy and oﬀ-policy.
17

On-policy TD method: Sarsa First we consider an on-policy TD method called Sarsa [94]. In on-policy method, we estimate qπ(s, a) for the current behavior policy π and change π toward greediness with respect to qπ. This method learns actionvalues based on transitions from a state-action pair to a state-action pair. Since it uses a tuple of transition experience (st, at, rt+1, st+1, at+1)for each update, it is named Sarsa and an action value is updated by:

Q(st, at) ← Q(st, at) + α [rt+1 + γQ(st+1, at+1) − Q(st, at)]

(2.18)

The pseudocode of Sarsa is given in Algorithm 5. An action can be greedily selected all the time, called greedy policy. Alternatively, most of the time, the agent selects an action with the highest estimated value, but with small probability selects an action uniformly at random, called -greedy policy that is one of the most commonly used methods (see Section 2.5 for other action selection methods).

Algorithm 5 Sarsa (on-policy TD) for estimating Q ≈ q∗

1: Initialize Q(s, a) for all s ∈ S, a ∈ A(s), arbitrarily, and Q(terminal-state, ·) = 0

2: repeat for each episode 3: Initialize s

4: Choose a from s using policy derived from Q (e.g., -greedy)

5: repeat for each step of episode

6:

Take action a, observe r, s

7:

Choose a from s using policy derived from Q (e.g., -greedy)

8:

Q(s, a) ← Q(s, a) + α [r + γQ(s , a ) − Q(s, a)]

9:

s←s

10:

a←a

11: until S is terminal

12: until

If all state-action pairs are visited inﬁnitely often, Sarsa converges with probability 1 to an optimal policy and action-value function.

Oﬀ-policy TD method: Q-learning Now we consider an oﬀ-policy TD method called Q-learning [97]. This method learns the optimal action-value, regardless of the policy being followed. This method is deﬁned by:

Q(st, at) ← Q(st, at) + α rt+1 + γ max Q(st+1, a) − Q(st, at) a

(2.19)

A minimal requirement of convergence to the optimal policy is that all state-action pairs are visited an inﬁnite number of times. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to the optimal action-values q∗ [94].
The pseudocode of Q-learning is given in Algorithm 6.

On-policy and Oﬀ-policy. An important challenge of reinforcement learning is the exploration–exploitation dilemma. The agent has to learn the optimal policy
18

Algorithm 6 Q-learning (oﬀ-policy TD) for estimating π ≈ π∗

1: Initialize Q(s, a) for all s ∈ S, a ∈ A(s), arbitrarily, and Q(terminal-state, ·) = 0 2: repeat for each episode

3: Initialize s

4: repeat for each step of episode

5:

Choose a from s using policy derived from Q (e.g., -greedy)

6:

Take action a, observe r, s

7:

Q(s, a) ← Q(s, a) + α [r + γ maxa Q(s , a) − Q(s, a)]

8:

s←s

9: until S is terminal

10: until

while behaving non-optimally, i.e., by exploring all actions. This dilemma brings about two main approaches for learning action values: on-policy and oﬀ-policy.
In on-policy methods, the agent learns the best policy while using it to make decisions. On the other hand, oﬀ-policy methods separate it into two policies. That is, the agent learns a policy diﬀerent from what currently generates behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. Since learning is from experience “oﬀ” the target policy, these methods are called oﬀ-policy learning. The on-policy methods are generally simpler than oﬀ-policy methods but they learn action values not for the optimal policy, but for a near-optimal policy that still explores [94]. The oﬀpolicy methods learn the optimal policy and they are considered more powerful and general but they are often of greater variance and are slower to converge [94].
While on-policy methods learn policies depending on actual behavior, oﬀ-policy methods learn the optimal policy independent of agent’s actual behavior, i.e., the policy actually used during exploration. In Sarsa, it updates action values using a value of the current policy’s action a in next state s . In Q-learning, it updates its action-value using the greedy (or optimal) action a of next state s but the agent selects an action by -greedy policy. In Q-learning, the target policy is the greedy policy and the behavior policy is -greedy policy.

2.4 Function Approximation Methods
Many classical reinforcement-learning algorithms have been applied to small ﬁnite and discrete state spaces and value functions are represented using a tabular form that stores the state(-action) values in a table. In such small and discrete problems, a lookup table represents all state-action values of a learning space. However, in many realistic problems with large and continuous state spaces, there are many more states than could possibly be entries in a table. In such problems, a major challenge is to represent and store value functions. Thus, the tabular methods typically used in reinforcement learning have to be extended to apply to such large problems, for example, using function approximation methods. The approximate value function is represented as a parameterized functional form with weight vector w ∈ Rd. vˆ(s, w) ≈ vπ(s) denotes the approximate value of state s given weight
19

vector w.
First we consider the squared diﬀerence between the approximate value vˆ(s, w) and the true value vπ(s) over the state space, i.e., the Mean Squared Value Error. Then, we study gradient-descent methods to minimize the error. Finally, we introduce linear function approximation based on the gradient-descent method.

Mean Squared Value Error. In tabular methods, learning at a certain state

yields an update to the state’s value function, though the values of all other states

are left unchanged. That is, an update is applied only to the current state and it

does not aﬀect value functions of the other states. Each state-action value from a

lookup table represents the true value function of one state-action pair. However, in

approximation methods, the number of states is larger than the number of weights,

the dimensionality of weight vector w. Thus, an update at one state aﬀects the

estimated values of many other states and it is not possible that all state values

are correctly estimated [94]. Updating at one state makes its estimated value more

accurate, but it may make values of other states less correct because the estimated

values of other states are changed as well. Hence, in the function approximation,

rather than trying to make zero error of value functions for all states, we aim to

balance the errors in diﬀerent states [94]. For that, it is necessary to specify a state

weighting or distribution µ(s) ≥ 0, s µ(s) = 1 in order to represent how much we care about the error in each state s [94]. For example, the fraction of time spent in s

may be used as µ(s). The squared diﬀerence between the approximate value vˆ(s, w)

and the true value vπ(s) is averaged with weighting over the state space by µ. The Mean Squared Value Error, denoted V E, is obtained:

V E(w) =. µ(s) [vπ(s) − vˆ(s, w)]2

(2.20)

s∈S

By the square root of V E, we can measure roughly how much the approximate values diﬀer from the true values [94].

Gradient-Descent Methods. We consider gradient-descent methods to minimize the mean squared error (Eq. (2.20)) on the observed data. The gradient-descent methods are commonly used in function approximation. The approximate value function vˆ(s, w) is a diﬀerentiable function of the weight vector w =. (w1, w2, . . . , wd) for all s ∈ S. We assume that all the states are encountered equally in learning. At each time step, we update the weight vector w. By the gradient descent method, the weight vector w is changed by a small amount in the direction that minimizes the V E, the error between true value function under policy π, vπ(s) and the approximate value function vˆ(s, w).

wt+1

=.

wt

−

1 α∇
2

[vπ (st )

−

vˆ(st,

wt)]2

= wt + α [vπ(st) − vˆ(st, wt)] ∇vˆ(st, wt)

(2.21)

where α is a positive step-size parameter and ∇vˆ(st, wt) is the vector of partial derivatives with respect to the elements of the weight vector:

∇vˆ(st, wt) =.

∂vˆ(st, wt) , ∂vˆ(st, wt) , . . . , ∂vˆ(st, wt)

∂w1

∂w2

∂wd

.

(2.22)

20

If the update is done on a single example, the update is called a ’stochastic’ gradient-descent update. When more than one example is used for an update, the gradient-descent method is called ’batch’ and the batch update is obtained as follows:

wt+1 =. wt + α [[vπ(si) − vˆ(si, wt)] ∇vˆ(si, wt)]
i

(2.23)

where si is ith state among all input states. The pseudocode of gradient TD is given below.

Algorithm 7 Gradient TD

1: Input: the policy π to be evaluated

2: Input: a diﬀerentiable function vˆ : S × Rd → R such that vˆ(terminal, ·) = 0

3: Initialize value-function weights w arbitrarily (e.g., w = 0)

4: repeat for each episode:

5: Initialize s

6: repeat for each step of episode:

7:

Choose a ∼ π(· | s)

8:

Take action a, observe r, s

9:

w ← w + α [r + γvˆ(s , w) − vˆ(s, w)] ∇vˆ(s, w)

10:

s←s

11: until s is terminal

12: until

Linear Function Approximation based on Gradient-Descent Method. The

approximate value function of state s, vˆ(s, w), is commonly represented in a linear

function with the weight vector of features, called

vector w ∈ Rd. State s a feature vector, x(s)

is =.

represented with (x1(s), x2(s), . . . ,

a real-valued xd(s)). Each

xi(s) is the value of a function xi : S → R and the value is called a feature of s. The

functions xi are also called basis functions in a linear function because they form

a linear basis for the set of approximate functions [94]. Thus, vˆ(s, w) is a linear

function of features of the state s, with the weight vector w. In linear methods,

if the feature vector, x(s) is a d-dimensional vector, the weight vector, w is also a

d-dimensional vector. The feature vector, x(s), and the weight vector, w, have the

same number of elements. Then the state-value function is approximated by the

inner product between w and x(s):

d
vˆ(s, w) =. w x(s) =. wixi(s)
i=1

(2.24)

This approximate value function is linear in the weights and we refer to it as a linear function approximator.

The gradient-descent methods above are commonly used in linear function approximation. The gradient-descent–based update in state st is:

wt+1 =. wt + α [rt+1 + γvˆ(st+1, wt) − vˆ(st, wt)] ∇vˆ(st, wt)

(2.25)

21

where ∇vˆ(s, w) (Eq. (2.21)), the gradient of the approximate value function with respect to w, is x(s).
We can extend the state-value function approximation, sˆ(s, w), to action-value function approximation, qˆ(s, a, w) ≈ q∗(s, a). Like the state-value function, the action-value function is approximated by linearly combining feature vector x(s, a) and weight vector w:

d
qˆ(s, a, w) =. w x(s, a) =. wixi(s, a)
i=1

(2.26)

and the gradient-descent update based on Sarsa method is: wt+1 =. wt + α [rt+1 + γqˆ(st+1, at+1, wt) − qˆ(st, at, wt)] ∇qˆ(st, at, wt).

(2.27)

Pseudocode for the complete algorithm is given as below.

Algorithm 8 Gradient Sarsa for Estimating qˆ ≈ q∗

1: Input: a diﬀerentiable function qˆ : S × A × Rd → R 2: Initialize value-function weights w ∈ Rd arbitrarily (e.g., w = 0)

3: repeat for each episode:

4: s, a ← initial state and action of episode (e.g., -greedy)

5: repeat for each step of episode:

6:

Take action a, observe r, s

7:

if s is terminal: then

8:

w ← w + α [r − qˆ(s, a, w)] ∇qˆ(s, a, w)

9:

Go to next episode

10:

end if

11:

Choose a as a function of qˆ(s , ·, w) (e.g., -greedy)

12:

w ← w + α [r + γqˆ(s , a , w) − qˆ(s, a, w)] ∇qˆ(s, a, w)

13:

s←s

14:

a←a

15: until

16: until

Feature Construction. In linear function approximation, as we have seen before, the value is obtained by sums of features times corresponding weights. Its computation relies on features. Appropriate features help to correctly estimate values, but, if the features are selected improperly, it may cause poor performance.
Features should represent the state space of the environment and convey the information necessary to learn the environment’s dynamics. Selecting appropriate features, i.e., feature engineering, remains a challenge because it requires domainspeciﬁc knowledge and a great engineering eﬀort. Representational design is based only on the system designer’s knowledge and intuition. In addition to the engineering problem, the linear form itself has a limitation that it cannot take into account any interactions between features [94]. For example, feature i can be good or bad depending on feature j. Linear methods assume that each feature is linearly independent of other features. Even with careful engineering, it is not possible for a
22

system designer to choose features with considering all interaction between features. Several works [82, 35, 36] have addressed this problem to construct features automatically. These methods are based on errors of the value function and add features that help improve the value estimation.
Geramifard et al. [35] introduce incremental Feature Dependency Discovery (iFDD) as an online feature expansion method in the context of a linear function approximation. Their method gradually creates features that help eliminate error of the value function approximation. The process begins with an initial set of binary features. Their method identiﬁes all conjunctions of existing features as potential features and increases the relevance of each potential feature by the absolute approximation error. If a potential feature’s relevance exceeds a predeﬁned threshold, the feature is added to the pool of features used for future approximation.
The authors extend iFDD to the batch setting in [36] and prove that BatchiFDD is a Matching Pursuit (MP) algorithm with its guaranteed rate of error-bound reduction. Like iFDD, Batch-iFDD does not require a large pool of features at initialization but expands the pool of potential features incrementally that are the conjunction of previously selected features. Batch-iFDD runs the least-squares TD (LSTD) algorithm to estimate the TD-error over all samples and then adds the most relevant feature to the feature set. Their empirical results show that Batch-iFDD outperformed the previous state of the art MP algorithm.
Even though features are constructed in an online manner and these methods overcome an imperfect initial selection of features, it is still crucial to provide good initial features because their feature constructions are based on the initial features.

2.5 Exploration
To maximize total reward, the agent must select the action with highest value (exploitation), but to discover such action it has to try actions not selected before (exploration). This exploration enables to experience other actions not taken before and it may increase the greater total reward in the long run because we would discover better actions. The trade-oﬀ between exploitation and exploration is one of the challenges in reinforcement learning [94]. We present three well-known exploration methods.

-greedy. The -greedy strategy is one of the most commonly used method. Most of the time, the agent selects an action with the highest estimated value, but with small probability selects an action uniformly at random. The drawback of -greedy is to choose equally among all actions.

Softmax. One alternative is the softmax (or Boltzmann) method. It gives weights to actions according to their value estimates. The good actions have exponentially higher probabilities of being selected. An action a is chosen with probability:

p=

eQ(s,a)/τ a eQ(s,a )/τ

(2.28)

23

where τ > 0, the temperature, is used for the degree of exploration. When τ → ∞, actions are selected randomly. When τ approach 0, actions are selected greedily.
It is not clear whether softmax action selection or -greedy action selection is better [94]. It depends on the task and on heuristics. parameter is easy to set with conﬁdence but setting τ requires knowledge of the likely action values and of powers of e [94].
Optimistic Value Initialization. Another method commonly used in modelbased learning (see Section 2.6) is optimistic value initialization, such as in the R-max method that gives the maximum reward rmax to unknown state-actions [9, 11, 28]. It encourages the agent to explore all states. Known and unknown stateactions are classiﬁed by the number of visits. For each time step, the agent behaves greedily. Another similar method is to add exploration bonuses for states with higher potential of learning [5] or with higher uncertainty [62, 63, 68, 69].

2.6 Model-Based and Model-Free Methods
In reinforcement learning, there are two main approaches to estimate state-action values: model-based and model-free. Model-based methods require a model of the environment such as dynamic programming and model-free methods learn without a model such as temporal-diﬀerence (TD) methods. A model simulates the environment’s dynamics and it allows to inference how the environment will behave. The model signiﬁes the transition function and the reward function in an MDP.

Model-based methods. Model-based methods learn the transition and reward models from interaction with the environment, and then use the learned model to calculate the optimal policy by value iteration. That is, a model of the environment is learned from experience and value functions are updated by value iteration over the learned model. By learning a model of the environment, an agent can use it to predict how the environment will respond to its actions, i.e., predict next state and next reward given a state and an action. If an agent learns an accurate model, it can obtain an optimal policy based on the model without any additional experiences in the environment. Model-based methods are more sample-eﬃcient than model-free methods but exhaustive exploration is often necessary to learn a perfect model of the environment [53]. However, learning a model allows the agent to perform targeted exploration. If some states are not visited enough or uncertain to learn a model correctly, this insuﬃciency of information drives the agent explore more those state. Thus, optimistic value initialization is commonly used for exploration method (see Section 2.5). If the agent take an action a in state s, the action value Q(s, a) is updated by the Bellman equation:

Q(s, a) =. R(s, a) + γ Pr(s | s, a) max Q(s , a ) a s

(2.29)

where 0 ≤ γ < 1 is the discount rate that determines the present value of future rewards. If γ = 0, the agent is only concerned with the immediate reward. The agent’s action inﬂuences only the current reward. If γ approaches 1, the agent

24

considers future rewards more strongly on its action. The optimal value function Q∗ can be obtained by iterating on the Bellman equation until it converges [94].
Transition and reward models are commonly learned by a maximum likelihood model. Suppose C(s, a) is the number of times that action a is taken in state s and C(s, a, s ) is the number of times that taking action a in state s transitions to state s . The transition probability from (s, a) pair to state s is obtained by:

Pr(s |s, a) = T (s, a, s ) = C(s, a, s )/C(s, a)

(2.30)

The reward model for (s, a) pair is also computed in a similar way:

R(s, a) = RSUM (s, a)/C(s, a)

(2.31)

where RSUM (s, a) is the sum of rewards that the agent receives when taking action a in state s. The most well-known model-based algorithm is R-MAX [9]. Algorithm 9 shows the pseudocode of R-MAX.

Algorithm 9 R-MAX

1: sr : an absorbing state

2: for all a ∈ A do

3: R(sr, a) ← Rmax 4: T (sr, a, sr) ← 1 5: end for

6: repeat

7: Choose a = π(s)

8: Take action a, observe reward r and next state s

9: C(s, a, s ) ← C(s, a, s ) + 1

10: C(s, a) ← C(s, a) + 1

11: RSUM (s, a) ← RSUM (s, a) + r

12: if C(s, a) ≥ m then

13:

R(s, a) ← RSUM (s, a)/C(s, a)

14:

for all s ∈ C(s, a, ·) do

15:

T (s, a, s ) ← C(s, a, s )/C(s, a)

16:

end for

17: else

18:

R(s, a) ← Rmax

19:

T (s, a, sr) ← 1

20: end if

21: Update Q-values using VI

22: s ← s

23: until converge

Most model-based methods assume a Dynamic Bayesian Network (DBN) transition model, and that each feature transitions independently of the others [54]. In Section 3.3, we study the factored MDP problem [7, 8, 23] that uses the DBN formalism and present an algorithm that learns the DBN structure of transition model.
25

Model-free methods. Model-free methods improve the value function directly from observed experience and do not rely on the transition and reward models. Value functions are learned by trial and error. These methods are simple and can have advantages when a problem is complex so that it is diﬃcult to learn an accurate model. However, model-free methods require more samples than model-based to learn value functions. When an action a is taken in state s, the agent receives a reward r, and moves to the next state s , the action value Q(s, a) based on Qlearning [98] is updated as follows:

Q(s, a) =. Q(s, a) + α r + γ max Q(s , a ) − Q(s, a) a

(2.32)

where 0 < α ≤ 1 is the step-size parameter and 0 ≤ γ < 1 is the discount rate. The step-size parameter α inﬂuences the rate of learning. When α = 1, the agent considers only the most recent information for learning.

In Section 3.2, we study a model-free method and apply Q-learning algorithm to a real taxi routing problem. In Section 3.3, we study model-based methods for a factored MDP problem.

2.7 Priority-Based Value Iteration
Value Iteration (VI) is a dynamic programming algorithm that performs complete sweeps of updates across the state space until convergence to optimal policies (see Section 2.2.2). VI is a simple algorithm but computationally expensive. If the state space is large, the computational cost of even one single sweep is also immense and multiple sweeps for convergence can cause extreme computational cost. This is due to some ineﬃciencies of updates in VI. First, some updates are useless. VI updates the entire state space at each iteration even though some updates do not change value functions. In fact, if a state value is changed after its update, then the values of its predecessor states are also likely to be changed but the values of other states that do not lead into the state remain unchanged. Updating these states have no eﬀect. Second, updates are not performed in an optimal order. Performing updates on a state after updating its successors can be more eﬃcient than that in an arbitrary order. That is, it is better to propagate backward from any state whose value has changed to states that lead into the state. If the updates are not performed in a good order, some states may need to be updated redundantly to converge.
When the DP algorithms do not sweep the state space for each iteration, we call them asynchronous (or sweepless) DP algorithms [94]. In these algorithms, the values of states can be updated in any order. Asynchronous DP methods give great ﬂexibility in selecting states to update [94]. Algorithms do not need to get locked into long sweeps over the entire state space and by the ﬂexibility of selecting the states we can improve the convergence rate. Eventual convergence of asynchronous DP algorithms is guaranteed as long as the algorithms continue to update the values of all the states. It is shown in the MDP literature that the performance of value iteration can be signiﬁcantly improved by ordering updates intelligently rather than by arbitrary order updates.
26

Algorithm 10 Prioritized sweeping algorithm

1: Initialize Q(s, a), Model (s, a), for all s, a, and PQueue to empty

2: loop

3: s ← current (non-terminal) state

4: a ← policy(s, Q)

5: Execute action a, observe resultant reward, r, and state, s

6: Model (s, a) ← r, s

7: p ← |r + γ maxa Q(s , a) − Q(s, a)|

8: if p > θ then

9:

insert s, a into PQueue with priority p

10: end if

11: repeat

12:

s, a ← ﬁrst(PQueue)

13:

r, s ← Model (s, a)

14:

Q(s, a) ← Q(s, a) + α [r + γ maxa Q(s , a) − Q(s, a)]

15:

for all s¯, a¯ predicted to lead to s do

16:

r¯ ← predicted reward for s¯, a¯, s

17:

p ← |r¯ + γ maxa Q(s, a) − Q(s¯, a¯)|

18:

if p > θ then

19:

insert s¯, a¯ into PQueue with priority p

20:

end if

21:

end for

22: until n times while PQueue is not empty

23: end loop

Prioritized Sweeping (PS) is a well-known prioritization method introduced by Moore et al. [78]. The principal idea of PS is to update backwards from states which values have changed to the states that lead into them, i.e., its predecessors. Prioritizing updates that are expected to cause large value changes, it has an eﬀect of propagating values backwards from states with a relatively high state-values and it enables to reduce the number of updates. PS maintains a priority queue to order updates and Bellman error is commonly used as the priority metric. The Bellman error is the diﬀerence between the current estimated value and the estimated value after applying the Bellman operator. PS can start from any state not just a goal state. If the priority of the state-action pair is greater than a certain threshold, then the pair is stored in the queue with its priority. For each iteration, a state-action pair (s, a) with the highest priority is removed from the queue and its value function is updated. For each predecessor of (s, a), its priority value is computed. If the priority greater than some threshold then the predecessor is inserted in the priority queue. Algorithm 10 shows the Prioritized Sweeping algorithm.
Prioritized methods help to improve considerably the performance of value iteration by reducing the number of updates, but maintaining the priority queue may result in an excessive overhead. It is tackled in a diﬀerent ways in the literature such as prioritizing updates without priority queue [21], stationary update orders [20, 25], partitioning the state space [102, 101], etc.
Dai et al. [21] show that the overhead for maintaining a priority queue can be
27

greater than its advantages. The authors introduce a prioritized value iteration algorithm that does not require a priority queue. The algorithm starts with goal state and performs a breadth-ﬁrst (or depth-ﬁrst) traversal of the transpose of the state space graph. States are backed up in the order they are encountered. The order of updates is sub-optimal but its smaller overhead allows to converge faster than other existing methods.
Topological value iteration (TVI) [20] performs updates in the order of a causal relation among layers of states. While previous prioritized algorithms dynamically updating state priorities, TVI uses stationary update order. Dibangoye et al. [25] propose an improved version of TVI, iTVI. iTVI builds mapping of states space S into a metric space (S, d) and performs updates in the order of metric d, the distance from the start state to a state, which is causal relations among states measured by standard shortest-path techniques like Dijkstra.
McMahan et al. [71] merge some features of Dijkstra’s algorithm and value iteration. Their algorithm called improved Prioritized Sweeping (IPS) reduces to Dijkstra’s algorithm when given a deterministic MDP. IPS is suitable to solve short path lengths problems.
[34] also proposes a prioritized value iteration algorithm based on Dijkstra’s algorithm to solve stochastic-shortest-path problems. Diﬀerent from IPS, their method can deal with multiple start and goal state and it has guaranteed convergence to the optimal solution.
Wingate et al. [102] present three enhancements such as prioritizing updates, partitioning, and variable reordering to accelerate value and policy iteration. As a priority metric, they introduce a new metric, H2, deﬁned as the value of the state plus the Bellman error and compare it with Bellman error. The proposed prioritized and partitioned algorithm selects a high-priority partition p, update state values in the partition, and then reprioritizes any partition which depends upon anything in p. It may reorder the states in the partition such that for each sweep. The state ordering is computed during initialization.
In the Dyna [93] framework, planning process is performed in addition to learning. While learning updates the value function by interacting with the real environment, planning performs some ﬁxed number of value-function updates in a model of environment that simulates a real environment. Queue-Dyna [83] improves Dyna [93] by prioritized value-function updates in planning process. The priority is determined by the prediction diﬀerence of Q-values between a state-action pair and its successor state. If the diﬀerence is larger than a predeﬁned threshold, then the state-action pair is placed in the queue. In planning process, a state-action pair with the highest priority is removed from the queue and updated with simulated experiences. If the state-value estimates of its predecessors or successors change, its all transitions become update candidates.
PAC(Probably Approximately Correct)-MDP learning is one of the approaches to exploration in RL. Its exploration strategy guarantees that with high probability the algorithm performs near optimally for all but a polynomial number of time steps [46]. The well-known algorithms are E3 [59] and R-max [9]. In PAC-MDP algorithms, when a state-action pair is visited suﬃciently many times, it is considered as known by the agent. Whenever a new state-action pair becomes known to the agent, planning step is executed. In [46], the authors propose several approaches
28

to improve the planning step in PAC-MDP learning. They reduce the number of planning steps by extending the notion of known state-action pairs by a notion of a known state. In action value updates, BAO updates are proposed, updating only the best action of each state instead of updating all actions of a given state. An extension to the prioritized sweeping algorithm is presented which adds only policy predecessors to the priority queue instead of all predecessors of a given state. In [47], they analyze theoretically and empirically on prioritization of Bellman updates. They show empirical evidence that ordering of updates in standard value iteration can signiﬁcantly inﬂuence its performance.
2.8 Non-Stationary Environment
The environment we have seen so far is assumed to be stationary, that is, the environment dynamics does not change over time. However, this is not realistic in many real-world problems. For example, in a traﬃc lights control problem, traﬃc patterns vary over time. We often encounter reinforcement learning problems that are eﬀectively non-stationary [94]. Non-stationary problems are the most common in reinforcement learning. In non-stationary problems, transition and reward probabilities are time-dependent. The true values of the actions and the agent’s policy change over time. In such cases, since the estimates continue to vary in response to the most recently received rewards, one of the most popular methods is to use a constant step-size parameter so that it can give more weight to recent rewards than to long-past rewards [94].
In fact, even in the stationary environment, the problems are non-stationary in the early stage of learning because the value of actions are undergoing learning. As the agent interacts with its environment, it learns incrementally the good policies and value functions from each observed experience. As we have discussed in Section 2.2.3, when the value function is changed for the agent’s current policy, the policy is also improved with respect to the current value function. Notwithstanding the nonstationarity in the early learning, as the interaction between two processes continues, value functions and policies become optimal.
When we use the traditional RL methods in non-stationary environment, it can cause an ineﬃciency in learning. The agent keeps track of dynamics changes and learns good policies corresponding to the current dynamics. When the environment dynamics changes, the knowledge that has been learned becomes useless and the agent has to learn the new dynamics. The problem is that even if the environment reverts to the previously learned dynamics, the agent has to learn it again because the learned knowledge in the past is not laid aside. Some previous works have been addressed non-stationary problems. In most proposed methods, it is assumed some degree of regularity in dynamics changes, and that the changed dynamics last long enough so that the agent can learn the changes.
Choi et al. [18] introduce hidden-mode Markov decision process (HM-MDP) that are a subclass of partially observable Markov decision processes (POMDP) to solve reinforcement learning problems in non-stationary environments. A hidden-mode model is deﬁned as a ﬁnite set of MDPs that share the same state space and action space, with possibly diﬀerent transition functions and reward functions. The authors
29

assume that the environment is in exactly one of the modes at any given time. Based on the hidden-mode model, a variant of the Baum–Welch algorithm is proposed to capture environmental changes and learn diﬀerent modes of the environment.
RL-CD [19] is also a method for solving reinforcement learning problems in non-stationary environments. The authors assume that the environment is nonstationary but it is divided into partial models estimated by observing the transitions and rewards. The algorithm evaluates how well the current partial model can predict the environment using a quality of a model that is a value inversely proportional to its prediction error. For each time step, the model with the highest quality is activated. If there is no model with quality higher than minimum quality threshold, a new model is created. Each environment dynamics is called a context. Whenever the currently active model is replaced, they consider a context change is detected. The algorithm starts with only one model and then incrementally creates new ones as they become necessary. In experiments, two non-stationary environments are used: ball catching and traﬃc lights control. In ball catching scenario, the movement of the ball change over time and in traﬃc scenario, three traﬃc patterns with diﬀerent car insertion distributions are used to build the non-stationarity of the environment. The experimental results show that the performance of RL-CD performs better than two traditional RL algorithms, Q-Learning and Prioritized Sweeping (PS).
TES [79] is an online framework that transfers the world dynamics in heterogeneous environments. The agent learns world models called views and collects them into a library to reuse in future tasks. A view is a decomposition of the transition function that consists of a structure component and a quantitative component. The structure component picks the features relevant to an action and the quantitative component deﬁnes how these features should be combined to approximate the distribution of action eﬀects. When the agent encounters a new task in a new environment, it selects a proper view from the library or adapts to new tasks or environments with completely new transition dynamics and feature distributions. In experiments, it is shown that TES is adapted to multi-dimensional heterogeneous environments with a small computational cost.
Rana et al. [85] apply two model-free approach, Q-learning and Q-learning with eligibility trace Q(λ), to solve the dynamic pricing problem with ﬁnite inventory and non-stationary demand. The agent aims to maximize the revenue for selling a given inventory by a ﬁxed deadline. The agent learns the demand behavior and, based on that, it optimizes their pricing strategies. In experiments, it is shown that the Q(λ) outperforms the standard Q-learning algorithm. If the initial Q-values can be set to the best estimated demand function, the learning converges faster than when no prior knowledge of demand is assumed. Q(λ) performs particularly well in situations where the demand between successive days exhibits self-correlation.
Bayesian policy reuse (BPR) [89] is a Bayesian framework to determine quickly the best policy for a novel task by reusing a policy from a library of existing policies.
BPR+ [52] extends BPR in a multi-agent setting to deal with non-stationary opponents. Thus, the tasks in BPR are opponent strategies in BPR+ and the policies in BPR are optimal policies against those stationary strategies in BPR+. While BPR assumes knowledge of all possible tasks and optimal policies is given a priori, BPR+ learns new models in an online fashion without prior knowledge. The learning agent detects that the current policies do not perform optimally and then learns
30

incorporating models of new strategies. BPR+ computes the probability of the rewards under the known models, and if this probability is lower than a threshold for some number of rounds, BPR+ considers that a new model is detected. The authors assume that the opponent will not change of strategies during a number of rounds. Then, the new model, i.e., opponent strategy, is learned by value iteration and its performance models are also updated accordingly to be able to detect switches to either a new or a previously known strategy. Experimental results show that BPR+ is capable of eﬃciently detecting opponent strategies and reacting quickly to behavior switches.
31

32

Part II Applications
33

Chapter 3
Model-Free and Model-Based Methods
3.1 Introduction
In this chapter, we study two main approaches for solving reinforcement learning problems: model-free and model-based methods.
We ﬁrst study a model-free method that learns directly from observed experiences without a model. We apply Q-learning [98], one of the widely-used model-free methods, to a real taxi routing problem with a customized exploration and exploitation strategy. In experiments, we investigate two important parameters of Q-learning – the step size and discount rate. We also investigate the inﬂuence of trade-oﬀ between exploration and exploitation on learning.
Then, we turn to a model-based method that learns transition and reward models of the environment. In particular, we address the factored MDP problem [7] where a state is represented by a vector of n variables. Most model-based methods are based on Dynamic Bayesian Network (DBN) transition models. Our algorithm learns the DBN structure including synchronic arcs and uses decision trees to represent transition functions.
3.2 Learning without Models
In this section, we study a model-free method. One of the widely-used model-free methods is Q-learning [98]. We apply Q-learning algorithm to a real taxi routing problem. We demonstrate that a reinforcement learning algorithm is able to progressively learn optimal actions for the routing to passenger pick-up points of an autonomous taxi in a real scenario at the scale of the city of Singapore. To improve action selection strategy, we present a customized exploration and exploitation strategy for the taxi problem. While model-free methods do not learn transition and reward models, they use two important parameters such as step size α and discount rate γ that inﬂuence the learning. We quantify the inﬂuence of the parameters on eﬀectiveness: step size, discount rate, and trade-oﬀ between exploration and exploitation.
35

3.2.1 Background and Related Work
Most studies addressing the taxi routing problem focus on providing the fastest route and a sequence of pick-up points [84] by mining historical data [105, 103, 104, 84]. Yuan et al. [103] uses road segments and travel time clustering to ﬁnd the fastest driving route. The authors build a landmark graph to model the traﬃc pattern and provide the time-dependent fastest route to a given destination. They then present in [104] a recommendation system for taxi drivers and passengers based on detecting parking places by clustering road segment extracted from GPS trajectories. The system recommends a parking place with high probability to get a passenger and suggests parking places or road segments where passengers can ﬁnd vacant taxis. Qu et al. [84] propose a method to recommend an entire driving route for ﬁnding passengers instead of a sequence of pick-up points. They develop a graph representation of a road network by mining the historical taxi GPS traces and generate a cost-eﬀectively optimal driving route for ﬁnding passengers. Those models rely on the availability of accurate historical data and trajectories. They might not be suitable for dynamical environments such as that of an autonomous taxi looking for optimal passenger pick-up points.
Reinforcement learning [94] has the potential to continuously and adaptively learn from interaction with the environment. Q-learning [98] is a widely used method because of its computational simplicity. In Q-learning, one does not require a model of transition functions and reward functions but learns directly from observed experience. In this study, we apply Q-learning algorithm to a real taxi routing problem.
While taxi routing has often been used as the example application for reinforcement learning algorithms, it often remained relegated to toy or small scale examples, as it is the case of the seminal 5×5 grid introduced in [26] and used for experimental purposes in [37, 53, 38, 29]. Learning pick-up points is a somewhat new application of reinforcement learning.

3.2.2 Q-learning for Taxi Routing

We assume an autonomous taxi agent does not know about the city and that the car moves completely depending on the estimated action values of reinforcement learning. The aim of this application is that the autonomous taxi decides where to go in order to pick up a passenger by learning both the values of actions given a state and the existence probability of passengers.
The learning agent takes an action a in state s, receives a reward r, and moves to the next state s . With Q-learning (see Section 2.3), the estimated value of taking action a in state s, denoted Q(s, a), is updated as:

Q(s, a) =. Q(s, a) + α r + γ max Q(s , a ) − Q(s, a) . a

(3.1)

We call an episode a series of steps until the agent ﬁnds a passenger. For the ﬁrst episode, the taxi located at a random position moves according to its policy. The episode ends when the taxi ﬁnds a passenger. Then, it moves to the passenger’s destination and starts a new episode. As the taxi moves it receives rewards and updates its action-value and the existence probability. The road network is discretized

36

Algorithm 11 Taxi Routing for Learning Pick-up Points

1: Initialize Q(s, a), existence probability of passengers p

2: repeat

3: repeat

4:

if greedy then

5:

V =. { a ∈ A | Q(s, a) ≥ maxa Q(s, a ) − η }

6:

if |V | > 1 then

7:

Select action a with highest probability p

8:

end if

9:

else /* not greedy */

10:

Select action a uniformly at random

11:

end if

12:

Take action a, obtain reward r, observe next state s

13:

Q(s, a) =. Q(s, a) + α [r + γ maxa Q(s , a ) − Q(s, a)]

14:

Increment visit count on s

15:

Update existence passenger probability p(s )

16:

if passenger found in s then

17:

Increment found count on s

18:

s becomes the end of the passenger route from s

19: 20:

else s =. s

21:

end if

22: until a passenger is found

23: until algorithm converges

and the movements correspond to steps in the discretized network. At each step, the taxi learns where passengers are likely to be located.
The Taxi Routing algorithm for learning pick-up points is outlined in Algorithm 11. According to the -greedy policy, an action a is selected in a given state s.
The action selection rule selects the action with the maximum estimated action value (greedy action). However, with this rule, the algorithm ignores other actions that, although they have slightly lesser value, may lead to a state having a higher chance to pick up a passenger. Hence, instead of selecting one greedy action, we loosen the selection condition by setting a lower bound below the maximum value in order to choose from more potentially valuable candidate actions (Line 5). The candidate actions are compared with existence probabilities of passengers in their corresponding states (Line 7). We later refer to the algorithm with this selection strategy as Q-learning using LB/Proba. This probability comparison is very eﬀective when actions share the same value (Q(s, a1) = . . . = Q(s, an)). In this case, we originally select one action at random because we consider all actions are the same. In fact, they may not be the same if one causes to move to a state with very high existence probability of passengers. Comparing probabilities reduce this kind of mistake.
After taking an action, we update the action-value in the current state s with reward r and next state s . As we visit a new state s , the visit and found counts are incremented and the existence probability of passengers is also recalculated. We
37

(a) Varying α, 12h to 13h

(b) Varying α, 14h to 15h

Figure 3.1 – Average number of steps with diﬀerent step-size α

repeat this procedure until we ﬁnd a passenger.

3.2.3 Performance Evaluation
For the sake of simplicity, in this work, we present the results for a map discretized into cells of 0.01 degree longitude × 0.01 degree latitude (about 1.1km × 1.1km) forming a 38 × 20 grid. At each cell of the grid, eight actions are possible: up, down, right, left, and diagonally. A step is the movement from one cell to an adjacent one. Although such a representation does not capture several natural constraints on the traﬃc, it is suﬃcient, with limited loss of generality, to evaluate the eﬀectiveness of the algorithm.
Since popular pick up points generally depend on the time of the day, we run the experiments for selected time intervals. Here, we present the results for two oﬀ-peak hours (12h to 13h and 14h to 15h), but we obtain comparable results for other time slots. At each episode we select 300 passengers according to actual geographical distribution in the given time interval in a dataset of taxi pickups and drop-oﬀs for a ﬂeet of one thousand taxis for one month in Singapore.
We ﬁrst look into the impact of the step-size parameter α, the discount rate γ, and the probability of exploration . We evaluate how these parameters inﬂuence the learning performance with ordinary Q-learning. We compare the average number of steps. The average steps are calculated at every 100 episode by dividing the total steps from the ﬁrst episode to the last by the total number of episodes.
Figures 3.1a–3.1b show the average number of steps with diﬀerent step-size parameter α values for diﬀerent time intervals. We compare four diﬀerent α with a ﬁxed γ (= 0.5) and (= 0.3). For all the time intervals, as the α is smaller, the average number of steps also decreases. Lower step-size values perform better. This indicates that accumulated experience aﬀects value estimation more signiﬁcant than recent experience, i.e., that the problem is indeed stochastic.
For the discount rate γ experiment, we ﬁxed α (= 0.5) and (= 0.3) and changed the γ. The average number of steps with diﬀerent γ values for diﬀerent time intervals are shown in Figures 3.2a–3.2b. In Figure 3.2b, the lowest γ (= 0.25) performs better. This means that immediate rewards are more important than future rewards.
38

(a) Varying γ, 12h to 13h

(b) Varying γ, 14h to 15h

Figure 3.2 – Average number of steps with diﬀerent discount rate γ

(a) Varying , 12h to 13h

(b) Varying , 14h to 15h

Figure 3.3 – Average number of steps with diﬀerent

In Figure 3.2a, as episodes continue, a higher γ (= 0.75) is slightly better. In Figure 3.2a, relatively longer step counts than those of the other time intervals are needed to achieve a goal. In this case, future rewards are more signiﬁcant than current rewards.
Figures 3.3a–3.3b show the average number of steps with diﬀerent values for diﬀerent time intervals, given α = 0.5 and γ = 0.5. The average number of steps for three cases ﬁrst decreases dramatically and then converges gradually. For all the time intervals, when is 0.1, the average number of steps is bigger than the other cases in early episodes but it dominates after about 30,000 episodes. At the beginning, exploration is more eﬀective and relatively inexpensive. Eventually, suﬃcient knowledge is accumulated and exploitation is worthy.
In the experiments, we saw how parameters α, γ, and behave in learning. While the optimal value of the parameter does not depend on the domain, the values of parameters α and γ depend on intrinsic properties of the state space.
We now compare Q-learning using LB/Proba (our algorithm) with ordinary Qlearning. For experiments, we select three parameter values shown in the previous section. The step-size parameter α is set to 0.25 because low learning rate is appro-
39

priate to our problem. The probability of exploration is set to 0.1. Since loosened selection for maximum action has the eﬀect of exploration, high is not needed. We take η = 0.01 to set the lower bound on the maximum action value per state. Two algorithms are compared by varying the discount rate γ value.
In Figures 3.4a–3.4d, when γ = 0.75 or 0.5, Q-learning using LB/Proba converges faster than Q-learning. On the other hand, when γ is 0.25 (Figures 3.4e–3.4f), Qlearning performs similarly well or slightly better than Q-learning using LB/Proba. These experiments show that when the learning rate α is low and the discount rate γ approaches 1, Q-learning using LB/Proba outperforms Q-learning. In other words, it has to accumulate much experience for value prediction and it considers future rewards more strongly. The reason is that Q-learning using LB/Proba depends on existence probability of passengers that requires enough experience and that is more related to long-term high rewards.
3.2.4 Demonstration Scenario
Figure 3.5 shows the initial screen of our pick-up point learning system. The red dot is a taxi and ﬂags are passengers. Passengers’ positions are based on a real dataset of Singaporean taxi trajectories.
In Figure 3.6, the user interface for simulation consists of two parts. The upper one is for the conﬁguration of simulation and the lower one is for displaying the learning on the map. Before starting the simulation, the user can conﬁgure the properties such as episode count and exploration percentage ( ). The episode count is used for consecutive executions. The exploration percentage deﬁnes how often we choose a random selection. The user starts simulation in three ways: manually move one step by ‘One Step’ button, automatically execute with a ﬁxed number of episodes by ‘Start Driving’ button, and repeat 100 times the ﬁxed number of episodes by ‘Experiment’ button. 1) Manually: Every time the user clicks the button, the taxi moves on the next cell according to the learning policy. The taxi moves are traced on the map by a green line that connects the current position and the next position (Figure 3.6). The next taxi position is displayed with a green dot. 2) Automatically: It enables continuous learning with the ﬁxed number of episodes. If the user sets to 100 episodes, the taxi does the pick-up learning 100 times. One episode means that the taxi ﬁnds a passenger. With 100 episodes, the taxi ﬁnds 100 passengers. 3) Simple experiment: We repeat 100 times the automatic learning explained above. If the user sets to 100 episodes and executes this experiment, it executes 100 (episode) × 100 (times), i.e., in total 10,000 episodes are executed. Experiment results are shown by ‘Average Steps’.
The user veriﬁes the experiment result in the interface such as the average steps and the existence probability of passengers. The average steps is calculated at every 100 episodes and it is obtained by dividing the total steps from the ﬁrst episode to the last by the total number of episodes. The user can see the average steps on the left of the ‘Experiment’ button (Figure 3.6). By clicking ‘Average Steps’, the user veriﬁes a list of average steps calculated at every 100 episodes. With this list, the user can also visualize a chart of average steps. The existence probability of passengers is also an experiment result that the user can see on the map. The probability is calculated every time the taxi moves and it is displayed on each cell
40

(a) Regular Q-learning vs LB/Proba, γ = 0.75, 12h to 13h

(b) Regular Q-learning vs LB/Proba, γ = 0.75, 14h to 15h

(c) Regular Q-learning vs LB/Proba, γ = 0.5, 12h to 13h

(d) Regular Q-learning vs LB/Proba, γ = 0.5,14h to 15h

(e) Regular Q-learning vs LB/Proba, γ = 0.25, 12h to 13h

(f) Regular Q-learning vs LB/Proba, γ = 0.25, 14h to 15h

Figure 3.4 – Regular Q-learning vs LB/Proba: Average number of steps as the number of episodes increases.

41

of the map. After experiment, the user can visualize the learned probability by a heatmap. Depending on the probability, the cells are ﬁlled in red, yellow, green, or blue color. The most probable places are in red color, less probable places in yellow or green, and the least probable places are in blue.
Through the simulation, the user can see an interesting behavior that the taxi moves inside road network areas (Figure 3.6). As experiments are repeated, the taxi traces draw features of Singaporean geography. That is obtained by reinforcement learning, not deliberately programmed in the system.
3.3 Learning Models
In the previous section, an agent improves the value function directly from observed experience and does not rely on the transition and reward functions. In contrast, model-based methods learn the transition and reward models and use these models to update value functions (see Section 2.6). Most model-based methods are based on Dynamic Bayesian Network (DBN) transition models and each feature’s transition is assumed to be independent from that of the others [7, 8, 23, 54]. In this section, we study a model-based method. In particular, we address the factored MDP problem [7, 8, 23] whose state is represented by a vector of n variables. As the size of the state spaces increases, representing MDPs with large state spaces is challenging in reinforcement learning. Factored MDPs using the DBN formalism is one approach to represent large MDPs compactly. We propose an algorithm that learns the DBN structure including synchronic arcs and uses decision trees to represent transition functions. We evaluate the eﬃciency of our algorithm by comparing with other algorithms.
3.3.1 Background: Factored MDP
A factored MDP ﬁrst proposed by Boutilier et al. [7] is an MDP where the state is represented by a vector of n variables. The transition function in the factored MDP is described by a DBN. Learning the structure of the DBN transition function is called structure learning [54]. Dynamic Bayesian Networks (DBN) are Bayesian Networks (BN) for time series modeling. An example of DBN is illustrated in Figure 3.7. Like in a BN, nodes represent variables and edges represent dependencies between two variables but DBNs include a temporal dimension: a state at time t + 1 depends only on its immediate past, i.e., states at time t.
The DBN model determines which features are relevant or not for the predictions of certain features. It represents transition functions compactly and reduces the computation complexity. It is also eﬀective in exploration to the unvisited states. Instead of exploring every state-action, the agent can make reasonable predictions about unseen state-action pairs.
In factored MDPs, a state is characterized by a ﬁnite set of random variables s = {x1, x2, . . . , xn}. We use xti to denote the variable xi at time t. The transition function from st to st+1 after taking action a is deﬁned by the conditional probability Pr(st+1 | st, a). To simplify notation, we omit action a and use the notation Pr(st+1 |
42

Figure 3.5 – The user interface
Figure 3.6 – Traces of taxi moves 43

Time t X1

X2

X3

X4

X5

Time t+1 X1

X2

X3

X4

X5

Figure 3.7 – Example of Dynamic Bayesian Network (DBN)

st). By the Bayes rule, the probability is decomposed as follows:

Pr(st+1 | st) = Pr(xt1+1 | st, xt2+1, xt3+1, . . . xtn+1) Pr(xt2+1 | st, xt3+1, xt4+1, . . . xtn+1) . . . Pr(xtn+1 | st)

(3.2)

In many cases, it is assumed that there is no synchronic arc, i.e., an arc from xi to xj at time t + 1 [8]. When the variable st+1 depends on only the variable st, then
the transition function (Eq. (3.2)) satisﬁes the independence criterion as follows:

Pr(st+1 | st) = Pr(xt1+1 | st) Pr(xt2+1 | st) . . . Pr(xtn+1 | st) = Pr(xti+1 | st)
i

(3.3)

In a DBN without synchronic arcs, this independence assumption is valid but such model may not be realistic. Thus, in our research, we focus on structure learning with synchronic arcs [8, 23]. We suppose that correlation between state features can exist at time t + 1 as an eﬀect of taking action a at time t. When the DBNs have synchronic arcs, each factor is dependent on its parents in the previous time step as well as other factors in the same time step as seen in Eq. (3.2). To simplify the notation, we use Parents(xi) to denote the parent set of variable xi. Parents(xi) consists of Parentst(xti+1), parents set at time t and Parentst+1(xti+1), parents set at time t + 1. We can rewrite the transition function with Parents(xi) as follows:

Pr(st+1 | st) = Pr(xti+1 | Parents(xti+1))
i

(3.4)

3.3.2 Related work
A factored MDP was ﬁrst proposed by Boutilier et al.[7] and the transition and reward functions are represented with Dynamic Bayesian Networks. In [8], the authors use DBNs with decision trees representing transition functions and rewards. Based on this representation, structured value iteration (SVI) and structured policy iteration (SPI) algorithms are proposed. They also consider synchronic constraints in a problem and extend their algorithm to deal with the synchronic constraints.
DBN-E3 [58] and Factored-Rmax [48] assume the DBN is known in advance and learn the transition probabilities from the structure of the DBN. In [92, 28, 11, 24, 53], the DBN structure is not given. The agent learns the structure of the DBN and then learns the transition probabilities from the DBN. In several methods [92, 28],

44

the maximum in-degree (the maximum number of parents of any factor) of the DBNs

is given as a prior knowledge.

SLF-Rmax [92] learns the conditional probabilities of DBNs when given the max-

imum in-degree of the DBNs. The algorithm enumerates all possible combinations

of factors as elements and keeps statistics for all pairs of the elements.

Diuk et al. [28] propose Met-Rmax which improves SLF-Rmax’s sample complex-

ity. Met-Rmax [28] is based on k-Meteorologists Problems. For n binary factors and

maximum in-degree D, all

n k

subsets of factors are considered as possible parents.

Each parent set corresponds to a hypothesis sub-class in Adaptive k-Meteorologists

and it predicts the outcome. The squared prediction error of meteorologist is used

to improve the eﬃciency of the algorithm.

Chakraborty et al. [11] present a similar approach called LSE-RMax but the

algorithm does not require knowledge of the in-degree of the DBNs. Instead, LSE-

RMax uses a planning horizon that satisﬁes a certain conditions.

Another approach is to use decision trees for building structured representations

of the problem.

Degris et al. [24] use the decision tree induction algorithms called ITI [96] to

learn the reward function and the DBN of the transition function. The generalization

property of the decision trees improves the policy faster than tabular representations.

RL-DT [53] improves Degris et al.’s algorithm with the relative eﬀects of transi-

tions and a diﬀerent exploration policy. The algorithm uses decision trees to general-

ize the relative eﬀects of actions across similar states in the domain. As exploration

policy, the agent ﬁrst explores the environment to learn an accurate model. When

it takes the actions it believes to be optimal, it switches into exploitation mode.

3.3.3 Algorithm for Structure Learning
We present an algorithm to learn the structure of the DBN transition functions with synchronic arcs, shown in Algorithm 12. We use decision trees to represent transition functions instead of tabular representations.
Similar to R-max [9], all unknown state-action values are initialized by a constant Rmax in order to encourage the agent to explore. Each time, the agent takes a greedy action.
To build decision trees, actions have to be visited suﬃciently often. We set a predeﬁned parameter m, the minimum number of visits required to unknown actions, to decide if actions are known or not. Whenever an action is taken, the visit count of the action is incremented (Line 9). If the number of visit for an action is equal to m, decision trees for the action are created (Line 13).
Generally, given an action, each factor s (i) has its own decision tree to estimate Pr(s (i) | ·, a), i.e., one decision tree represents Pr(s (i) | ·, a). We reduce the number of decision trees by choosing some relevant factors whose values are constantly changed whenever the action is taken. Transition functions of non-changed factors are identity functions. Since decision trees of those non-changed factors do not aﬀect the estimation of transition from state s to s , we do not create their decision trees. We collect all value-changed factors in Fa whenever action a is selected (Line 10). Then, for each factor s (i) in Fa, we create a decision tree of the DBNs.
45

LearnTransitionFunction estimates Pr(s (i) | s, a) from the corresponding decision tree and updates the tree with s and s (i). The action value is computed with the obtained transition functions (Line 23). If there is any factor that is still not predictable, we update action value with Rmax (Line 21) to make learn more the state-action value.

Algorithm 12 Learning the structure of the DBN with synchronic arcs

1: Input: initial action value Rmax , minimum visit count on action m

2: // Initialization

3: ∀a ∈ A, ∀s ∈ S, Q(s, a) ← Rmax

4: repeat

5: repeat

6:

a ← argmaxa ∈AQ(s, a )

7:

Execute a, obtain reward r and observe next state s

8:

if C(a) < m then

9:

C(a) ← C(a) + 1

10:

Fa ← RecordChangedFactors(a)

11:

else

12:

if C(a) == m then

13:

BuildDecisionT rees(a)

14:

end if

15:

// Estimate transition function

16:

for each factor s (i) do

17:

Pr(s (i) | s, a) ← LearnTransitionFunction(s, a, s (i))

18:

end for

19:

// Update action-values

20:

if ∃i, Pri(s (i) | s, a) = ⊥ then

21:

Q(s, a) ← Rmax

22:

else

23:

Q(s, a) ← R(s, a) + γ s Pr(s | s, a)maxa ∈AQ(s , a )

24:

end if

25:

end if

26:

s←s

27: until reaching the terminal state

28: until algorithm converges

BuildDecisionTrees is shown in Algorithm 13. To build decision trees, we ﬁrst select parents factors that will be used as nodes of the decision tree (Line 3). For each factor s (i) of Fa, F indParents applies χ2 test to all other factors of time t and to all other factors of Fa at time t + 1 to ﬁnd its parents, P arent(s (i)), shown in Eq. (3.4). par is its parents at time t and parsync is its parents at time t + 1. However, it is diﬃcult to conclude that parsync is a parents set of factor s (i) because χ2 test determines whether there is a signiﬁcant relation between two variables but it does not determines which one causes the other. To decide which one is a parent, we predeﬁne the order of features. Suppose there are two features xi and xj and they are related to each other at time t + 1 by χ2 test. If xi precedes xj in order, then we consider xj a synchronic parent of xi. For feature xi, we
46

place its potential parent features after xi. All following features are candidates for synchronic parents. Using this veriﬁcation, F indRealSyncParents(i) determines which factors are real synchronic parents and returns parsync. CreateDecisionTree builds a decision tree whose nodes are elements of par and parsync. In our algorithm, we use HoeﬀdingTree [30, 55] that is an incremental decision tree induction algorithm that is capable of learning from massive data streams1.
Algorithm 13 BuildDecisionTrees 1: Input: action a 2: for each factor s (i) of Fa do 3: (par, parsync) ← F indParents(i) 4: parsync ← F indRealSyncParents(i) 5: CreateDecisionT ree(par, parsync) 6: end for
3.3.4 Experiments
We evaluate our algorithm with respect to three diﬀerent algorithms: Q-learning [98], R-max [9], and LSE-RMax [11]. Q-learning is a model-free algorithm and R-max is a model-based algorithm. LSE-RMax is a factored model-based algorithm with tabular representations. We apply our algorithm to the coﬀee delivery task [8, 23].
Coﬀee Delivery Task A robot goes to a coﬀee shop to buy coﬀee and delivers the coﬀee to its owner in his oﬃce. It may rain on the way to the coﬀee shop or the oﬃce. The robot will get wet if it does not have an umbrella.
The state is described by six Boolean variables:
• HRC: the robot has coﬀee
• HOC: the robot’s owner has coﬀee
• W: the robot is wet
• R: it is raining
• U: the robot has an umbrella
• O: the robot is at the oﬃce
As discussed in Algorithm 13, we have to order state features to decide which features are synchronic parents. The feature order used in our experiments is the same order as the list above. A synchronic arc exists between HRC and HOC when selecting DelC action. After the delivering action, the robot loses the coﬀee depends on whether the owner successfully gets the coﬀee or not. The probability setting about value changes of HRC and HOC features is explained below.
The robot has four actions:
1http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/HoeffdingTree.html
47

• Go: Moves to oﬃce or coﬀee shop with success probability 0.9. • BuyC: Buy coﬀee if it is in the coﬀee shop with success probability 0.9. • DelC: Deliver coﬀee to its owner if it is in the oﬃce • GetU: Get an umbrella if it is in the oﬃce with success probability 0.9. All actions can be noisy, i.e., tasks are non-deterministic. In our experiments, we set the success probabilities of Go, BuyC, and GetU actions to 0.9. It rains on the way to the coﬀee shop or the oﬃce with probability 0.3. For DelC action, we set the probability of value changes for HRC and HOC features as follows: if the owner receives the coﬀee, the robot loses it. This happens with probability 0.8. Otherwise, the owner fails to receive the coﬀee. Then, the robot keeps holding it with probability 0.8 or loses it with probability 0.2. The robot gets a reward of 0.8 if its owner has coﬀee and an additional reward of 0.2 if it is dry. The start state is that the robot is at the oﬃce without a coﬀee. When the robot delivers a coﬀee to its owner, an episode ﬁnishes.
Figure 3.8 – Average Step Counts
Experimental Results Figure 3.8 shows the average step counts for diﬀerent algorithms. The average steps are calculated at every 100 episode by dividing the total steps from the ﬁrst episode to the last by the total number of episodes. The optimal policy is “(GetU→)Go→BuyC→Go→DelC”. The goal has to be achieved in four or ﬁve steps in the best case. Q-learning converges to the optimal policy faster than the others. Our algorithm is also relatively fast. R-max converges late in comparison to Q-learning and our algorithm. R-max has to learn transition functions for each state-action pair, so it explores more to visit every state-action pair. Unlike the others, LSE-Rmax does not learn eﬀectively the optimal policy.
The average rewards for diﬀerent algorithms are shown in Figure 3.9. For each episode, the agent gets either 0.8 or 1.0 reward. If the agent gets wet, -0.2 penalty
48

Figure 3.9 – Average Reward is given at the terminal state, i.e., the agent gets 0.8 as total reward. When the agent notices this penalty, it will learn an alternative way to avoid the penalty. Our algorithm learns well this penalty case. This is because it learns more accurately the transition models by factoring states. LSE-Rmax is lower than our algorithm but it is slightly better than Q-learning and R-max. By this experiment, we can see that factorization methods learn eﬀectively correct models that lead to increase rewards.
Figure 3.10 – Average Computation Time Figure 3.10 shows the average computation time for each episode. Computation time is measured on action-value update of each algorithm. Q-learning and R-max are faster than LSE-Rmax and our algorithm that use factorized state spaces. In non-factored models, updating a value is quite simple because they just ﬁnd and update the corresponding state-action pair from a tabular, however, factored models
49

have to learn a transition function for each factor. It means the computation cost to learn transition functions by factoring states is greater than that of non-factored methods.
3.4 Discussion and Future Research
In the taxi application, we selected two oﬀ-peak hours (12h to 13h and 14h to 15h) and we investigated the learning behavior during the ﬁxed time periods. In a real problem, passenger behavior changes over time. For example, locations of passengers change suddenly because of unexpected events. In the taxi problem, goals, transition and reward probabilities of the environment can change over time. Eﬀectively, its environment is highly non-stationary (see Section 2.8).
In this chapter, we applied Q-learning algorithm, a model-free method, to the taxi application. Since reinforcement Learning is basically applicable to non-stationary environment, the learning agent can adapt continually to dynamics changes. However, we cannot expect a more responsive adaptation to the changes and when the environment reverts to the previously learned dynamics, the learned knowledge in the past becomes useless.
The Q-learning algorithm is fast but it does not consider a model of the environment. Since the taxi problem is non-stationary, it may be better to apply a model-based method and make it detect the environment changes. In addition to model-based methods’ advantages such as sample-eﬃciency and prediction about the next state and next reward, we can expect to solve the non-stationarity of the taxi problem.
Some previous works have addressed non-stationary problems. Choi et al. [18] introduce hidden-mode Markov decision process (HM-MDP) and assume that the environment is in exactly one of the modes at any given time. In RL-CD [19], the environment is divided into partial models estimated by observing the transitions and rewards. For each time step, the model with the highest quality is activated. The algorithm starts with only one model and then incrementally creates new ones as they become necessary. In TES [79], the agent learns world models called views and collects them into a library to reuse in future tasks. When the agent encounters a new task in a new environment, it selects a proper view from the library or adapts to the new task with completely new transition dynamics and feature distributions. BPR+ [52] is in a multi-agent setting to deal with non-stationary opponents. The learning agent detects that the current policies do not perform optimally and then learns incorporating models of new strategies.
In the taxi problem, we can diﬀerentiate the passenger behaviors with various time periods such as morning-rush hour, evening-rush hour, oﬀ-peak hours and holidays. Then, the agent can learn models depending on the diﬀerent time periods. However, even in the same period the movement may be varied dynamically. We will need a more ﬂexible method to adapt to environment dynamics. Like those existing methods [19, 79, 89, 52], the environment dynamics of the taxi problem can be divided into partial models that are stored in a library. For each time, the agent use a partial model that predicts well the environment. If the prediction error of the current model is larger than a threshold, the agent selects another model from
50

the library. If the environment dynamics is completely diﬀerent from the existing models, it creates a new model. This will be more ﬂexibly adaptable to a nonstationary environment than selecting pre-deﬁned modes by a system designer. In addition, the taxi application we have discussed so far is based on a single agent but it has to be extended to a multi-agent setting. In a multi-agent environment, it will be important for the agent to have an ability to detect non-stationary opponents and learn optimal policies against changed opponent strategies. When opponent strategies are not known a priori, the agent has to adapt to the new environment. Instead of ﬁxed models, the ﬂexible models proposed above will be able to deal with such non-stationary problems.
3.5 Conclusion
In this chapter, we discussed two learning approaches – learning without models and learning models to estimate action values.
One of the well-known model-free methods is Q-learning. We apply Q-learning algorithm to a real taxi routing problem. We also investigate the inﬂuence of the step size, discount rate and trade-oﬀ between exploration/exploitation on learning. To improve action selection strategy, we proposed a customized exploration and exploitation strategy for the taxi problem.
In model-based methods, we address the factored MDP problem in a nondeterministic setting. Most model-based methods are based on DBN transition models. We proposed an algorithm that learns the DBN structure including synchronic arcs. Decision trees are used to represent transition functions. In experiments, we show the eﬃciency of our algorithm by comparing with other algorithms. We also demonstrate that factorization methods allow to learn eﬀectively complete and correct models to obtain the optimal policies and through the learned models the agent can accrue more cumulative rewards.
51

52

Chapter 4
Focused Crawling
In this chapter, we extend our discussion to a very large and continuous domain, in particular, a focused crawling problem.
4.1 Introduction
Focused crawlers are autonomous agents designed to collect Web pages relevant to a predeﬁned topic, for example to build a search engine index. Given a start page (the seed ), a crawler browses Web pages by exploiting hyperlinks of visited Web pages to ﬁnd relevant pages. Usually, crawlers maintain a priority queue of new URLs, called the frontier. Each new URL is assigned a priority value and URLs are fetched from the queue in decreasing order of priority. Since the focused crawler aims to collect as many relevant pages as possible while avoiding irrelevant pages, the key success factor of crawling systems is how good the scoring policy is.
The priority score is initially based on contextual similarity to the target topic [10, 27], on link analysis measures such as PageRank and HITS [61, 80, 2], or on a combination of both [3, 12]. However, links that look less relevant to the target topic but that can potentially lead to a relevant page in the long run may still be valuable to select. Reinforcement learning (RL) enables the agent to estimate which hyperlink is the most proﬁtable over the long run. A few previous studies have applied reinforcement learning to crawling [86, 44, 74, 75], but they require an oﬀ-line training phase and their state deﬁnitions do not consider the link structure; for example, states are represented with a vector which consists of the existence or frequency of speciﬁc keywords.
Hence, we propose a reinforcement learning based crawling method that learns link scores in an online manner, with new representations of states and actions considering both content information and the link structure. Our method assumes that the whole Web graph structure is not known in advance. To properly model the crawling environment as a Markov decision process (MDP), instead of considering each individual page as a state and each individual hyperlink as an action, we generalize pages and links based on some features that represent Web pages and the next link selection, thus reducing the size of the state–action space. To allow eﬃcient computation of a link score, i.e. action-value, we approximate it by a linear combination of the feature vector and a weight vector. Through this modeling, we
53

can estimate an action value for each new link, to add it to the frontier. As action values computed at diﬀerent time steps are used in the frontier, we investigate a synchronous method that recalculates scores for all links in the frontier, along with an asynchronous one that only compute those of all outlinks of the current page. As an improved asynchronous method, we propose moderated update to reach a balance between action-values updated at diﬀerent time steps. In experiments, we compare our proposed crawling algorithms based on reinforcement learning and an algorithm without learning.
This chapter is organized as follows. Section 4.2 presents some important background. Section 4.3 introduces our algorithm, focused crawling with reinforcement learning. Section 4.4 describes the details of our experiments and shows the performance evaluation of our algorithm. Section 4.5 presents prior work in the literature. Section 4.6 discuss some possible extensions. Section 4.7 concludes with some further work.
4.2 Background
Web Crawling. A Web crawler is an agent which autonomously browses Web pages and collects all visited pages. The fetched pages may be stored and indexed in a repository. Web crawlers are in particular used to index Web pages by search engines in order to provide users with fast search. Starting with a set of seed URLs, the crawler visits these URLs, retrieves all hyperlinks from the pages and adds them to the queue of unvisited URLs, called the frontier. The URLs in the queue are visited according to some priority policy. The process repeats until a certain number of pages are collected or some other objective is achieved. The priority of URLs in the queue depends on which crawling strategy is used. In a breadth-ﬁrst crawl, the frontier can be implemented as a ﬁrst-in-ﬁrst-out (FIFO) queue. The best-ﬁrst crawler assigns a priority to each unvisited URL based on an estimate value of the linked page and the frontier is implemented as a priority queue. Most crawling algorithms in the literature are variations of best-ﬁrst [73].
Focused Crawler, Topical Locality, and Tunneling. A focused crawler selects from the frontier the links that are likely to be most relevant to a speciﬁc topic(s). It aims to retrieve as many relevant pages as possible and avoid irrelevant pages. This process consequently brings considerable savings in network and computational resources. While general-purpose crawlers may follow breadth-ﬁrst search, focused crawlers are best-ﬁrst search with their own priority strategies.
Focused crawlers are based on topical locality [22, 72]. That is, pages are likely to link to topically related pages. Web page authors usually create hyperlinks in order to help users navigate, or to provide further information about the content of the current page. If hyperlinks are used for the latter purpose, the linked pages may be on the same topic as the current page and hyperlinks can be useful information for topic-driven crawling. Davison [22] shows empirical evidence of topical locality on the Web. He demonstrates that linked pages are likely to have high textual similarity. Menczer [72] extends the study and formalizes two general conjectures, link–content conjecture and link–cluster conjecture, representing connections from
54

the Web’s link topology to its lexical and semantic content. The measurement results of these conjectures conﬁrm the existence of link–content conjecture that a page is similar to the pages that link to it and that of link–cluster conjecture that two pages are considerably more likely to be related if they are within a few links from each other. The author shows that the relevance probability is maintained within a distance of three links from a relevant page, but then decays rapidly.
To selectively retrieve pages relevant to a particular topic, focused crawlers have to predict whether an extracted URL points to a relevant page before actually fetching the page. Anchor text and surrounding text of the links are exploited to evaluate links. Davison [22] shows that titles, descriptions, and anchor text represent the target page and that anchor text is most similar to the page to which it points. Anchor text may be useful in discriminating unvisited child pages.
Although a focused crawler depends on the topical locality, pages on the same topic may not be linked directly and it can be necessary to traverse some oﬀ-topic pages to reach a relevant page, called tunneling [6]. When going through oﬀ-topic pages, it is needed to decide if the crawl direction is good or not. Bergmark et al. [6] propose a tunneling technique that evaluates the current crawl direction and decides when to stop a tunneling activity. They show tunneling improves the eﬀectiveness of focused crawling and the crawler should be allowed to follow a series of bad pages in order to get to a good one. Ester et al. [32] also propose a tunneling strategy that reacts to changing precision. If precision decreases dramatically, the focus of the crawl is broaden. Conversely, if precision increases, the focus goes back to the original user interest.
4.3 Focused Crawling and Reinforcement Learn-
ing
The goal of focused crawling is to collect as many pages relevant to the target topic as possible while avoiding irrelevant pages because the crawler is assumed to have limited resources such as network traﬃc or crawling time. Thus, in a sequence of crawling, link selection should not be a random choice.
To achieve the crawling goal, given a page, the agent selects the most promising link likely to lead to a relevant page. Even though a linked page looks less relevant to the target topic, if it can potentially lead to a relevant page in the long run, it might be valuable to select it. At each time step, the agent has to estimate which hyperlink can lead to a relevant page. It will be a key success factor in crawling if the agent has an ability to estimate which hyperlink is the most proﬁtable over the long run.
Reinforcement learning ﬁnds an optimal action in a given state that yields the highest total reward in the long run by the repeatedly interaction with the environment. With reinforcement learning, the optimal estimated value of hyperlinks (actions) are learned as pages (states) are visited. The agent can evaluate if a link selection can yield a long-term optimal reward and selects the most promising link based on the estimation. In this section, we discuss how to model a focused crawling with Reinforcement Learning. Like most focused crawlers, we assume that pages with similar topics are close to each other. Our crawling strategy is based on the
55

topical locality and tunneling technique. We also assume that the whole Web graph structure is not known to the crawling agent in advance.
4.3.1 Markov Decision Processes (MDPs) in Crawling
To model the crawling environment in an MDP M = S, A, R, T , we deﬁne Web pages as states S and direct hyperlinks of a page as actions A. When the crawling agent follows a hyperlink from the current page, a transition from the current page to the linked page occurs and a relevance to the target topic is computed for the linked page to evaluate if the selected hyperlink leads to a page relevant to the target topic or not. The transition function T is the probability of transition from the current page to the linked page on taking the hyperlink. The reward r ∈ R is a relevance value of the linked page to the given topic. For the next crawling step, the agent selects a hyperlink with the highest estimation value from the newly visited page, and so on.
Before applying the model above to solve our crawling problem, we must consider two issues: ﬁrst, scalability of state-action space in a reinforcement learning, second, applicability to a crawling task without loss of its inherent process property. For the scalability problem, we reduce a state-action space by generalization presented in this section and update value functions with linear function approximation discussed in Section 4.3.3. For the applicability issue, in order to preserve the original crawling process we prioritize updates, see Section 4.3.2 and 4.3.3.
In this section, we discuss modeling a crawling task as an MDP. As we mentioned above, we deﬁne Web pages as states and direct hyperlinks of a page as actions. However, Web pages are all diﬀerent, there are a huge amount of pages on the Web, and they are linked together like the threads of a spider’s Web. If each single Web page is deﬁned as a state and each direct hyperlink as an action, it makes learning a policy intractable due to the immense number of state-action pairs. Furthermore, in reinforcement learning, optimal action-values are derived after visiting each stateaction pair inﬁnitely often. It is not necessary for a crawler to visit the same page several times. Thus, our MDPs can not be modeled directly from a Web graph. Instead, we generalize pages and links based on some features that represent Web pages and the next link selection. By this generalization, the number of state-action pairs is reduced and Web graph is properly modeled in an MDP. Some pages with the same feature values are in the same state. Some hyperlinks with the same feature values also can be treated as the same action. The features extracted from pages and hyperlinks are presented in the following.
States. A Web page is abstracted with some features of Web pages in order to deﬁne a state. The features of a state consists of two types of information. The ﬁrst one is proper information about the page itself. The second is relation information with respect to surrounding pages. Page relevances to the target topic and to some categories are the current pages’ own information. Relevance change, average relevance of parent pages, distance from the last relevant page represent the relation with the pages surrounding the current page. In order to properly obtain the relation information, each unvisited link should retain parent links. The crawling agent is assumed not to know the whole Web graph in advance, thus each link initially
56

does not know how many parents they have but parent information is progressively updated as pages are crawled. When a page is visited, the URL of the current page is added to all outlinks of the page as their parent. Each link has at least one parent link. If a link has many parents, it means that the link is referenced by several pages.
Most features are continuous variables, which we speciﬁed with two diﬀerent indexes discretized into 5 and 6 buckets according to value ranges: 1) the range [0.0, 0.2] by 0, [0.2, 0.4] by 1, . . . , [0.8, 1.0] by 4. 2) the range [0.0, 0.1] by 0, [0.1, 0.3] by 1, . . . , [0.9, 1.0] by 5. The relevance value is also discretized according to value ranges as above but occasionally the value has to be converted to a Boolean to specify if a page is relevant to a given topic or not. For example, two features, the Average Relevance of Relevant Parents and Distance from the Last Relevant Page, require true/false value regarding to the relevance. To avoid an arbitrary threshold for relevance, we simplify the deﬁnition of relevant page as follows: if a crawled page has a tf-idf based relevance greater than 0.0 or simply contains the target topic word, the page is deﬁned to be relevant to the topic.
• Relevance of Target Topic: The target topic relevance based on textual content is computed by cosine similarity between a word vector of target topic and that of the current page and it is discretized according to value ranges.
• Relevance Change of Target Topic: The current page’s relevance to the target topic is compared to the weighted average relevance of all its ancestors on the crawled graph structure. The weighted average relevance of all its ancestors are computed in an incremental manner by applying an exponential smoothing method on the parents pages.
Before we explain how to calculate the weighted average relevance of its all ancestors, we simply note that in the exponential smoothing method, the weighted average y at time i with an observation xi is calculated by: yi = β ·xi +(1−β)·yi−1, where β(0 < β < 1) is a smoothing factor. The exponential smoothing assigns exponentially decreasing weights on past observations. In other words, recent observations are given relatively more weight than the older observations.
In our crawling example, if the relevance of a page x is denoted rl(x), then the weighted average relevance of x, wrl(x) is obtained by wrl(x) = β · rl(x) + (1 − β) · maxx →x wrl(x ).
If the current page has many parents, i.e. many path from its ancestors, the maximum average among them, maxx →x wrl(x ), is used for the update. wrl(x) is the weighted average relevance over x and all its ancestors on the crawled graph structure.
Then, we can calculate the relevance change between current page p and wrl(x) where x is a parent of p: change ← rl(p) − maxx→p wrl(x).
The change helps to detect how much the relevance of the current page is increased or decreased than the average relevance of its ancestors.
The relevance change to the current page from its ancestors is discretized according to value ranges. With predeﬁned parameters δ1 and δ2, the diﬀerence
57

within δ1 is indexed by 0, the increase by δ2 is indexed by 1, increase more than δ2 is indexed by 2, decrease by δ2 is indexed by 3, and decrease more than δ2 is indexed by 4.
• Relevances of Categories: Given a target topic, its related categories in a category hierarchy such as the Open Directory Project (ODP, https://www. dmoz.org/, http://curlie.org/) are properly selected by the designer of the system. For each category, its relevance is calculated by cosine similarity between a word vector of the category and that of the current page. It is discretized according to value ranges.
• Average Relevance of All Parents: The average of all parents’ relevance is calculated and discretized according to value ranges.
• Average Relevance of Relevant Parents: The average of relevant parents’ relevance is calculated and discretized according to value ranges.
• Distance from the Last Relevant Page: The distance on the crawl path from the last relevant ancestor page to the current page is simply calculated by adding 1 to the parent’s distance. If there are many parents, the minimum distance among them is used. The distance value is capped at 9 to keep it within a ﬁnite range.

0 distance =
1 + parent’s distance

if it is a relevant page otherwise

(4.1)

Actions. In order to deﬁne actions, all hyperlinks in a Web page are also abstracted with some features in a similar way as pages are. Relevances to the target topic and to some categories are used to predict the relevance of the page that a hyperlink points to. Diﬀerent from pages, hyperlinks do not have suﬃcient information to calculate the values. Thus, the URL text, the anchor text and surrounding text of a hyperlink are used to compute. Here, the relevance is not a true relevance but a prediction because it is not possible to know which page will be pointed by a hyperlink before following the link. In order to support the relevance prediction, the average relevances of parent pages are also used as features that represent the relation with the pages surrounding the link. Each hyperlink has at least one parent. If the link is referenced by several pages, it can have many parents. As mentioned above, parent information is progressively updated as pages are crawled and each unvisited link retains parent links. Then, the parent information is used to compute average relevance of parent pages. The features for action are a subset of those of states, namely:
• Relevance of Target Topic
• Relevances of Categories
• Average Relevance of All Parents
• Average Relevance of Relevant Parents
58

The size of a discretized state space is (10)4 · (10)num of categories · 5 and the size of action space is (10)3 · (10)num of categories. For example, if there is just one category, the size of the state space is 5 · 105 and the size of the action space is 104.
4.3.2 MDPs with Prioritizing Updates
In a focused crawl, the agent visits a Web page and extracts all hyperlinks from the page. The hyperlinks are added to the priority queue, called frontier. A link with the highest priority is selected from the frontier for the next visit. The frontier plays a crucial role in the crawling process. The agent can take the broad view of the crawled graph’s boundary, not focusing on a speciﬁc area of the whole crawled graph. Unvisited URLs are maintained in the frontier with priority score and therefore, for each iteration, the most promising link can be selected from the boundary of the crawled graph. Thus, the Web crawler can consistently select the best link regardless of its current position.
We use a temporal diﬀerence (TD) method of reinforcement learning in order to make crawling agents learn good policies in an online, incremental manner as crawling agents do (see Section 2.3). In most TD methods, each iteration of value updates is based on an episode, a sequence of state transitions from a start state to the terminal state. For example, at time t, in state s the agent takes an action a according to its policy, which results in a transition to state s . At time t + 1 in the successor state of s, state s , the agent takes its best action a followed by a transition to state s and so on until the terminal state. While crawling, if the agent keeps going forward by following the successive state transition, it can fall into crawling traps or local optima. That is the reason why a frontier is used importantly in crawling. It is necessary to learn value functions in the same way as crawling tasks.
To keep the principle idea of crawling tasks, we model our crawling agent’s learning with prioritizing the order of updates that is one of value iteration methods to propagate the values in an eﬃcient way (see Section 2.7). With a prioritized update method, the crawling agent does not follow anymore the successive order of state transitions. Each state-action pair is added to the frontier with its estimated action value. For each time, it selects the most promising state-action pair among all pairs as the traditional crawling agent does.
4.3.3 Linear Function Approximation with Prioritizing Updates
We have modeled our crawling problem as an MDP and deﬁned features of the states and the actions in Section 4.3.1. Then, we have presented prioritized updates in reinforcement learning to follow the original crawling process in Section 4.3.2. In this section, we discuss how to represent and update action-value functions based on the state and action features deﬁned in Section 4.3.1.
As discussed in Section 4.3.2, the crawling frontier is a priority queue. Each URL in the frontier is associated with a priority value. The links are then fetched from the queue in order of assigned priorities. In our crawling model, we estimate an action value for each unvisited link and add it to the frontier with its action value.
59

In reinforcement learning, if a state space is small and discrete, the action value functions are represented and stored in a tabular form. But, this method is not suitable for our crawling problem with a large state-action space. Thus, we use a function approximation method, in particular the linear function approximation, to represent action values (see Section 2.4). The action value function is approximated by linearly combining the feature vector x(s, a) and the weight vector w with Eq. (2.26). State and action features deﬁned in Section 4.3.1 are used as the components of a feature vector x(s, a). At each time step, the weight vector w is updated using a gradient descent method, as in Eq. (2.27). The approximated action-value obtained from Eq. (2.26) is used as the priority measure.
When we calculate action-values only for the outlinks of the current page with newly updated weights and add them to the frontier, an issue can arise in the scope of state-action pairs regarding computation of action value. This problem is caused from the prioritized order of selecting a link from the frontier. If the agent keeps going forward by following the successive state transition, it is correct that calculating action values is applied only to the direct outlinks because the next selection is decided from one of the all outlinks. However, in the prioritized order selecting from the frontier, when the weight vector w is changed, action values of all links in the frontier also have to be recalculated with the new w. We call this synchronous method. Recalculating for all links is the correct method but it involves an excessive computational overhead. Otherwise, we can calculate action-value only for outlinks of the current page and/or recalculate all links(actions) in the frontier that are from the current state. The action values of all other links in the frontier are left unchanged. We call this asynchronous method. This method does not incur computational overhead but action values of all links in the frontier are calculated at diﬀerent time steps and make it diﬃcult to choose the best action from the frontier. In experiments, we compare the performance of the two methods.
Since the asynchronous method has an advantage that does not need to recalculate action values of all unvisited links in the frontier, we try to improve the asynchronous method. The problem of asynchronous method is that action values computed in diﬀerent time steps exist together in the frontier and it can cause a noise in selection. Thus, we reduce the action value diﬀerences in the frontier by manipulating weight updates. The TD error is the diﬀerence between the estimates at two successive time steps, r + γqˆ(s , a , w) and qˆ(s, a, w). Updating the error to weights signiﬁes that the current estimate qˆ(s, a, w) is adjusted toward the update target r + γqˆ(s , a , w). In order to moderate the TD error, we adjust the estimate qˆ(s , a , w) by the amount of the TD error when updating weights as follows:

w ← w + α [r + γ(qˆ(s , a , w) − δ) − qˆ(s, a, w)] ∇qˆ(s, a, w)

(4.2)

where δ = r + γqˆ(s , a , w) − qˆ(s, a, w). We call this moderated update. In fact, this moderated update can be shown to have same eﬀect as reducing the step-size α of the original update by 1 − γ.

w = w + α [r + γ(qˆ(s , a , w) − δ) − qˆ(s, a, w)] ∇qˆ(s, a, w) = w + αδ∇qˆ(s, a, w) − αγδ∇qˆ(s, a, w) = w + α(1 − γ)δ∇qˆ(s, a, w)

60

The idea behind the moderated update is to decrease an overestimated action value or to increase an underestimated action value of the update target in order to make a balance between action-values updated at diﬀerent time steps.
In experiments, we compare the performance of synchronous, asynchronous methods and asynchronous method with moderated update.
Our reinforcement learning for crawling is outlined in Algorithm 14. The crawling task is started with seed pages (lines 5–13). The frontier is ﬁlled with (s, a) pairs of all outlinks from the seed pages. A link is extracted from the frontier according to the -greedy policy (lines 16–20). With small probability , the agent selects a link uniformly at random. Otherwise, it selects greedily a link from the frontier. The agent fetches the page corresponding to the selected link and deﬁnes feature values of the newly visited state as described in Section 4.3.1 (line 24). All outlinks in the fetched page are retrieved (line 25). For each outlink, action feature values are deﬁned as described in Section 4.3.1 (line 30). The weight vector w of linear function approximation is updated based on a reward and feature vectors of the new state returned from the fetch in line 24 (lines 32–39). With the updated weight vector, an estimated action value of each outlink is computed and added to the frontier with the estimated value. If we use the synchronous method, action values of all hyperlinks in the frontier (l, ·, ·) are recalculated (lines 40–43). With the asynchronous method, hyperlinks (l , s , ·) that are from the state s are updated with new action values (lines 44–47). This process repeats until the visit counter reaches the predeﬁned visit limit.
4.4 Experimental Results
A crawling task starts with a seed page and terminates when the visit counter reaches the predeﬁned visit limit. In our experiments, the limit of the page visit is set to 10,000. For each time step, the agent crawls a page and obtains a reward based on two values: cosine similarity based on tf-idf, and cosine similarity with word2vec vectors (w2v) pre-trained from https://nlp.stanford.edu/projects/glove/. If a crawled page has a tf-idf based relevance greater than 0.0 or simply contains the target topic word, the page is relevant to the target topic and then the agent receives a reward of 30. If a page has a tf-idf based relevance lower than 0.0 but it has a w2v based relevance greater than 0.5 or 0.4, the agent receives a reward of 30 or 20 respectively because the content of such page is rather related to the target topic and could eventually lead to a relevant page. Otherwise, the agent receives a reward -1 per time step.
As a crawling environment, we use a database dump of Simple English Wikipedia provided by the site https://dumps.wikimedia.org/. As target topics to use in our experiments, we choose three topics, Fiction, Olympics, and Cancer, of which relevant pages are fairly abundant and another three topics, Cameras, Geology, and Poetry, of which relevant pages are sparse in our experimental environment. For each target topic, a main page corresponding to the topic is used as a seed page. In all experiments, parameter settings for learning are = 0.1, discount rate γ = 0.9, and step size α = 0.001. For topic Olympics and Fiction, step size α is set to 0.0005.
61

Algorithm 14 Focused Crawling based on Reinforcement Learning

1: Input: seed links Seeds, maximum number of pages to visit LIMIT PAGES 2: Initialize value-function weights w ∈ Rd

3: B ← ∅ # contains (s, a) pairs

4:

5: while Seeds is not empty do

6: Select a link l from Seeds

7: s ← Fetch and parse page l

8: L ← Extract all outlinks of l

9: for each l ∈ L do

10:

(l , s , a ) ← Get action features a of l

11:

Add (l , s , a ) to (s , a ) pair of B with initial Q-value

12: end for

13: end while

14:

15: while visited pages < LIMIT PAGES do

16: if With probability then

17:

Select a (s, a) pair uniformly at random from B and select a link (l, s, a) from

the pair

18: else

19:

Select a (s, a) pair from B with highest Q-value and select a link (l, s, a) from

the pair

20: end if

21: if l is visited then

22:

continue

23: end if

24: r, s ← Fetch and parse page (l, s, a)

25: L ← Extract all outlinks of l

26: for each l ∈ L do

27:

if l is visited then

28:

continue

29:

end if

30:

(l , s , a ) ← Get action features a of l

31: end for

32: if visited page is relevant then

33:

w ← w + α [r − qˆ(s, a, w)] ∇qˆ(s, a, w)

34: else

35:

Choose a as a function of qˆ(s , ·, w) with -greedy policy

36:

δ ← r + γqˆ(s , a , w) − qˆ(s, a, w)

37:

w ← w + α [r + γqˆ(s , a , w) − qˆ(s, a, w)] ∇qˆ(s, a, w) #original update

38:

w ← w + α [r + γ(qˆ(s , a , w) − δ) − qˆ(s, a, w)] ∇qˆ(s, a, w) #moderated update

39: end if

40: for each (·, ·) pair ∈ B do #synchronous method

41:

Calculate Q-value of (·, ·)

42:

Update (·, ·) to B with Q-value

43: end for

44: for each (s , ·) pair ∈ L do #asynchronous method

45:

Calculate Q-value of (s , ·)

46:

Add (l , s , ·) to (s , ·) pair of B with Q-value

47: end for

48: visited pages ← visited pages + 1

49: end while
62

Each feature of state and action is speciﬁed with two indexes discretized 5 and 6 buckets. In the case of ’Relevance Change of Target Topic’ feature, it is discretized into 5 buckets. The parameter δ1 and δ2 used for discretizing its value are set to 0.1 and 0.3 respectively and smoothing factor β is set to 0.4. The number of features are diﬀerent depending on a target topic because categories vary according to the target topic. In our experiments, categories are empirically pre-selected based on the Open Directory Project (ODP, http://www.dmoz.org, http://curlie.org/), an open directory of Web links. The ODP is a widely-used Web taxonomy that is maintained by a community of volunteers. Among the target topics of our experiments, for Cancer, four related categories, Disease, Medicine, Oncology and Health, are chosen from the category hierarchy. For Fiction, there are two related categories, Literature and Arts. For Olympics, one related category, Sports, is selected, for Cameras, three categories, Photography, Movies, and Arts, for Geology, two categories, Earth and Science, and for Poetry, two related categories, Literature and Arts. A state is a eight + 2 · α dimensional vector where α is the number of categories selected from a category hierarchy. Like a state, an action is represented as a six + 2 · α dimensional feature vector.
In this section, we compare our three proposed crawling algorithms based on reinforcement learning (synchronous method, asynchronous method, and asynchronous method with moderated update), and an algorithm without learning. The nolearning algorithm is served as a baseline of performance. It uses w2v based cosine similarity as a priority in the crawling frontier and does not use any features or learning update formulas presented in Section 4.3. In no learning algorithm, when crawling a page, each outlinks is added to the frontiers with its w2v based cosine similarity based on the URL text, the anchor text and surrounding text of the hyperlink. Then, a link with the highest priority is selected for the next crawling.
Given a crawling task that visit 10,000 pages, Figure 4.1 shows the accumulated number of relevant pages per time step during the crawling task. The x axis represents time step of a crawling task and the y axis marks the accumulated number of relevant pages per time step. Each curve is the average of 100 tasks. For each task, all data obtained during a crawling task is initialized, for example, hyperlinks in the frontier and parent information of hyperlinks, etc., but the weight vector learned in the precedent task is maintained. For all target topics, the algorithm without learning ﬁnds relevant pages progressively as time steps increase. For some topics such as Olympics, Cameras, Geology, and Poetry, we can see a sharp increase in early time steps. This is because a given seed page is a main page corresponding to each target topic, thus, the agent has more chance to meet relevant pages in early time steps. Compared to no learning with monotonous increase, reinforcement learning algorithms speed up ﬁnding relevant pages. In particular, for topic Cancer, Fiction, Geology, and Poetry, the accumulated number of relevant pages is increased abruptly. It means that reinforcement learning eﬀectively helps ﬁnd relevant pages as time steps increase. For topic Olympics and Cameras, the agent based on reinforcement learning follows a similar curve as no learning but ﬁnds more relevant pages.
Figure 4.2 displays the quality of the experimental results above. The x axis marks w2v-based relevance discretized into 10 intervals and the y axis represents the number of relevant pages per relevance level. Each bar is the average of 100
63

tasks. In lower or higher levels of relevance, there is no signiﬁcant diﬀerence among all algorithms because there are not many pages corresponding to those relevances on the environment. Meanwhile, it is apparent that learning and no learning algorithms have a big diﬀerence of performance for 3rd to 6th relevance levels depending on the distribution of relevant pages on the Web graph for each topic. Among learning algorithms, their performance results are similar or slightly diﬀerent depending on topics. For topic Cancer, Geology, and Poetry, learning algorithms ﬁnd similar number of relevant pages per relevant level. For topic Fiction, Olympics and Cameras, we can see a bit diﬀerence of performance between learning algorithms.
Figure 4.3 shows learning curves of three algorithms on the diﬀerent target topics. Each curve is the average of 10 independent trials. The x axis represents the number of crawling tasks and the y axis marks the number of relevant pages per task. A crawling task consists of visiting 10,000 pages. The learning curves show how the learning is improved as a task repeats. For each task, the weight vector learned in the precedent task is maintained and all other data obtained during a crawling task is initialized, for example, hyperlinks in the frontier and parent information of hyperlinks, etc. The same seed pages are given for each task. Thus, each crawling task starts in the same condition except the weight vector. By the learning curves, we can see how a crawling task is improved given the same condition. We compare reinforcement learning algorithms with no learning algorithm. Since each task is executed under the same condition, no learning algorithm’s performance is the same regardless of the number of crawling tasks. For all target topics, reinforcement learning algorithms have better performance than the algorithm without learning. Those performances are generally 1.2 to 1.6 times, in particular, for topic Cancer, 2.5 times better than that of no learning algorithm. In Figure 4.3(a)–(c), among all reinforcement learning algorithms, the synchronous method has the highest performance. In asynchronous methods, the moderated update outperforms the original update. In Figure 4.3(d)–(f), the moderated update ﬁnds relevant pages more than the other algorithms. Relevant pages of the three topics exist sparsely on the environment and thus those topics need more exploration. Since action values of unvisited links in the frontier are calculated at diﬀerent time steps, those diﬀerent values can hinder a good selection. By the moderated method, we can reduce the action value diﬀerences between time steps and eﬀectively explore promising links while being less inﬂuenced by time steps.
From Figure 4.3, we see that the synchronous method is better in general but the overhead of updating all action values cannot be ignored. For example, the computation time of synchronous method is 654 seconds while that of asynchronous and no learning are 55 and 28 seconds respectively for one crawling task of topic Olympics. Thus, if we consider the overhead of updates, the asynchronous method with moderated update can be a good alternative and may be even better in the environment in which the agent needs more exploration and in which action value diﬀerences are inﬂuenced by time steps.
64

4.5 Related Work
Chakrabarti et al. [10] ﬁrst introduced focused crawling to selectively seek out pages that are relevant to a pre-deﬁned set of topics, using both a classiﬁer and a distiller, to guide the crawler. The classiﬁer is based on naive Bayesian method and evaluates the relevance of a page with respect to the given topics. The distiller identiﬁes if a page is a great access point to many relevant pages within a few links.
Diligenti et al. [27] introduced a context-focused crawler that improves on traditional focused crawling. Their classiﬁer is trained by a context graph with multiple layers and used to estimate the link distance of a crawled page from a set of target pages.
Basically, the relevance is measured based on the textual content but the Web graph structure is also exploited to evaluate relevance in many crawling methods. PageRank and HITS are two famous algorithms that rely on the link structure of the Web to calculate the relevance to the target pages.
Kleinberg [61] proposes the HITS algorithm that discovers relevant authoritative pages by the link structure analysis. He introduces the notion of authority and hub based on the relationship between relevant pages. An authority is a prominent page related to the query topic. A hub is a page that has links to many relevant authoritative pages.
Page et al. [80] introduce PageRank, a method for rating Web pages based on the Web graph structure. The importance of the pages, PageRank is measured by counting citations or backlinks to a given page.
The intelligent crawler [2] proposed by Aggarwal et al. statistically learns the characteristics of the link structure of the Web during the crawl. Using this learned statistical model, the crawler gives priorities to URLs in the frontier. The crawler computes the interest ratios for each of the individual factors such as page content, URL token, link, and sibling. Then, the interest ratios are combined by a linear combination of the weighted logarithms of individual factors. The combined ratio is used to estimate the probability of a candidate URL satisfying the user needs.
Almpanidis et al. [3] propose a latent semantic indexing classiﬁer that combines link analysis and content information. Chau et al. [12] focus on how to ﬁlter irrelevant documents from a set of documents collected from the Web, through a classiﬁcation that combines Web content analysis and Web structure analysis. For each page, a set of content-based and link-based features is deﬁned and used for input data of the classiﬁcation.
Most crawling approaches use classiﬁcation methods to evaluate priority but a few previous works have applied reinforcement learning to focused crawling.
Rennie et al. [86] ﬁrst use reinforcement learning in Web crawling. Their algorithm calculates Q-values for hyperlinks in training data, then learns a value function that maps hyperlinks to future discounted reward by using naive Bayes text classiﬁers. It performs better than traditional crawling with breadth-ﬁrst search but the training is performed oﬀ-line. The authors deﬁne that the state is the bit vector indicating which pages remain to be visited and the actions are choosing a particular hyperlink in the frontier.
Grigoriadis et al. [44] propose a focused crawling that uses reinforcement learning
65

to estimate link score. The algorithm is composed of two modes such as training and crawling. In training, an agent visits pages by selecting randomly hyperlinks until it ﬁnds a relevant page. As the agent visits pages, a neural network gradually learns states’ estimate values. In crawling, the agent evaluated all outlinks using the trained neural network but in fact these link scores inherit their parent’s score. These hyperlinks with their scores are added to the queue. The crawler selects the hyperlink with the highest score. In [44], every page represents a state that consists of a feature vector of 500 binary values and actions are the hyperlinks in each page. Each binary value in a state represents the existence of a speciﬁc keyword and the binary vector of a state is used as input data of neural network to estimate its statevalue. State values are approximated with gradient descent function approximation based on neural network.
InfoSpiders [74, 75] is a distributed adaptive online crawler based on genetic algorithms. It is inspired by ecological models in which a population of agents lives, learn, reproduce and die in an environment. Each agent consists of the genotype, that determines its searching behavior, and a neural network used to evaluate links. The neural net learns to estimate the Q values of links extracted from a source page. The cosine similarity between the agent’s keyword vector and the page containing the link is calculated and it is used as a reward. Then, the neural net’s link scores and the cosine similarity are combined. This procedure is similar to the reinforcement learning algorithm. Based on the combined score, the agent selects one of the links in the frontier. In [74, 75], all hyperlinks in the frontier are considered as actions. Each input unit of the neural net receives a weighted count of the frequency with which the keyword occurs in the vicinity of the link to be traversed.
Those methods require an oﬀ-line training phase and their state deﬁnitions do not consider the link structure; for example, states are represented with a vector which consists of the existence or frequency of speciﬁc keywords. Our method learns link scores in an online manner, with new representations of states and actions considering both content information and the link structure.
Meusel et al. [76] combine online classiﬁcation and bandit-based selection strategy. To select a page to be crawled, they ﬁrst use the bandit-based approach to choose a host with the highest score. Then, a page from the host is taken using the online classiﬁer. Similarly, Gouriten et al. [42] use bandits to choose estimators for scoring the frontier.
Like reinforcement learning, crawling involves a trade-oﬀ between exploration and exploitation of information: greedily visiting URLs that have high estimate scores vs exploring URLs that seem less promising but might lead to more relevant pages and increase overall quality of crawling. Pant et al. [81] demonstrate that the best-N-ﬁrst outperforms the naive best-ﬁrst. The best-N-ﬁrst algorithm picks and fetches top N links from the frontier for each iteration of crawling. Increasing N results in crawlers with greater emphasis on exploration and consequently a reduced emphasis on exploitation [81].
66

4.6 Future Work
We have seen how to model a crawling task using the MDP formalism. The crawling agent eﬀectively learns link scores based on reinforcement learning. In this section, we discuss some possible extensions to improve our method.
First, the dataset we used is suﬃciently good to verify the eﬀectiveness of reinforcement learning based crawler but it should be evaluated in larger and various datasets, such as full English Wikipedia and dataset from the site http: //commoncrawl.org/, etc.
Second, the state and action representation is based on both content information and the link structure. Among features, categories related to a target topic is required to be pre-selected by a system designer. As we have seen in section 2.4, approximated value functions rely on feature. Thus, poor feature selection may result in poor performance. Instead of manually selecting features based on domainspeciﬁc knowledge, it is necessary to build up an eﬃcient mechanism for category selection.
Finally, the method we have proposed is based on a single agent. In a real problem, the number of outlinks of a page may be much larger and the size of the frontier will grow fast. An agent does a crawling task in a large environment, it will be a time-consuming. To accelerate crawling performance, we can consider multiple crawling agents. That is, executing multiple crawling agents in parallel. Each agent crawls Web pages and learns action values independently. The agents will explore diﬀerent parts of the environment. They will have diﬀerent experience because they will visit diﬀerent pages (states) and receive diﬀerent rewards. As a result, they will also have distinct scoring policies with respect to their own value functions. We can consider each agent to be completely independent, i.e. they do not share any information including the frontier. If the frontier is shared by all agents, scoring policies also have to be merged in some way. A few research works [45, 100, 33] show that combining the diﬀerent policies outperforms a single agent.
Grounds et al. [45] present an approach of parallelization to solve single-agent RL problems more quickly. The value functions are represented using linear function approximators and updated by SARSA(λ) algorithm. Each agent learns independently in a separate simulation. During learning, agents update feature weights and maintain visit counts for each features. In every predeﬁned time step, a synchronous merging operation is executed and it calculates a weighted average of the feature value estimates collected from all the agents, favoring features with high probabilities of visitation. To improve the merging process, selective communication of signiﬁcant information and asynchronous message passing are proposed.
Wiering et al. [100] present several ensemble approaches that combine the policies of multiple independently learned RL algorithms. Among the ﬁve RL algorithms used for combining, three algorithms, Q-learning, Sarsa, QV-learning, learn stateaction values and two algorithms, actor–critic (AC) and AC learning automaton, learn preference values of policies. The authors combine the diﬀerent policies with four ensemble methods such as majority voting (MV), rank voting, Boltzmann multiplication (BM), and Boltzmann addition. Their experiments show that the BM and MV ensembles signiﬁcantly outperform the other ensemble methods and the single RL algorithms.
67

FauBer et al. [33] propose several ensemble methods that combine parameterized state-value functions of multiple agents. Temporal-Diﬀerence (TD) and ResidualGradient (RG) learning are used to update state-value functions. These functions are combined with Majority Voting and Average of the state-values to learn joint policies. Another improvement is an average predicted state-value that explicitly combines the state-values of all agents for the successor state. Their experiments show that ensemble methods outperforms a single agent.
4.7 Conclusion
In this chapter, we applied reinforcement learning to focused crawling. We propose new representations for Web pages and next link selection using contextual information and the link structure. A number of pages and links are generalized with the proposed features. Based on this generalization, we used a linear function approximation with gradient descent to score links in the frontier. We investigated the trade-oﬀ between synchronous and asynchronous methods. As an improved asynchronous method, we propose moderated update to reach a balance between action-values updated at diﬀerent time steps. Experimental results showed that reinforcement learning allows to estimate long-term link scores and to eﬃciently crawl relevant pages. In future work, we hope to evaluate our method in larger and various datasets, such as full English Wikipedia and dataset from the site http://commoncrawl.org/, etc. Another challenging possibility is to build up an eﬃcient mechanism for categories selection to avoid a system designer pre-selecting proper categories for each target topic. We also want to investigate other ways to deal with exploration/exploitation. Finally, extending single agent based method to multiple crawlers will be an interesting future work.
68

(a) Cancer

(b) Fiction

(c) Olympics

(d) Cameras

(e) Geology

(f) Poetry

Figure 4.1 – Accumulated Number of Relevant Pages per Time Step

69

(a) Cancer

(b) Fiction

(c) Olympics

(d) Cameras

(e) Geology

(f) Poetry

Figure 4.2 – Number of Relevant Pages per Relevance Interval

70

(a) Cancer

(b) Fiction

(c) Olympics

(d) Cameras

(e) Geology

(f) Poetry

Figure 4.3 – Number of Relevant Pages as Tasks Repeat

71

72

Chapter 5
Inﬂuence Maximization
In this chapter, we continue our discussion with another domain with rich applications, the inﬂuence maximization problem.
5.1 Introduction
Word-of-mouth, buzz marketing, and viral marketing have been used as eﬀective marketing strategies traditionally conducted in oﬄine networks. Oﬄine social network activities have been extended to online social networks such as Facebook and Twitter, etc., and the popularity of such social media has rapidly increased over the last decade. Social networking sites are good platforms not only for communication among users but also for information diﬀusion. Some information is disseminated to many other users through the network. Since these social networks can play an important role in the spread of information at a very large scale, they have attracted interest in the area of online viral marketing. Detecting inﬂuential users is an important problem for eﬃcient online viral marketing.
Suppose that a company develops a new product and hopes to market the product to a large number of people on an online network. The company would like to choose some users of the network to give free samples of the product while expecting they spread information after use, recommend it, or ultimately lead to purchase it. When we want to advertise the product eﬃciently with a limited budget for giving samples, a problem that can arise is to determine who the most inﬂuential users are. The problem assumes that a few inﬂuential users, i.e., seeds, can trigger a large diﬀusion of information via a network.
Given a social network, the inﬂuence maximization problem is to choose an optimal initial seed set of a given size to maximize inﬂuence under a certain information diﬀusion model such as the independent cascade (IC) model, the linear threshold (LT) model, etc. It was ﬁrst proposed by Domingos and Richardson [31, 87] and formulated as an optimization problem by Kempe et al. [60]. The IM problem has been actively studied in the literature [31, 87, 60, 57, 77, 14].
In many existing algorithms, the whole topological structure of a social network is given in advance. However, it is known that the complete knowledge of the topological structure of a social network is typically diﬃcult to obtain [107, 70, 39, 77]. Even though the complete graph is given, the graph may change dynamically [107].
73

Mihara et al. [77] address inﬂuence maximization problem for unknown graphs and show that a reasonable inﬂuence spread can be achieved even when knowledge of the social network topology is limited and incomplete.
Another unrealistic aspect of many existing methods is that these methods do not take into account topical interests of users. In fact, users have their own interests and are more likely to be inﬂuenced by information that is related to their interests. That is, the spread of information varies depending on the topic of a post. There are some works that study the topic-based inﬂuence maximization problem [49, 15, 13, 66]. Their methods consider multiple topic distributions on nodes and a query but we focus on one target topic and study inﬂuence maximization for a given topic.
In this study, assuming that the graph structure is incomplete or can change dynamically, we address a topic-based inﬂuence maximization problem for an unknown graph and show how it can be phrased, again, as a Markov decision process. In order to know a part of the graph structure and discover potentially promising nodes, we probe nodes that may have a big audience group. Then, we ﬁnd the most inﬂuential seeds to maximize topic-based inﬂuence by using reinforcement learning [94]. As we select seeds with a long-term impact in the inﬂuence maximization problem, action values in the reinforcement learning signify how good it is to take an action in a given state over the long run. Therefore, we learn action values of nodes from interaction with the environment by reinforcement learning. For this, nodes are generalized with some features that represent a node’s proper information and relation information with respect to surrounding nodes and we deﬁne states and actions based on these features. Then, we evaluate action value for each probed node and select a node with the highest action value to activate.
In the following section, we review the inﬂuence maximization problem. In Section 5.3, we present topic-based inﬂuence maximization problem for unknown graphs and our method for the problem. Section 5.4 presents prior work in the literature. Section 5.5 discuss several opportunities for future work and we conclude in Section 5.6.
5.2 Background
The inﬂuence maximization problem is to choose an optimal initial seed set of a given size in a given social network that maximizes the total amount of inﬂuence under a certain information diﬀusion model.
The inﬂuence maximization problem is formally deﬁned as follows:
Problem [Inﬂuence Maximization Problem]. We ﬁx a graph G = (V, E) and a parameter (called budget) k ≤ |V | where v ∈ V are nodes and e ∈ E are edges between nodes. Let σ(S) the expected number of active nodes through a seed set S under a given diﬀusion model. The inﬂuence maximization problem is to select a seed set S ⊆ V with |S| = k that maximizes the inﬂuence σ(S).
The inﬂuence function is monotone if σ(S) ≤ σ(T ) for all S ⊆ T ⊆ V and it is submodular if σ(S ∪ {v}) − σ(S) ≥ σ(T ∪ {v}) − σ(T ) for all S ⊆ T ⊆ V and for all v ∈ V.
Algorithm 15 shows the greedy algorithm of the inﬂuence maximization problem.
74

Algorithm 15 Greedy(G, k, p)
1: Input: Graph G, budget k, inﬂuence probabilities p 2: S0 ← ∅ 3: for i = 1, 2, . . . , k do 4: v ← arg max [σ(Si−1 ∪ {v}) − σ(Si−1)]
v∈/Si
5: Si ← Si−1 ∪ {v} 6: end for 7: return Sk
In each round, the algorithm computes the marginal inﬂuence of each node v ∈/ Si and adds the maximum one to the seed set Si until |S| = K.
The inﬂuence maximization problem was ﬁrst proposed by Domingos and Richardson [31, 87] and formulated as an optimization problem by Kempe et al. [60]. It is NP-hard to determine the optimum for inﬂuence maximization but the greedy algorithm (Algorithm 15) provides a (1 − 1/e) approximation ratio for a non-negative, monotone, and submodular inﬂuence function σ(S) [60].
There are two well-known diﬀusion models: the independent cascade (IC) model and the linear threshold (LT) model. In the IC model, each active node tries to activate each inactive neighbor with a predeﬁned inﬂuence probability. In the LT model, an inactive node becomes active if the ratio of its active neighbors exceeds a predeﬁned threshold. In these models, a node can change its state from inactive to active but it cannot switch in the other direction.
Most existing algorithms for the inﬂuence maximization problem are based on the independent cascade (IC) model proposed by Goldenberg et al. [41]. The IC model starts with an initial (or seed) set of active nodes, denoted S0. For each time step t, each node v activated in step t, i.e., v ∈ St, tries to activate each inactive neighbor w. This attempt succeeds with a probability pv,w. If there are multiple nodes that try to activate node w, their attempts are sequenced in an arbitrary order. If v succeeds, then w will become active in step t + 1 and it is added in St+1. However, whether w is activated or not, v cannot make any further attempts to activate w in subsequent rounds. The process continues until no more activations are possible.
The linear threshold (LT) model is proposed by Granovetter and Schelling [43, 90]. Suppose that a node v is inﬂuenced by each neighbor w with a weight bv,w such that w: neighbor of v bv,w ≤ 1. Each node v chooses a threshold θv uniformly at random from the interval [0, 1]. Given an initial (or seed) set of active nodes, denoted S0, at each time step t, all nodes that were active in step t−1 remain active, and any node v is activated if the total weight of its active neighbors is at least θv:
w: active neighbor of v bv,w ≥ θv. This diﬀusion process ends when no more node is to be activated.
In inﬂuence maximization problems, we ﬁnd the most inﬂuential seeds to maximize inﬂuence under a certain information diﬀusion model such as the IC model, the LT model, etc.
75

5.3 Topic-Based Inﬂuence Maximization Algorithm for Unknown Graphs
We ﬁrst deﬁne our problem and brieﬂy explain our method. Then, we present two main parts of our algorithm: selecting seeds and probing nodes.
5.3.1 Problem Statement and our Method
In many existing algorithms, the whole topological structure of a social network is assumed to be provided and the complete knowledge is used to ﬁnd the optimal seed sets. However, it is known that the complete knowledge of the topological structure of a social network is typically diﬃcult to obtain [107, 70, 39, 77]. Even though the complete graph is given, the graph may change dynamically [107]. Thus, in this study, we assume that the graph structure is incomplete or can change dynamically. We ﬁnd the most inﬂuential seeds for an unknown graph while probing nodes in order to know a part of the graph structure and discover potentially promising nodes. The most related work is inﬂuence maximization for unknown graphs proposed by Mihara et al. [77]. Their work shows that a reasonable inﬂuence spread can be achieved even when knowledge of the social network topology is limited and incomplete.
Another unrealistic thing in many existing methods is that these methods do not take into account topical interests of users. In fact, users have their own interests and are more likely to be inﬂuenced by information that is related to their interests. That is, the spread of information varies depending on the topic of a post. For example, a post about cars will be spread though users who are interested in cars. It will be diﬀerent from the information spread of a post about dogs. There are some works that study topic-based inﬂuence maximization problems [49, 15, 13]. Their methods consider multiple topic distributions on nodes and a query but we will focus on one target topic and study inﬂuence maximization for a given topic.
In this study, we address a topic-based inﬂuence maximization problem for an unknown graph. Assuming that a social graph G = (V, E) is directed, V is known but E is not known, we ﬁnd the most inﬂuential seeds to maximize topic-based inﬂuence while probing nodes that may have a big audience group. For selecting a seed, instead of diﬀerentiating all individual nodes, we ﬁrst choose some features that represent a node’s proper information and relation information with respect to surrounding nodes. We will call the generalized form with the features about relation information the state. The generalized form with the features about a node’s proper information is called action. Then, we evaluate a node based on its action and state. An action value will signify how valuable it is to choose an action (a node) to activate in a given state in order to maximize the inﬂuence spread. The agent chooses a node based on its action value to activate. Since it is similar to the concept of action value in reinforcement learning [94], we use the reinforcement learning methodology to learn action values. In short, we probe nodes to discover the graph structure and choose nodes with the highest action value as seeds.
Before we move to the next subsection, we discuss the inﬂuence maximization problem and the focused crawling problem to help understand our modeling. Recall that in the focused crawling, the agent collects Web pages relevant to the target
76

topic using a frontier. The problem itself does not consider long-term eﬀects, but a reinforcement learning approach allows to estimate long-term link scores, as we have seen in the previous chapter. In the inﬂuence maximization, the agent aims to choose the most inﬂuential seeds to maximize inﬂuence. This problem already takes into account long-term values but not necessarily the planning dimension that reinforcement learning introduces.
Those two problems are based on diﬀerent objectives and have been studied in diﬀerent ways, but they have some similarities caused from structural characteristics of web graphs and the nature of tasks.
In the focused crawling, Web pages are connected by hyperlinks but they are not linked randomly. Pages are likely to be linked to topically related pages (see Section 4.2). In the inﬂuence maximization, users are also likely to be friends of other users who have similar interests. The feature selection in the following subsection is inspired by the features used in the focused crawling problem (see Section 4.3.1).
In addition, selecting seeds with the highest action values is similar to linkselections from the frontier in prioritized order in the focused crawling problem. Thus, it can have the similar issue discussed in the previous chapter and action values can be balanced in the same way as we did in the focused crawling (see Section 4.3.3). However, while the crawling agent selects a link from a frontier for each time step, the agent in the IM problem selects one probed node with the highest action values, and then depends on information diﬀusion from the selected node.
We continue the details of our modeling for the inﬂuence maximization problem in the following subsection while considering such similarities and diﬀerences.
5.3.2 Modeling and Algorithm
We ﬁrst explain how to deﬁne states and actions and to compute the value of actions in order to select seeds and then discuss how to probe nodes. The whole algorithm is shown as Algorithm 16.
5.3.2.1 Selecting Seeds
As we mentioned above, a node is generalized with some features that represent a node’s proper information and relation information with respect to surrounding nodes, called action and state, respectively. Then, we evaluate a node based on its state and action. The features of states and actions are presented in the following.
State. The state features are based on relation information with respect to surrounding nodes. Since the complete graph structure is not known in advance, each node does not know all actual parents (i.e., incoming nodes) and then we have to progressively update parent information while visiting nodes by probing or tracing activated nodes. When visiting a node, we let its child nodes know who are parents by referencing the current visiting node.
For topic (or category) based features, in order to decide whether a post is relevant to the given topic (or category), we can use a classiﬁcation method or cosine similarity between a word vector of the given topic (or category) and that of a post. When we use cosine similarity, a threshold θ has to be selected. Then, if
77

similarity is greater than the threshold θ, we can consider it relevant to the given topic.
Based on this, we can compute a posting rate of a given topic (or category) among all posts generated by a user as follows: for a user, the number of the user’s posts that are relevant to the given topic (or category) is divided by the number of all posts generated by the user.
Then, as in the previous chapter, we discretized the posting rate into 10 buckets according to value ranges: the range [0.0, 0.1] by 0, [0.1, 0.2] by 1, . . . , [0.9, 1.0] by 9.
• Average Posting Rate of All Parents for Given Topic: The average posting rate for the given topic over all parent nodes is calculated and discretized according to value ranges.
• Average Posting Rate of All Parents for Categories: First, some categories relevant to the given topic are properly preselected from a category hierarchy such as the Open Directory Project (ODP, https://www.dmoz.org/) by the system designer. Then, based on the preselected categories, the average posting rate for each category over all parent nodes is calculated and discretized according to value ranges.
• Posting Rate Change for Given Topic: The current node’s posting rate for the given topic is compared to the weighted average posting rate for the given topic over all its ancestors on the probed graph structure. The weighted average posting rate over all its ancestors is computed in an incremental manner by applying an exponential smoothing method on its parents nodes.
We denote posting rate for the given topic in node x as posting(x). The weighted average posting rate from all x’s ancestors to x, wposting(x), is obtained by wposting(x) = β · posting(x) + (1 − β) · maxx →x wposting(x ), where β(0 < β < 1) is a smoothing factor. If the current node has many parents, i.e., many path from its ancestors, the maximum average among them, maxx →x wposting(x ), is used for the update. The exponential smoothing assigns exponentially decreasing weights on past observations. In other words, recent observations are given relatively more weight than the older observations.
Then, we can calculate the posting rate change between current node z and wposting(x) where x is a parent of z: change ← posting(z)−maxx→z wposting(x).
The change helps to detect how much the posting rate of the current node for the given topic is increased or decreased than the average posting rate of its ancestors.
The posting rate change to the current node from its ancestors is discretized according to value ranges. With predeﬁned parameters δ1 and δ2, the diﬀerence within δ1 is indexed by 0, the increase by δ2 is indexed by 1, increase more than δ2 is indexed by 2, decrease by δ2 is indexed by 3, and decrease more than δ2 is indexed by 4.
78

• Distance from the Last Activated Node: The distance from the last activated node is simply calculated by adding 1 to the parent’s distance. If there are many parents, the minimum distance among them is used. The distance value is capped at 9 to keep it within a ﬁnite range.

0 distance =
1 + parent’s distance

if it is activated otherwise

(5.1)

Action. The action features are based on a node’s proper information consisting of two types of information. One is general behaviors of the user on a social network and the other is the user’s topic interest. The feature ‘Number of Children’ is a good indicator to see if a user has a big audience group or not. The feature ‘Number of Posts’ can be used to predict user’s activity. The two other features, such as ‘Posting Rate for Given Topic’ and ‘Posting Rate for Categories’, represent user’s interest.

• Number of Children: This is obtained by simply counting child nodes and the count is discretized according to 10 value ranges.
• Number of Posts: This is also obtained by simply counting posts generated by a user and the count is discretized according to 10 value ranges.
• Posting Rate for Given Topic: This is the posting rate for the given topic among all posts generated by a user and the rate is discretized according to value ranges.
• Posting Rate for Categories: Based on the preselected categories, the posting rate of each category among all posts generated by a user is computed and the rate is discretized according to value ranges.

The size of a discretized state space is (10)3 · (10)num of categories and the size of action space is (10)3 · (10)num of categories. For example, if there is just one category, the size of the state space is 104 and the size of the action space is 104.
Based on feature values discussed above, we evaluate the action value of each node. As in the previous chapter, we use gradient-descent based linear function approximation (see Section 2.4) to compute action values because the state-action space is very large. We deﬁne a weight vector w that has the same size of feature vector x(s, a). Recall that the value of action a in state s, qˆ(s, a, w), is approximated by linearly combining feature vector x(s, a) and weight vector w:

d
qˆ(s, a, w) =. w x(s, a) =. wixi(s, a)
i=1
and the weight vector w is updated as follows: wt+1 =. wt + α [rt+1 + γqˆ(st+1, at+1, wt) − qˆ(st, at, wt)] ∇qˆ(st, at, wt).

(5.2) (5.3)

Here, α (0 < α ≤ 1) is the step-size parameter that inﬂuences the rate of learning; r is a reward received for taking action a; the discount rate γ (0 ≤ γ < 1) determines

79

the present value of future rewards. st+1 is the next state and at+1 is an action in st+1 under a given policy π. We deﬁne reward r as the rate of activated child node, and (st+1, at+1) as state-action pair of a child node that has the highest action value among all child nodes.
We update weight vector w for all activated nodes. After activating a seed, the information inﬂuence is spread depending on users’ choices. Thus, the update has to be on a node explicitly activated by a learning agent. We extend the scope of action. Activating a node by the learning agent is an explicit action. We consider activating nodes by inﬂuence spread from the initially selected node as an implicit action because they yield rewards and next states in the same mechanism of the explicitly activated nodes.
For each probed node, we use the learned w to evaluate an action value of the node and select nodes with the highest action values as seeds. However, if action values of the probed nodes are computed at diﬀerent time steps and they are not synchronized with the same weight vector w, we may have the same issue as we have seen in the focused crawling problem (see Section 4.3.3). In this case, it would be good to use moderated update in order to reduce the action value diﬀerences at diﬀerent time steps by manipulating weight updates. The details of the moderated update is presented in Section 4.3.3.
5.3.2.2 Probing Nodes
As a complete knowledge of a social network is not given in advance, we need to probe nodes in order to partially know the graph. An eﬀective method of probing nodes will be to compute action values for all inactive nodes and to choose a node with the highest action value. However, if there are a huge number of nodes, the computational cost can be extremely high. Alternatively, we can use the out-degrees of nodes. In fact, a node with high out-degree means that the node has a big audience. Such node is likely to spread information more widely than nodes with low out-degree. In Sample Edge Count (SEC) [70], a biased sampling method, nodes with the highest expected degree are greedily probed. This method is eﬀective for ﬁnding hub nodes of large degree. Thus, in our algorithm, the expected degrees of nodes are initially set to 0 and we update them progressively while probing nodes and select nodes with the highest expected degrees.
The pseudocode of topic-based inﬂuence maximization for an unknown graph is shown in Algorithm 16. The expected out-degree dout of each node in V is initialized with 0 (lines 2–4). For each time, m nodes are probed and stored in C (Algorithm 17). With small probability , a state-action pair is selected uniformly at random from C. Otherwise, a state-action pair with the highest action value is selected. Then, a node from the selected pair is taken (lines 10–14). The seed node is activated (line 16) and all activated nodes from the seed node are collected in At (line 17). Since some features are based on parent information, the state and action of each activated node z are deﬁned in increasing order of distance d (line 21). Then the weight vector w is updated based on the states and actions of activated nodes. Among state-action pairs of all child nodes of z , a state-action pair with the highest action-value is selected for the next state-action pair (line 26) and then
80

it is used for updating weight vector w (line 27). If the activated node is in C, it is removed from C. If not, the expected out-degree of each parent node not in C is incremented by 1 (lines 28–34). This process repeats until the number of seeds reaches the predeﬁned budget k.
The pseudocode of probing nodes is given in Algorithm 17. For each time, we select a node that is not in C that contains all probed nodes (lines 2–6). With small probability , an inactive node is selected uniformly at random. Otherwise, a node with the highest expected out-degree dout is selected. Then, the selected node is probed and its actual out-degree is obtained (line 7). The expected out-degree of each parent node not in C is incremented by 1. To compute the action value of the probed node, the state and action of the node are deﬁned, the action value is computed and the node is added to corresponding state-action set of C (lines 11–13). This process is repeated m times.
5.4 Related Work
Some studies [40, 4, 99, 17] analyze diﬀusion patterns on social medias. Goel et al. [40] investigate the online diﬀusion structures of seven diverse do-
mains: Yahoo! Kindness, Zync, Secretary Game, Twitter News Stories, Twitter Videos, Friend Sense, and Yahoo! Voice. In spite of the heterogeneity of data, the distribution of diﬀusion patterns over all seven cases is striking in its similarity. The authors ﬁnd that in all domains large cascades are not only rare, but even when present they occur within one degree of a few dominant individuals. In particular, less than 10% of adoptions take place in cascades consisting of more than 10 nodes, and less than 10% occur in trees that extend more than two generations (depths) from the seed.
While the previous study focus on the most inﬂuential users, Bakshy et al. [4] consider all individuals and study their impact on the spread of information on Twitter. The authors quantify the inﬂuence of a given post by the number of users who subsequently repost the URL through the Twitter follower graph and describe the cascades with user attributes and past inﬂuence properties of seed users. They ﬁnd that a small fraction of posted URLs are reposted thousands of times but most posted URLs do not spread at all. The average cascade size is 1.14. The maximum depth of cascades is 9 and most URLs are not reposted at all. It implies that most events do not spread at all and large cascades are rare. They also ﬁnd that the number of followers is an informative feature, the number of tweets is also a good feature to predict user’s activity, and past performance provides the most informative set of features to predict the cascades. Spreading information using the most inﬂuential users is the most cost-eﬀective way. However, the authors ﬁnd that “ordinary inﬂuencers” – individuals who exert average, or even less-than-average inﬂuence – are under many circumstances more cost-eﬀective.
In [99], the authors propose a method to identify topics using Twitter data by detecting communities in the hashtag co-occurrence network, and to quantify the topical diversity of user interests and content. They verify which user characteristics make people inﬂuential by observing several individual properties: number of retweets, number of followers, number of tweets, content interestingness, and di-
81

versity of interests. They found that high social inﬂuence of an individual can be obtained when a user has a big audience group, produces lots of interesting content, and stays focused on a ﬁeld.
Cheng et al. [17] examine the problem of predicting the growth of cascades on photo-resharing data from Facebook. To describe the growth and spreading of cascades, ﬁve classes of features are used: the content, the original poster, the resharer, the graph structure of the cascade, and temporal characteristics of the cascade. The authors ﬁnd that the set of temporal features outperforms all other individual feature sets but it is still possible to obtain reasonable performance without the temporal features. The features of the content and the original poster become less important as more of the cascade is observed while the importance of temporal features remains relatively stable. They also ﬁnd that the greater the number of observed reshares, the better the prediction of the growth of a cascade and that breadth, rather than depth in an initial cascade is a better indicator of larger cascades.
The Inﬂuence maximization problem has been actively studied in the literature [31, 87, 60, 57, 77, 14]. The problem was ﬁrst proposed by Domingos and Richardson [31, 87] and formulated as an optimization problem by Kempe et al. [60].
IRIE [57] integrates inﬂuence ranking (IR) and inﬂuence estimation (IE) methods for the inﬂuence maximization problem. The authors use the independent cascade (IC) model and its extension IC-N model as the information diﬀusion process. The IR method generates a global inﬂuence ranking of the nodes and selects the highest ranked node as the seed. However, IR computes the inﬂuence for individual nodes. To overcome this shortcoming, the IR method is integrated with a simple inﬂuence estimation (IE) method. After one seed is selected, additional inﬂuence impact of this seed to each node in the network is computed and then the result is used to adjust next round computation of inﬂuence ranking. In experiments, IRIE is compared with PMIA [16], CELF [64], SAEDV [56], Degree, and PageRank on ﬁve real-world social networks such as ArXiv, DBLP, Epinions, Slashdot, and LiveJournal. The authors show that IRIE is much more robust and stable both in running time and memory usage than other algorithms.
In inﬂuence maximization problems, most algorithms assume that the entire topological structure of a social network is given. However, complete knowledge of the graph structure is typically diﬃcult to obtain. Mihara et al. [77] introduce an inﬂuence maximization problem for unknown graphs and propose a heuristic algorithm called IMUG for the problem. They assume that only the set of nodes is known and the set of links is unknown. The topological structure of a graph is partially obtained by probing a node to get a list of its friends. In each round, IMUG probes m nodes with the highest expected degree, selects k seed nodes with the highest expected degree and then triggers inﬂuence spread from the selected seed nodes. IMUG is simulated on ﬁve real social networks: NetHEPT, DBLP, Amazon, Facebook-small, and Facebook-large. The IC model is used as an inﬂuence cascade model. IMUG achieves 60–90% of the inﬂuence spread of the algorithms using the entire social network topology even when only 1–10% of the social network topology is known. The authors show that we can achieve a reasonable inﬂuence spread even when knowledge of the social network topology is limited and incomplete.
The probability on edges is usually acquired by learning from the real-world data and the obtained estimates always have some inaccuracy comparing to the
82

true value. The uncertainty in edge probability estimates may aﬀect the performance of the inﬂuence maximization task. Chen et al. [14] propose the problem of robust inﬂuence maximization to address the impact of uncertainty in edge probability estimates. Because of the uncertainty, the authors consider that the input to the inﬂuence maximization task is not edge inﬂuence probability on every edge of a social graph, but an interval in which the true probability may lie with high probability. The authors provide the LUGreedy algorithm that solves this problem with a solution-dependent bound. They also study uniform sampling and adaptive sampling methods based on information cascade to eﬀectively reduce the uncertainty on parameters and increase the robustness of the LUGreedy algorithm. The experimental results validate the usefulness of the information cascade based sampling method, and that robustness may be sensitive to the uncertainty of parameter space, i.e., the product of all intervals on all edges.
5.5 Future Work
This work creates several opportunities for future work. First, our method is not validated with experiments yet. Experimental evalua-
tion is left for future work. It should be based on diﬀerent diﬀusion models : the independent cascade (IC) model, the linear threshold (LT) model, etc., and diﬀerent social networks.
In the classical IM problem, the most typical application is viral marketing that a company promotes a new product in an online social network. The IM problem aims to maximize inﬂuence in such a scenario. In our study, we extended to topicbased inﬂuence maximization for an unknown graph. We can extend it again to a more realistic environment. In the real world, there is not just one company that wants to promote its product in an online social network. Many companies may competitively use viral marketing on the same social network. In such a case, the problem is extended to maximize inﬂuence in a competitive environment, which is called the competitive inﬂuence maximization problem [67, 106, 65]. Then, our concern will be how a company eﬀectively maximizes its information inﬂuence in a social media when many companies competitively spread their information in the same social media. The competitive IM problem aims to ﬁnd a strategy against opponents’ strategies. Lin et al. [67] propose a reinforcement learning approach for the competitive IM problem. They deﬁne the problem with an MDP. The states are deﬁned through some features that represent the current occupation status as well as the condition of the network, for example, number of free (or non-occupied) nodes, summation of degrees of all free nodes, maximum degree among all free nodes, etc. Actions are four strategies called degree-ﬁrst, max-weight, blocking, sub-greedy. For example, the degree-ﬁrst strategy chooses high degree nodes as seeds, the max-weight strategy chooses nodes whose overall weights of adjacent edges are maximal, etc. The learning agent chooses a strategy in a given state. In experiments, the method is tested with two scenarios that the opponent’s strategy is known and unknown and the eﬀectiveness of their method is shown. An extension of our method in such environment will be an interesting challenge for future work. Multi-agent reinforcement learning will be a good method for the competitive IM
83

problem to learn the optimal strategy against opponents’ strategies.
5.6 Conclusion
In this chapter, we addressed a topic-based inﬂuence maximization problem for an unknown graph. Assuming that the graph structure is incomplete or can change dynamically, we probe nodes that may have a big audience group, in order to know a part of the graph structure and discover potentially promising nodes. Then we ﬁnd the most inﬂuential seeds to maximize topic-based inﬂuence by using reinforcement learning. Nodes are generalized with some features and we deﬁne states and actions based on these features. Action values of nodes are learned from interaction with the environment by reinforcement learning. We then evaluate action values for each probed node and select a node with the highest action value to activate. Experimental evaluation and extension to a more realistic environment, for example the competitive inﬂuence maximization problem, are left for future work.
84

Algorithm 16 Topic-based Inﬂuence Maximization for an Unknown Graph

1: S ← ∅, A ← ∅, C ← ∅ // seed set S, active node set A, probed node set C

2: for each node z ∈ V do

3: dout(z) ← 0 4: end for

5: for t = 1 . . . k do

6: // Probe

7: ProbingNodes(C)

8:

9: // Select a seed node

10: if With probability then

11:

Select a (s, a) pair uniformly at random from C and select a node (z, s, a)

from the pair

12: else

13:

Select a (s, a) pair from C with highest action-value and select a node

(z, s, a) from the pair

14: end if

15: S ← S ∪ {(z, s, a)}

16: Activate node z

17: Create At = {(z , d) : activated node z at time t, distance d from z} 18: A ← A ∪ At 19: for d = 0 . . . max d do

20:

for each activated node (z , d ) ∈ At do

21:

Deﬁne state s and action a of z

22:

end for

23: end for

24: for each activated node (z , d) ∈ At do

25:

Get state-action pair (s , a ) from z and observe r

26:

(s , a ) ←

arg max

v(s , a , w)

(s ,a )∈State-Action(out(z ))

27:

w ← w + α [r + γqˆ(s , a , w) − qˆ(s , a , w)] ∇qˆ(s , a , w)

28:

if z ∈ C then

29:

Remove (z , d) from C

30:

else

31:

for each parent node p ∈/ C do

32:

dout(p) ← dout(p) + 1

33:

end for

34:

end if

35: end for

36: t ← t + 1

37: end for

38: return S

85

Algorithm 17 ProbingNodes

1: for j = 1 . . . m do

2: if With probability then

3:

Select inactive node z ∈/ C uniformly at random

4: else

5:

Select inactive node z = arg max{dout(z) | z ∈/ C}

z∈V

6: end if

7: dout(z) ← actual out-degree of z 8: for each parent node p ∈/ C do

9:

dout(p) ← dout(p) + 1

10: end for

11: Extract state s and action a from z

12: Calculate action-value with w

13: Add (z, s, a) to s of C with the action-value

14: end for

86

Chapter 6
Conclusion
In this chapter, we discuss the limitations of the proposed methods and some possible directions for future work. Then, we close with some remarks.
6.1 Future Work
In this thesis, we applied reinforcement learning to several applications. For these applications, our work creates multiple possible directions for future work.
In the taxi routing problem, we should take into account the non-stationarity of the taxi problem to make policies adapt to environment dynamics. In a real problem, passenger behavior changes over time. It means that goals, transition and reward probabilities of the environment can change over time.
Since reinforcement learning is basically applicable to non-stationary environment, our learning agent can adapt continually to dynamics changes. However, we cannot expect a more responsive adaptation to the changes and when the environment reverts to the previously learned dynamics, the learned knowledge in the past becomes useless. We will need a method that can explicitly address the nonstationarity. Q-learning algorithm we used is fast but it does not consider a model of the environment. It may be better to apply a model-based method and make it detect the environment changes.
In order to adapt ﬂexibly to environment dynamics, the environment model of the taxi problem may need to be divided into partial models that are stored in a library as shown in [19, 79, 89, 52]. For each time, the agent use a partial model that predicts well the environment. If the prediction error of the current model is larger than a threshold, the agent selects another model from the library. If the environment dynamics is completely diﬀerent from the existing models, it creates a new model. This will be more ﬂexibly adaptable to a non-stationary environment than selecting pre-deﬁned modes by a system designer. In addition, the taxi application we have discussed so far is based on a single agent but it has to be extended to a multi-agent setting. In a multi-agent environment, it will be important for the agent to have an ability to detect non-stationary opponents and learn optimal policies against changed opponent strategies. When opponent strategies are not known a priori, the agent has to adapt to the new environment. Instead of ﬁxed models, the ﬂexible models proposed above will be able to deal with such non-stationary problems.
87

In the focused crawling problem, the most obvious is to apply our algorithm in larger and various datasets, such as full English Wikipedia and dataset from the site http://commoncrawl.org/, etc. In our work, we used a database dump of Simple English Wikipedia provided by the site https://dumps.wikimedia.org/. The dataset was suﬃciently good to verify the eﬀectiveness of reinforcement learning based crawler but we have to consider a bit larger and more realistic environments.
Another interesting possibility is to build up an eﬃcient mechanism for category selection. Among state and action features, categories related to a target topic are manually pre-selected by a system designer. Since poor feature selection may result in poor performance, it is very important to select appropriate categories for the target topic. The current system relies on human knowledge and intuition about the speciﬁc domain. They should be selected in an intelligent and automatic way.
Finally, we can consider multiple crawling agents in order to accelerate crawling performance. One simple method using multiple agents is that all agents are completely independent and they do not share any information including the frontier. In fact, when the agents explore diﬀerent parts of the environment, they will have distinct scoring policies with respect to their own value functions because they have diﬀerent experience. An alternative is to share information between agents such as the frontier and scoring policies. In that case, scoring policies have to be merged in some way. A few research works [45, 100, 33] show that combining the diﬀerent policies outperforms a single agent.
In the inﬂuence maximization problem, our method is not validated with experiments yet. Experimental evaluation is left for future work. It should be based on diﬀerent diﬀusion models and diﬀerent social medias.
In our study, we extended the classical IM problem with incomplete knowledge of graph structure and topic-based user’s interest. Assuming that the graph structure is incomplete or can change dynamically, we addressed a topic-based inﬂuence maximization problem for an unknown graph. We can extend it again to more realistic environment. In the real world, there is not just one company that wants to promote its product in an online social network. Many companies may competitively use viral marketing on the same social network. We call such problem the competitive inﬂuence maximization problem [67, 106, 65]. Then, our concern will be how a company eﬀectively maximizes its information inﬂuence in a social media when many companies competitively spread their information in the same social media. The competitive IM problem aims to ﬁnd a strategy against opponent’ strategies. Lin et al. [67] propose an reinforcement learning approach for the competitive IM problem. An extension of our method in such environment will be an interesting challenge for future work. Multi-agent reinforcement learning will be a good method for the competitive IM problem to learn the optimal strategy against opponents’ strategies.
6.2 Conclusion
In this thesis, we applied reinforcement learning methods to sequential decision making problems in dynamic environments and explored several diﬀerent reinforcement learning methods such as a model-free method, a model-based method, and a linear function approximation method. There are many other diﬀerent methods presented
88

in the literature. We cannot say which algorithm is truly better than others in general. However, we have to choose a right representation of states and actions and a right method for the given problem and its domain because the performance of learning is inﬂuenced by the used representation and method.
We tried to use an appropriate method for each application. For instance, in the taxi routing problem, a tabular based model-free method is used and it is suﬃciently good to learn value functions for the problem but we can also use a model-based method when considering the non-stationarity of the problem. However, a function approximation method will not be necessary for this problem because the problem has only position features. If there are many features, a tabular based model-free method is not suﬃcient to store all state-action pairs and then it needs to extend with an approximate method. In the focused crawling problem and the inﬂuence maximization problem, we used a linear function approximation method. Since the state and action spaces are large, we cannot store all state-action values in tabular forms. Thus, a tabular based model-free method is not used. A model-based method is also diﬃcult to apply to the problems because actions are very noisy. Even though we select a proper method for a given problem, there may be some things that do not match well with the nature of task, especially if the problem is under slightly diﬀerent conditions or assumptions. In that case, the selected method has to be adapted to the problem. For example, in the focused crawling problem and the inﬂuence maximization problem, the learning algorithms had to be tuned for their tasks. Another important factor that inﬂuences learning performance is how to represent states and actions. In the taxi routing problem, the state and action spaces are clear to deﬁne. However, it may not always be clear beforehand which features to use for a given problem if the problem is complex and hard to model in an MDP or if it is diﬃcult to know what characteristics the environment has. For example, in the focused crawling problem and the inﬂuence maximization problem, it was not straightforward to select features that represent states and actions. As we have seen through this thesis, reinforcement learning is a good method to solve a sequential decision making problem in a dynamic environment. It is important to choose a good representation of states and actions and an appropriate method for a given problem.
89

90

Bibliography
[1] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng. An application of reinforcement learning to aerobatic helicopter ﬂight. In B. Scho¨lkopf, J. C. Platt, and T. Hoﬀman, editors, Advances in Neural Information Processing Systems 19, pages 1–8. MIT Press, 2007.
[2] C. C. Aggarwal, F. Al-Garawi, and P. S. Yu. Intelligent crawling on the World Wide Web with arbitrary predicates. In WWW, 2001.
[3] G. Almpanidis, C. Kotropoulos, and I. Pitas. Combining text and link analysis for focused crawling. An application for vertical search engines. Inf. Syst., 32(6), 2007.
[4] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts. Everyone’s an inﬂuencer: Quantifying inﬂuence on twitter. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM ’11, pages 65–74, New York, NY, USA, 2011. ACM.
[5] A. Baranes and P. Y. Oudeyer. R-iac: Robust intrinsically motivated exploration and active learning. IEEE Transactions on Autonomous Mental Development, 1(3):155–169, Oct 2009.
[6] D. Bergmark, C. Lagoze, and A. Sbityakov. Focused crawls, tunneling, and digital libraries. In ECDL, 2002.
[7] C. Boutilier, R. Dearden, and M. Goldszmidt. Exploiting structure in policy construction. In Proceedings of the 14th International Joint Conference on Artiﬁcial Intelligence - Volume 2, IJCAI’95, pages 1104–1111, San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc.
[8] C. Boutilier, R. Dearden, and M. Goldszmidt. Stochastic dynamic programming with factored representations. Artif. Intell., 121(1-2):49–107, Aug. 2000.
[9] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal reinforcement learning. JMLR, 3, 2003.
[10] S. Chakrabarti, M. van den Berg, and B. Dom. Focused crawling: A new approach to topic-speciﬁc web resource discovery. In WWW, 1999.
[11] D. Chakraborty and P. Stone. Structure learning in ergodic factored mdps without knowledge of the transition function’s in-degree. In L. Getoor and T. Scheﬀer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 737–744, New York, NY, USA, 2011. ACM.
91

[12] M. Chau and H. Chen. A machine learning approach to web page ﬁltering using content and structure analysis. Decis. Support Syst., 44(2), 2008.
[13] S. Chen, J. Fan, G. Li, J. Feng, K.-l. Tan, and J. Tang. Online topic-aware inﬂuence maximization. Proc. VLDB Endow., 8(6):666–677, Feb. 2015.
[14] W. Chen, T. Lin, Z. Tan, M. Zhao, and X. Zhou. Robust inﬂuence maximization. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pages 795–804, New York, NY, USA, 2016. ACM.
[15] W. Chen, T. Lin, and C. Yang. Real-time topic-aware inﬂuence maximization using preprocessing. Computational Social Networks, 3(1):8, Nov 2016.
[16] W. Chen, C. Wang, and Y. Wang. Scalable inﬂuence maximization for prevalent viral marketing in large-scale social networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’10, pages 1029–1038, New York, NY, USA, 2010. ACM.
[17] J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proceedings of the 23rd International Conference on World Wide Web, WWW ’14, pages 925–936, New York, NY, USA, 2014. ACM.
[18] S. P. M. Choi, D.-Y. Yeung, and N. L. Zhang. An environment model for nonstationary reinforcement learning. In S. A. Solla, T. K. Leen, and K. Mu¨ller, editors, Advances in Neural Information Processing Systems 12, pages 987– 993. MIT Press, 2000.
[19] B. C. da Silva, E. W. Basso, A. L. C. Bazzan, and P. M. Engel. Dealing with non-stationary environments using context detection. In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06, pages 217– 224, New York, NY, USA, 2006. ACM.
[20] P. Dai and J. Goldsmith. Topological value iteration algorithm for markov decision processes. In Proceedings of the 20th International Joint Conference on Artiﬁcal Intelligence, IJCAI’07, pages 1860–1865, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.
[21] P. Dai and E. A. Hansen. Prioritizing bellman backups without a priority queue. In Proceedings of the Seventeenth International Conference on International Conference on Automated Planning and Scheduling, ICAPS’07, pages 113–119. AAAI Press, 2007.
[22] B. D. Davison. Topical locality in the web. In SIGIR, 2000.
[23] T. Degris and O. Sigaud. Factored Markov Decision Processes, pages 99–126. John Wiley & Sons, Inc., 2013.
[24] T. Degris, O. Sigaud, and P.-H. Wuillemin. Learning the structure of factored markov decision processes in reinforcement learning problems. In Proceedings
92

of the 23rd International Conference on Machine Learning, ICML ’06, pages 257–264, New York, NY, USA, 2006. ACM.
[25] J. S. Dibangoye, B. Chaib-draa, and A.-i. Mouaddib. A novel prioritization technique for solving markov decision processes. In FLAIRS Conference, pages 537–542, 2008.
[26] T. G. Dietterich. The MAXQ method for hierarchical reinforcement learning. In ICML, 1998.
[27] M. Diligenti, F. Coetzee, S. Lawrence, C. L. Giles, and M. Gori. Focused crawling using context graphs. In VLDB, 2000.
[28] C. Diuk, L. Li, and B. R. Leﬄer. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 249–256, New York, NY, USA, 2009. ACM.
[29] C. Diuk, A. L. Strehl, and M. L. Littman. A hierarchical approach to eﬃcient reinforcement learning in deterministic domains. In AAMAS, 2006.
[30] P. Domingos and G. Hulten. Mining high-speed data streams. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’00, pages 71–80, New York, NY, USA, 2000. ACM.
[31] P. Domingos and M. Richardson. Mining the network value of customers. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’01, pages 57–66, New York, NY, USA, 2001. ACM.
[32] M. Ester, M. Groß, and H.-P. Kriegel. Focused web crawling: A generic framework for specifying the user interest and for adaptive crawling strategies. In VLDB, 2001.
[33] S. Faußer and F. Schwenker. Ensemble methods for reinforcement learning with function approximation. In Proceedings of the 10th International Conference on Multiple Classiﬁer Systems, MCS’11, pages 56–65, Berlin, Heidelberg, 2011. Springer-Verlag.
[34] M. D. Garcia-Hernandez, J. Ruiz-Pinales, E. Onaindia, J. G. Avin˜a Cervantes, S. Ledesma-Orozco, E. Alvarado-Mendez, and A. Reyes-Ballesteros. New prioritized value iteration for markov decision processes. Artif. Intell. Rev., 37(2):157–167, Feb. 2012.
[35] A. Geramifard, F. Doshi, J. Redding, N. Roy, and J. P. How. Online discovery of feature dependencies. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML’11, pages 881–888, USA, 2011. Omnipress.
93

[36] A. Geramifard, T. J. Walsh, N. Roy, and J. P. How. Batch-ifdd for representation expansion in large mdps. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, UAI’13, pages 242–251, Arlington, Virginia, United States, 2013. AUAI Press.
[37] M. Ghavamzadeh and S. Mahadevan. A multiagent reinforcement learning algorithm by dynamically merging Markov decision processes. In AAMAS, 2002.
[38] M. Ghavamzadeh and S. Mahadevan. Learning to communicate and act using hierarchical reinforcement learning. In AAMAS, 2004.
[39] M. Gjoka, M. Kurant, C. T. Butts, and A. Markopoulou. Walking in facebook: A case study of unbiased sampling of osns. In Proceedings of the 29th Conference on Information Communications, INFOCOM’10, pages 2498–2506, Piscataway, NJ, USA, 2010. IEEE Press.
[40] S. Goel, D. J. Watts, and D. G. Goldstein. The structure of online diﬀusion networks. In Proceedings of the 13th ACM Conference on Electronic Commerce, EC ’12, pages 623–638, New York, NY, USA, 2012. ACM.
[41] J. Goldenberg, B. Libai, and E. Muller. Talk of the network: A complex systems look at the underlying process of word-of-mouth. Marketing Letters, 12(3):211–223, Aug 2001.
[42] G. Gouriten, S. Maniu, and P. Senellart. Scalable, generic, and adaptive systems for focused crawling. In HyperText, pages 35–45, 2014.
[43] M. Granovetter. Threshold models of collective behavior. American Journal of Sociology, 83(6):1420–1443, 1978.
[44] A. Grigoriadis and G. Paliouras. Focused crawling using temporal diﬀerencelearning. In G. A. Vouros and T. Panayiotopoulos, editors, SETN, 2004.
[45] M. Grounds and D. Kudenko. Parallel reinforcement learning with linear function approximation. In Proceedings of the 5th , 6th and 7th European Conference on Adaptive and Learning Agents and Multi-agent Systems: Adaptation and Multi-agent Learning, ALAMAS’05/ALAMAS’06/ALAMAS’07, pages 60–74, Berlin, Heidelberg, 2008. Springer-Verlag.
[46] M. Grze´s and J. Hoey. Eﬃcient planning in r-max. In The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS ’11, pages 963–970, Richland, SC, 2011. International Foundation for Autonomous Agents and Multiagent Systems.
[47] M. Grze´s and J. Hoey. On the convergence of techniques that improve value iteration. In The 2013 International Joint Conference on Neural Networks (IJCNN), pages 1–8, Aug 2013.
[48] C. Guestrin, R. Patrascu, and D. Schuurmans. Algorithm-directed exploration for model-based reinforcement learning in factored mdps. In In Proceedings
94

of the International Conference on Machine Learning, pages 235–242. Morgan Kaufmann Publishers Inc, 2002.
[49] J. Guo, P. Zhang, C. Zhou, Y. Cao, and L. Guo. Personalized inﬂuence maximization on social networks. In Proceedings of the 22Nd ACM International Conference on Information & Knowledge Management, CIKM ’13, pages 199– 208, New York, NY, USA, 2013. ACM.
[50] M. Han, P. Senellart, S. Bressan, and H. Wu. Routing an autonomous taxi with reinforcement learning. In Proc. CIKM, Indianapolis, USA, Oct. 2016. Industry track, short paper.
[51] M. Han, P. Senellart, and P.-H. Wuillemin. Focused crawling through reinforcement learning. In Proc. ICWE, June 2018.
[52] P. Hernandez-Leal, M. Taylor, B. Rosman, L. E. Sucar, and E. M. de Cote. Identifying and tracking switching, non-stationary opponents: A bayesian approach, 2016.
[53] T. Hester and P. Stone. Generalized model learning for reinforcement learning in factored domains. In AAMAS, 2009.
[54] T. Hester and P. Stone. Learning and Using Models, pages 111–141. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
[55] G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’01, pages 97–106, New York, NY, USA, 2001. ACM.
[56] Q. Jiang, G. Song, G. Cong, Y. Wang, W. Si, and K. Xie. Simulated annealing based inﬂuence maximization in social networks. In Proceedings of the TwentyFifth AAAI Conference on Artiﬁcial Intelligence, AAAI’11, pages 127–132. AAAI Press, 2011.
[57] K. Jung, W. Heo, and W. Chen. Irie: Scalable and robust inﬂuence maximization in social networks. In Proceedings of the 2012 IEEE 12th International Conference on Data Mining, ICDM ’12, pages 918–923, Washington, DC, USA, 2012. IEEE Computer Society.
[58] M. Kearns and D. Koller. Eﬃcient reinforcement learning in factored mdps. In Proceedings of the 16th International Joint Conference on Artiﬁcial Intelligence - Volume 2, IJCAI’99, pages 740–747, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc.
[59] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Mach. Learn., 49(2-3):209–232, Nov. 2002.
[60] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of inﬂuence through a social network. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’03, pages 137–146, New York, NY, USA, 2003. ACM.
95

[61] J. M. Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM, 46(5), 1999.
[62] J. Z. Kolter and A. Y. Ng. Near-bayesian exploration in polynomial time. In A. P. Danyluk, L. Bottou, and M. L. Littman, editors, Proceedings of the 26th International Conference on Machine Learning (ICML-09), page 65, 2009.
[63] G. Konidaris and A. Barto. Building portable options: Skill transfer in reinforcement learning. In Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence, pages 895–900, 2007.
[64] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and N. Glance. Cost-eﬀective outbreak detection in networks. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, pages 420–429, New York, NY, USA, 2007. ACM.
[65] H. Li, S. S. Bhowmick, J. Cui, Y. Gao, and J. Ma. Getreal: Towards realistic selection of inﬂuence maximization strategies in competitive networks. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD ’15, pages 1525–1537, New York, NY, USA, 2015. ACM.
[66] Y. Li, J. Fan, D. Zhang, and K.-L. Tan. Discovering your selling points: Personalized social inﬂuential tags exploration. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD ’17, pages 619–634, New York, NY, USA, 2017. ACM.
[67] S.-C. Lin, S.-D. Lin, and M.-S. Chen. A learning-based framework to handle multi-round multi-party inﬂuence maximization on social networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages 695–704, New York, NY, USA, 2015. ACM.
[68] M. Lopes, T. Lang, M. Toussaint, and P. yves Oudeyer. Exploration in modelbased reinforcement learning by empirically estimating learning progress. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 206–214. Curran Associates, Inc., 2012.
[69] M. Lopes and P. Y. Oudeyer. The strategic student approach for life-long exploration and learning. In 2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL), pages 1–8, Nov 2012.
[70] A. S. Maiya and T. Y. Berger-Wolf. Beneﬁts of bias: Towards better characterization of network sampling. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 105–113, New York, NY, USA, 2011. ACM.
[71] H. B. McMahan and G. J. Gordon. Fast exact planning in markov decision processes. In ICAPS, pages 151–160, 2005.
96

[72] F. Menczer. Lexical and semantic clustering by web links. J. Am. Soc. Inf. Sci. Technol., 55(14), 2004.
[73] F. Menczer. Web crawling. In B. Liu, editor, Web Data Mining: Exploring Hyperlink, Content and Usage Data. Springer, 2007.
[74] F. Menczer and R. K. Belew. Adaptive retrieval agents: Internalizing local contextand scaling up to the web. Mach. Learn., 39(2-3), 2000.
[75] F. Menczer, G. Pant, and P. Srinivasan. Topical web crawlers: Evaluating adaptive algorithms. ACM Trans. Internet Technol., 4(4), 2004.
[76] R. Meusel, P. Mika, and R. Blanco. Focused crawling for structured data. In CIKM, 2014.
[77] S. Mihara, S. Tsugawa, and H. Ohsaki. Inﬂuence maximization problem for unknown social networks. In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015, ASONAM ’15, pages 1539–1546, New York, NY, USA, 2015. ACM.
[78] A. W. Moore and C. G. Atkeson. Prioritized sweeping: Reinforcement learning with less data and less time. Mach. Learn., 13(1):103–130, Oct. 1993.
[79] T. T. Nguyen, T. Silander, and T.-Y. Leong. Transferring expectations in model-based reinforcement learning. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2, NIPS’12, pages 2555–2563, USA, 2012. Curran Associates Inc.
[80] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab, 1999.
[81] G. Pant, P. Srinivasan, and F. Menczer. Exploration versus exploitation in topic driven crawlers. In WWW Workshop on Web Dynamics, 2002.
[82] R. Parr, C. Painter-Wakeﬁeld, L. Li, and M. Littman. Analyzing feature generation for value-function approximation. In Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pages 737–744, New York, NY, USA, 2007. ACM.
[83] J. Peng and R. J. Williams. Eﬃcient learning and planning within the dyna framework. Adapt. Behav., 1(4):437–454, Apr. 1993.
[84] M. Qu, H. Zhu, J. Liu, G. Liu, and H. Xiong. A cost-eﬀective recommender system for taxi drivers. In KDD, 2014.
[85] R. Rana and F. S. Oliveira. Real-time dynamic pricing in a non-stationary environment using model-free reinforcement learning. Omega, 47:116 – 126, 2014.
[86] J. Rennie and A. McCallum. Using reinforcement learning to spider the web eﬃciently. In ICML, 1999.
97

[87] M. Richardson and P. Domingos. Mining knowledge-sharing sites for viral marketing. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pages 61–70, New York, NY, USA, 2002. ACM.
[88] M. Riedmiller, T. Gabel, R. Hafner, and S. Lange. Reinforcement learning for robot soccer. Autonomous Robots, 27(1):55–73, Jul 2009.
[89] B. Rosman, M. Hawasly, and S. Ramamoorthy. Bayesian policy reuse. Machine Learning, 104(1):99–127, Jul 2016.
[90] T. C. Schelling. Micromotives and macrobehavior. WW Norton & Company, 2006.
[91] S. Singh and D. Bertsekas. Reinforcement learning for dynamic channel allocation in cellular telephone systems. In Proceedings of the 9th International Conference on Neural Information Processing Systems, NIPS’96, pages 974– 980, Cambridge, MA, USA, 1996. MIT Press.
[92] A. L. Strehl, C. Diuk, and M. L. Littman. Eﬃcient structure learning in factored-state mdps. In AAAI, 2007.
[93] R. S. Sutton. Integrated architecture for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference (1990) on Machine Learning, pages 216–224, San Francisco, CA, USA, 1990. Morgan Kaufmann Publishers Inc.
[94] R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, 1998.
[95] G. Tesauro, R. Das, H. Chan, J. Kephart, D. Levine, F. Rawson, and C. Lefurgy. Managing power consumption and performance of computing systems using reinforcement learning. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1497–1504. Curran Associates, Inc., 2008.
[96] P. E. Utgoﬀ, N. C. Berkman, and J. A. Clouse. Decision tree induction based on eﬃcient tree restructuring. Mach. Learn., 29(1):5–44, Oct. 1997.
[97] C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Cambridge, UK, May 1989.
[98] C. J. C. H. Watkins and P. Dayan. Technical note: Q-learning. Mach. Learn., 8(3-4), 1992.
[99] L. Weng and F. Menczer. Topicality and impact in social media: Diverse messages, focused messengers. PLOS ONE, 10(2):1–17, 02 2015.
[100] M. A. Wiering and H. van Hasselt. Ensemble algorithms in reinforcement learning. Trans. Sys. Man Cyber. Part B, 38(4):930–936, Aug. 2008.
98

[101] D. Wingate and K. D. Seppi. P3vi: A partitioned, prioritized, parallel value iterator. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning, ICML ’04, pages 109–, New York, NY, USA, 2004. ACM.
[102] D. Wingate and K. D. Seppi. Prioritization methods for accelerating mdp solvers. J. Mach. Learn. Res., 6:851–881, Dec. 2005.
[103] J. Yuan, Y. Zheng, X. Xie, and G. Sun. T-drive: Enhancing driving directions with taxi drivers’ intelligence. TKDE, 25(1), 2013.
[104] N. J. Yuan, Y. Zheng, L. Zhang, and X. Xie. T-ﬁnder: A recommender system for ﬁnding passengers and vacant taxis. TKDE, 25(10), 2013.
[105] Y. Zheng. Trajectory data mining: An overview. ACM Trans. Intell. Syst. Technol., 6(3), 2015.
[106] Y. Zhu, D. Li, and Z. Zhang. Minimum cost seed set for competitive social inﬂuence. In IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications, pages 1–9, April 2016.
[107] H. Zhuang, Y. Sun, J. Tang, J. Zhang, and X. Sun. Inﬂuence maximization in dynamic social networks. In 2013 IEEE 13th International Conference on Data Mining, pages 1313–1318, Dec 2013.
99

