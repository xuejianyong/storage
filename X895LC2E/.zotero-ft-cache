INTRODUCTION TO MACHINE LEARNING

Introduction to Machine Learning
Alex Smola and S.V.N. Vishwanathan
Yahoo! Labs Santa Clara
–and– Departments of Statistics and Computer Science
Purdue University –and–
College of Engineering and Computer Science Australian National University

published by the press syndicate of the university of cambridge The Pitt Building, Trumpington Street, Cambridge, United Kingdom cambridge university press The Edinburgh Building, Cambridge CB2 2RU, UK 40 West 20th Street, New York, NY 10011–4211, USA 477 Williamstown Road, Port Melbourne, VIC 3207, Australia Ruiz de Alarc´on 13, 28014 Madrid, Spain Dock House, The Waterfront, Cape Town 8001, South Africa http://www.cambridge.org c Cambridge University Press 2008 This book is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press. First published 2008 Printed in the United Kingdom at the University Press, Cambridge
Typeface Monotype Times 10/13pt System LATEX 2ε [Alexander J. Smola and S.V.N. Vishwanathan]
A catalogue record for this book is available from the British Library Library of Congress Cataloguing in Publication data available ISBN 0 521 82583 0 hardback
Author: vishy Revision: 252 Timestamp: October 1, 2010 URL: svn://smola@repos.stat.purdue.edu/thebook/trunk/Book/thebook.tex

Contents

Preface

page 1

1 Introduction

3

1.1 A Taste of Machine Learning

3

1.1.1 Applications

3

1.1.2 Data

7

1.1.3 Problems

9

1.2 Probability Theory

12

1.2.1 Random Variables

12

1.2.2 Distributions

13

1.2.3 Mean and Variance

15

1.2.4 Marginalization, Independence, Conditioning, and

Bayes Rule

16

1.3 Basic Algorithms

20

1.3.1 Naive Bayes

22

1.3.2 Nearest Neighbor Estimators

24

1.3.3 A Simple Classiﬁer

27

1.3.4 Perceptron

29

1.3.5 K-Means

32

2 Density Estimation

37

2.1 Limit Theorems

37

2.1.1 Fundamental Laws

38

2.1.2 The Characteristic Function

42

2.1.3 Tail Bounds

45

2.1.4 An Example

48

2.2 Parzen Windows

51

2.2.1 Discrete Density Estimation

51

2.2.2 Smoothing Kernel

52

2.2.3 Parameter Estimation

54

2.2.4 Silverman’s Rule

57

2.2.5 Watson-Nadaraya Estimator

59

2.3 Exponential Families

60

2.3.1 Basics

60

v

vi

0 Contents

2.3.2 Examples

62

2.4 Estimation

66

2.4.1 Maximum Likelihood Estimation

66

2.4.2 Bias, Variance and Consistency

68

2.4.3 A Bayesian Approach

71

2.4.4 An Example

75

2.5 Sampling

77

2.5.1 Inverse Transformation

78

2.5.2 Rejection Sampler

82

3 Optimization

91

3.1 Preliminaries

91

3.1.1 Convex Sets

92

3.1.2 Convex Functions

92

3.1.3 Subgradients

96

3.1.4 Strongly Convex Functions

97

3.1.5 Convex Functions with Lipschitz Continous Gradient 98

3.1.6 Fenchel Duality

98

3.1.7 Bregman Divergence

100

3.2 Unconstrained Smooth Convex Minimization

102

3.2.1 Minimizing a One-Dimensional Convex Function 102

3.2.2 Coordinate Descent

104

3.2.3 Gradient Descent

104

3.2.4 Mirror Descent

108

3.2.5 Conjugate Gradient

111

3.2.6 Higher Order Methods

115

3.2.7 Bundle Methods

121

3.3 Constrained Optimization

125

3.3.1 Projection Based Methods

125

3.3.2 Lagrange Duality

127

3.3.3 Linear and Quadratic Programs

131

3.4 Stochastic Optimization

135

3.4.1 Stochastic Gradient Descent

136

3.5 Nonconvex Optimization

137

3.5.1 Concave-Convex Procedure

137

3.6 Some Practical Advice

139

4 Online Learning and Boosting

143

4.1 Halving Algorithm

143

4.2 Weighted Majority

144

Contents

vii

5 Conditional Densities

149

5.1 Logistic Regression

150

5.2 Regression

151

5.2.1 Conditionally Normal Models

151

5.2.2 Posterior Distribution

151

5.2.3 Heteroscedastic Estimation

151

5.3 Multiclass Classiﬁcation

151

5.3.1 Conditionally Multinomial Models

151

5.4 What is a CRF?

152

5.4.1 Linear Chain CRFs

152

5.4.2 Higher Order CRFs

152

5.4.3 Kernelized CRFs

152

5.5 Optimization Strategies

152

5.5.1 Getting Started

152

5.5.2 Optimization Algorithms

152

5.5.3 Handling Higher order CRFs

152

5.6 Hidden Markov Models

153

5.7 Further Reading

153

5.7.1 Optimization

153

6 Kernels and Function Spaces

155

6.1 The Basics

155

6.1.1 Examples

156

6.2 Kernels

161

6.2.1 Feature Maps

161

6.2.2 The Kernel Trick

161

6.2.3 Examples of Kernels

161

6.3 Algorithms

161

6.3.1 Kernel Perceptron

161

6.3.2 Trivial Classiﬁer

161

6.3.3 Kernel Principal Component Analysis

161

6.4 Reproducing Kernel Hilbert Spaces

161

6.4.1 Hilbert Spaces

163

6.4.2 Theoretical Properties

163

6.4.3 Regularization

163

6.5 Banach Spaces

164

6.5.1 Properties

164

6.5.2 Norms and Convex Sets

164

7 Linear Models

165

7.1 Support Vector Classiﬁcation

165

viii

0 Contents

7.1.1 A Regularized Risk Minimization Viewpoint

170

7.1.2 An Exponential Family Interpretation

170

7.1.3 Specialized Algorithms for Training SVMs

172

7.2 Extensions

177

7.2.1 The ν trick

177

7.2.2 Squared Hinge Loss

179

7.2.3 Ramp Loss

180

7.3 Support Vector Regression

181

7.3.1 Incorporating General Loss Functions

184

7.3.2 Incorporating the ν Trick

186

7.4 Novelty Detection

186

7.5 Margins and Probability

189

7.6 Beyond Binary Classiﬁcation

189

7.6.1 Multiclass Classiﬁcation

190

7.6.2 Multilabel Classiﬁcation

191

7.6.3 Ordinal Regression and Ranking

192

7.7 Large Margin Classiﬁers with Structure

193

7.7.1 Margin

193

7.7.2 Penalized Margin

193

7.7.3 Nonconvex Losses

193

7.8 Applications

193

7.8.1 Sequence Annotation

193

7.8.2 Matching

193

7.8.3 Ranking

193

7.8.4 Shortest Path Planning

193

7.8.5 Image Annotation

193

7.8.6 Contingency Table Loss

193

7.9 Optimization

193

7.9.1 Column Generation

193

7.9.2 Bundle Methods

193

7.9.3 Overrelaxation in the Dual

193

7.10 CRFs vs Structured Large Margin Models

194

7.10.1 Loss Function

194

7.10.2 Dual Connections

194

7.10.3 Optimization

194

Appendix 1 Linear Algebra and Functional Analysis

197

Appendix 2 Conjugate Distributions

201

Appendix 3 Loss Functions

203

Bibliography

221

Preface
Since this is a textbook we biased our selection of references towards easily accessible work rather than the original references. While this may not be in the interest of the inventors of these concepts, it greatly simpliﬁes access to those topics. Hence we encourage the reader to follow the references in the cited works should they be interested in ﬁnding out who may claim intellectual ownership of certain key ideas.
1

2 Structure of the Book

Introduction

0 Preface

Duality and Estimation

Density Estimation
Graphical Models
Conditional Densities

Linear Models

Kernels

Optimization

Moment Methods

Conditional Random Fields

Structured Estimation

Introduction

Duality and Estimation

Density Estimation
Graphical Models
Conditional Densities

Linear Models

Kernels

Optimization

Moment Methods

Conditional Random Fields

Structured Estimation

Reinforcement Learning

Canberra, August 2008

Reinforcement Learning

Introduction

Duality and Estimation

Density Estimation
Graphical Models
Conditional Densities

Linear Models

Kernels

Optimization

Moment Methods

Conditional Random Fields

Structured Estimation

Reinforcement Learning

1
Introduction
Over the past two decades Machine Learning has become one of the mainstays of information technology and with that, a rather central, albeit usually hidden, part of our life. With the ever increasing amounts of data becoming available there is good reason to believe that smart data analysis will become even more pervasive as a necessary ingredient for technological progress.
The purpose of this chapter is to provide the reader with an overview over the vast range of applications which have at their heart a machine learning problem and to bring some degree of order to the zoo of problems. After that, we will discuss some basic tools from statistics and probability theory, since they form the language in which many machine learning problems must be phrased to become amenable to solving. Finally, we will outline a set of fairly basic yet eﬀective algorithms to solve an important problem, namely that of classiﬁcation. More sophisticated tools, a discussion of more general problems and a detailed analysis will follow in later parts of the book.
1.1 A Taste of Machine Learning Machine learning can appear in many guises. We now discuss a number of applications, the types of data they deal with, and ﬁnally, we formalize the problems in a somewhat more stylized fashion. The latter is key if we want to avoid reinventing the wheel for every new application. Instead, much of the art of machine learning is to reduce a range of fairly disparate problems to a set of fairly narrow prototypes. Much of the science of machine learning is then to solve those problems and provide good guarantees for the solutions.
1.1.1 Applications Most readers will be familiar with the concept of web page ranking. That is, the process of submitting a query to a search engine, which then ﬁnds webpages relevant to the query and which returns them in their order of relevance. See e.g. Figure 1.1 for an example of the query results for “machine learning”. That is, the search engine returns a sorted list of webpages given a query. To achieve this goal, a search engine needs to ‘know’ which
3

4
Web Images Maps News Shopping Gmail more !

1 Introduction
Sign in

Google

machine learning

Search

Advanced Search Preferences

Web Scholar

Results 1 - 10 of about 10,500,000 for machine learning. (0.06 seconds)

Machine learning - Wikipedia, the free encyclopedia As a broad subfield of artificial intelligence, machine learning is concerned with the design and development of algorithms and techniques that allow ... en.wikipedia.org/wiki/Machine_learning - 43k - Cached - Similar pages
Machine Learning textbook Machine Learning is the study of computer algorithms that improve automatically through experience. Applications range from datamining programs that ... www.cs.cmu.edu/~tom/mlbook.html - 4k - Cached - Similar pages

Sponsored Links
Machine Learning Google Sydney needs machine learning experts. Apply today! www.google.com.au/jobs

machine learning www.aaai.org/AITopics/html/machine.html - Similar pages

Machine Learning A list of links to papers and other resources on machine learning. www.machinelearning.net/ - 14k - Cached - Similar pages

Introduction to Machine Learning This page has pointers to my draft book on Machine Learning and to its individual chapters. They can be downloaded in Adobe Acrobat format. ... ai.stanford.edu/~nilsson/mlbook.html - 15k - Cached - Similar pages

Fig. 1.1.

Machine Learning - Artificial Intelligence (incl. Robotics ... Machine Learning - Artificial Intelligence. Machine Learning is an international forum for research on computational approaches to learning.
The 5 top scoring webpages for the query “machine learning” www.springer.com/computer/artificial/journal/10994 - 39k - Cached - Similar pages
Machine Learning (Theory) Graduating students in Statistics appear to be at a substantial handicap compared to graduating students in Machine Learning, despite being in substantially ... hunch.net/ - 94k - Cached - Similar pages

pages are relevant and which pages match the query. Such knowledge can be Amazon.com: Machine Learning: Tom M. Mitchell: Books Amazon.com: Machine Learning: Tom M. Mitchell: Books. www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077 - 210k -
gained fromCacsheedv- Seimrilaar palgessources: the link structure of webpages, their content, Machine Learning Journal
the frequency with which users will follow the suggested links in a query, or Machine Learning publishes articles on the mechanisms through which intelligent systems improve their performance over time. We invite authors to submit ... pages.stern.nyu.edu/~fprovost/MLJ/ - 3k - Cached - Similar pages
from exampClSe2s29:oMfachqinue Leearrineings in combination with manually ranked webpages.
STANFORD. CS229 Machine Learning Autumn 2007. Announcements. Final reports from
Increasingly machine learning rather than guesswork and clever engineering this year's class projects have been posted here. ... cs229.stanford.edu/ - 10k - Cached - Similar pages
is used to automate the process of designing a good search engine [RPB06].

A rather related application1 2 i3s4 5c6o7l8la9 1b0 orNeaxttive ﬁltering. Internet bookstores such as Amazon, or video rental sites such as Netﬂix use this informa-

tion extensively to entice users to machine learning purchase adSdearichtional goods (or rent more

movies). The problem is quite similar to the one of web page ranking. As Search within results | Language Tools | Search Tips | Dissatisfied? Help us improve | Try Google Experimental

before, we want to obtain a sorted list (in this case of articles). ©2008 Google - Google Home - Advertising Programs - Business Solutions - About Google The key dif-

ference is that an explicit query is missing and instead we can only use past

purchase and viewing decisions of the user to predict future viewing and

purchase habits. The key side information here are the decisions made by

similar users, hence the collaborative nature of the process. See Figure 1.2

for an example. It is clearly desirable to have an automatic system to solve

this problem, thereby avoiding guesswork and time [BK07].

An equally ill-deﬁned problem is that of automatic translation of doc-

uments. At one extreme, we could aim at fully understanding a text before

translating it using a curated set of rules crafted by a computational linguist

well versed in the two languages we would like to translate. This is a rather

arduous task, in particular given that text is not always grammatically cor-

rect, nor is the document understanding part itself a trivial one. Instead, we

could simply use examples of translated documents, such as the proceedings

of the Canadian parliament or other multilingual entities (United Nations,

European Union, Switzerland) to learn how to translate between the two

1.1 A Taste of Machine Learning

5

languages. In other words, we could use examples of translations to learn

how to translate. This machine learning approach proved quite successful

[?].

Many security applications, e.g. for access control, use face recognition as

one of its components. That is, given the photo (or video recording) of a

person, recognize who this person is. In other words, the system needs to

classify the faces into one of many categories (Alice, Bob, Charlie, . . . ) or

decide that it is an unknown face. A similar, yet conceptually quite diﬀerent

problem is that of veriﬁcation. Here the goal is to verify whether the person

in question is who he claims to be. Note that diﬀerently to before, this

is now a yes/no question. To deal with diﬀerent lighting conditions, facial

expressions, whether a person is wearing glasses, hairstyle, etc., it is desirable

to have a system which learns which features are relevant for identifying a

person.

Hello. Sign in to get personalized recommendations. New customer? Start here.

Another application where learning helps is the problem of named entity YourAmazon.com

Today's Deals Gifts & Wish Lists Gift Cards

Your Account | Help

Books

recognition (see Figure 1.4). That is, the problem of identifying entities, Books

Advanced Search Browse Subjects Hot New Releases Bestsellers The New York Times® Best Sellers Libros En Español Bargain Books Textbooks

Join Amazon Prime and ship Two-Day for free and Overnight for $3.99. Already a member? Sign in.

such

as

places,

titles,

names, actions, etc. from documents. Such Machine Learning (Mcgraw-Hill International Edit) (Paperback)

Quantity: 1

steps

are

by Thomas Mitchell (Author) "Ever since computers were invented, we have wondered whether

crucial in the automatic digestion and understanding of documents. Some they might be made to learn..." (more)

(30 customer reviews)

or

Sign in to turn on 1-Click ordering.

List Price: $87.47

modern e-mail clients, such as Apple’s Mail.app nowadays ship with the Price: $87.47 & this item ships for FREE with Super Saver Shipping. Details

More Buying Choices

Availability: Usually ships within 4 to 7 weeks. Ships from and sold by Amazon.com. Gift-

16 used & new from

ability

to

identify

addresses in wrap available.

mails

and

ﬁling

them automatically $52.00 Have one to sell?

in

an

16 used & new available from $52.00
address book. While systems Share your own customer images using hand-crafted rules can lead to satisfacSearch inside another edition of this book Also Available in: List Price: Our Price: Other Offers:

tory results, it is far more eﬃcient to use examples of marked-up documents Are You an Author or Publisher?

Hardcover (1)

$153.44 $153.44 34 used & new from $67.00

Find out how to publish

your own Kindle Books
to learn such dependencies automatically, in particular if we want to de-

Better Together
ploy our system in many languages. For instance, while ’bush’ and ’rice’ Buy this book with Introduction to Machine Learning (Adaptive Computation and Machine Learning) by Ethem Alpaydin today! Buy Together Today: $130.87

Customers Who Bought This Item Also Bought

Pattern Recognition and Machine Learning (Information Science and Statistics) by Christopher M. Bishop
(30) $60.50

Artificial Intelligence: A Modern Approach (2nd Edition) (Prentice Hall Series in Artificial Intelligence) by Stuart Russell
(76) $115.00

› Explore similar items : Books (50)

The Elements of Statistical Learning by T. Hastie
(25) $72.20

Pattern Classification (2nd Edition) by Richard O. Duda
(25) $115.00

Data Mining: Practical Machine Learning Tools and Techniques, Second Edition (Morgan Kaufmann Series in Data Management Systems) by Ian H. Witten
(21) $39.66

Editorial Reviews
Fig. 1.2. Books recommended by Amazon.com when viewing Tom Mitchell’s Ma- Book Description This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly
chine Learning Book [Mit97]. It is desirable for the vendor to recommend relevant changing field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logical and coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding text for college students. --This text refers to the Hardcover edition.
books which a user Book Info might purchase. Presents the key algorithms and theory that form the core of machine learning. Discusses such theoretical issues as How does learning performance vary with the number of training examples presented? and Which learning algorithms are most appropriate for various types of learning tasks? DLC: Computer algorithms. --This text refers to the Hardcover edition.

Product Details Paperback: 352 pages
Publisher: McGraw-Hill Education (ISE Editions); 1st edition (October 1, 1997)
Language: English
ISBN-10: 0071154671
ISBN-13: 978-0071154673
Product Dimensions: 9 x 5.9 x 1.1 inches
Shipping Weight: 1.2 pounds (View shipping rates and policies)
Fig. 1.3. 11 Pictures of the Average Customer Review: same person (30 customer reviews) taken from the Yale face recognition Amazon.com Sales Rank: #104,460 in Books (See Bestsellers in Books)
database. The challenge is to Popular in this category: (What's this?) recognize that we are dealing with the same per#11 in Books > Computers & Internet > Computer Science > Artificial Intelligence > Machine Learning
son in all 11 cases. (Publishers and authors: Improve Your Sales) In-Print Editions: Hardcover (1) | All Editions

Would you like to update product info or give feedback on images? (We'll ask you to sign in so we can get back to you)
Inside This Book (learn more)
Browse and search another edition of this book. First Sentence: Ever since computers were invented, we have wondered whether they might be made to learn. Read the first page Browse Sample Pages: Front Cover | Copyright | Table of Contents | Excerpt | Index | Back Cover | Surprise Me! Search Inside This Book:

Customers viewing this page may be interested in these Sponsored Links (What's this?)
Online Law Degree http://www.edu-onlinedegree.org Juris Doctor JD & LLM Masters Low tuition, Free Textbooks
Learning CDs www.mindperk.com Save on powerful mind-boosting CDs & DVDs. Huge Selection
Video Edit Magic www.deskshare.com/download Video Editing Software trim, modify color, and merge video

Tags Customers Associate with This Product (What's this?) Click on a tag to find related items, discussions, and people.

machine learning (6)

computer science (1)

artificial intelligence (2)

pattern recognition (1)

Advertise on Amazon Search Products Tagged with

Your tags: Add your first tag

6

1 Introduction

HAVANA (Reuters) - The European Union’s top development aid official left Cuba on Sunday convinced that EU diplomatic sanctions against the communist island should be dropped after Fidel Castro’s retirement, his main aide said.

<TYPE="ORGANIZATION">HAVANA</> (<TYPE="ORGANIZATION">Reuters</>) - The <TYPE="ORGANIZATION">European Union</>’s top development aid official left <TYPE="ORGANIZATION">Cuba</> on Sunday convinced that EU diplomatic sanctions against the communist <TYPE="LOCATION">island</> should be dropped after <TYPE="PERSON">Fidel Castro</>’s retirement, his main aide said.

Fig. 1.4. Named entity tagging of a news article (using LingPipe). The relevant locations, organizations and persons are tagged for further information extraction.

are clearly terms from agriculture, it is equally clear that in the context of contemporary politics they refer to members of the Republican Party.
Other applications which take advantage of learning are speech recognition (annotate an audio sequence with text, such as the system shipping with Microsoft Vista), the recognition of handwriting (annotate a sequence of strokes with text, a feature common to many PDAs), trackpads of computers (e.g. Synaptics, a major manufacturer of such pads derives its name from the synapses of a neural network), the detection of failure in jet engines, avatar behavior in computer games (e.g. Black and White), direct marketing (companies use past purchase behavior to guesstimate whether you might be willing to purchase even more) and ﬂoor cleaning robots (such as iRobot’s Roomba). The overarching theme of learning problems is that there exists a nontrivial dependence between some observations, which we will commonly refer to as x and a desired response, which we refer to as y, for which a simple set of deterministic rules is not known. By using learning we can infer such a dependency between x and y in a systematic fashion.
We conclude this section by discussing the problem of classiﬁcation, since it will serve as a prototypical problem for a signiﬁcant part of this book. It occurs frequently in practice: for instance, when performing spam ﬁltering, we are interested in a yes/no answer as to whether an e-mail contains relevant information or not. Note that this issue is quite user dependent: for a frequent traveller e-mails from an airline informing him about recent discounts might prove valuable information, whereas for many other recipients this might prove more of an nuisance (e.g. when the e-mail relates to products available only overseas). Moreover, the nature of annoying emails might change over time, e.g. through the availability of new products (Viagra, Cialis, Levitra, . . . ), diﬀerent opportunities for fraud (the Nigerian 419 scam which took a new twist after the Iraq war), or diﬀerent data types (e.g. spam which consists mainly of images). To combat these problems we

1.1 A Taste of Machine Learning

7

Fig. 1.5. Binary classiﬁcation; separate stars from diamonds. In this example we are able to do so by drawing a straight line which separates both sets. We will see later that this is an important example of what is called a linear classiﬁer.
want to build a system which is able to learn how to classify new e-mails. A seemingly unrelated problem, that of cancer diagnosis shares a common structure: given histological data (e.g. from a microarray analysis of a patient’s tissue) infer whether a patient is healthy or not. Again, we are asked to generate a yes/no answer given a set of observations. See Figure 1.5 for an example.
1.1.2 Data
It is useful to characterize learning problems according to the type of data they use. This is a great help when encountering new challenges, since quite often problems on similar data types can be solved with very similar techniques. For instance natural language processing and bioinformatics use very similar tools for strings of natural language text and for DNA sequences. Vectors constitute the most basic entity we might encounter in our work. For instance, a life insurance company might be interesting in obtaining the vector of variables (blood pressure, heart rate, height, weight, cholesterol level, smoker, gender) to infer the life expectancy of a potential customer. A farmer might be interested in determining the ripeness of fruit based on (size, weight, spectral data). An engineer might want to ﬁnd dependencies in (voltage, current) pairs. Likewise one might want to represent documents by a vector of counts which describe the occurrence of words. The latter is commonly referred to as bag of words features.
One of the challenges in dealing with vectors is that the scales and units of diﬀerent coordinates may vary widely. For instance, we could measure the height in kilograms, pounds, grams, tons, stones, all of which would amount to multiplicative changes. Likewise, when representing temperatures, we have a full class of aﬃne transformations, depending on whether we represent them in terms of Celsius, Kelvin or Farenheit. One way of dealing

8

1 Introduction

with those issues in an automatic fashion is to normalize the data. We will discuss means of doing so in an automatic fashion.
Lists: In some cases the vectors we obtain may contain a variable number of features. For instance, a physician might not necessarily decide to perform a full battery of diagnostic tests if the patient appears to be healthy.
Sets may appear in learning problems whenever there is a large number of potential causes of an eﬀect, which are not well determined. For instance, it is relatively easy to obtain data concerning the toxicity of mushrooms. It would be desirable to use such data to infer the toxicity of a new mushroom given information about its chemical compounds. However, mushrooms contain a cocktail of compounds out of which one or more may be toxic. Consequently we need to infer the properties of an object given a set of features, whose composition and number may vary considerably.
Matrices are a convenient means of representing pairwise relationships. For instance, in collaborative ﬁltering applications the rows of the matrix may represent users whereas the columns correspond to products. Only in some cases we will have knowledge about a given (user, product) combination, such as the rating of the product by a user.
A related situation occurs whenever we only have similarity information between observations, as implemented by a semi-empirical distance measure. Some homology searches in bioinformatics, e.g. variants of BLAST [AGML90], only return a similarity score which does not necessarily satisfy the requirements of a metric.
Images could be thought of as two dimensional arrays of numbers, that is, matrices. This representation is very crude, though, since they exhibit spatial coherence (lines, shapes) and (natural images exhibit) a multiresolution structure. That is, downsampling an image leads to an object which has very similar statistics to the original image. Computer vision and psychooptics have created a raft of tools for describing these phenomena.
Video adds a temporal dimension to images. Again, we could represent them as a three dimensional array. Good algorithms, however, take the temporal coherence of the image sequence into account.
Trees and Graphs are often used to describe relations between collections of objects. For instance the ontology of webpages of the DMOZ project (www.dmoz.org) has the form of a tree with topics becoming increasingly reﬁned as we traverse from the root to one of the leaves (Arts → Animation → Anime → General Fan Pages → Oﬃcial Sites). In the case of gene ontology the relationships form a directed acyclic graph, also referred to as the GO-DAG [ABB+00].
Both examples above describe estimation problems where our observations

1.1 A Taste of Machine Learning

9

are vertices of a tree or graph. However, graphs themselves may be the observations. For instance, the DOM-tree of a webpage, the call-graph of a computer program, or the protein-protein interaction networks may form the basis upon which we may want to perform inference.
Strings occur frequently, mainly in the area of bioinformatics and natural language processing. They may be the input to our estimation problems, e.g. when classifying an e-mail as spam, when attempting to locate all names of persons and organizations in a text, or when modeling the topic structure of a document. Equally well they may constitute the output of a system. For instance, we may want to perform document summarization, automatic translation, or attempt to answer natural language queries.
Compound structures are the most commonly occurring object. That is, in most situations we will have a structured mix of diﬀerent data types. For instance, a webpage might contain images, text, tables, which in turn contain numbers, and lists, all of which might constitute nodes on a graph of webpages linked among each other. Good statistical modelling takes such dependencies and structures into account in order to tailor suﬃciently ﬂexible models.

1.1.3 Problems
The range of learning problems is clearly large, as we saw when discussing applications. That said, researchers have identiﬁed an ever growing number of templates which can be used to address a large set of situations. It is those templates which make deployment of machine learning in practice easy and our discussion will largely focus on a choice set of such problems. We now give a by no means complete list of templates.
Binary Classiﬁcation is probably the most frequently studied problem in machine learning and it has led to a large number of important algorithmic and theoretic developments over the past century. In its simplest form it reduces to the question: given a pattern x drawn from a domain X, estimate which value an associated binary random variable y ∈ {±1} will assume. For instance, given pictures of apples and oranges, we might want to state whether the object in question is an apple or an orange. Equally well, we might want to predict whether a home owner might default on his loan, given income data, his credit history, or whether a given e-mail is spam or ham. The ability to solve this basic problem already allows us to address a large variety of practical settings.
There are many variants exist with regard to the protocol in which we are required to make our estimation:

10

1 Introduction

Fig. 1.6. Left: binary classiﬁcation. Right: 3-class classiﬁcation. Note that in the latter case we have much more degree for ambiguity. For instance, being able to distinguish stars from diamonds may not suﬃce to identify either of them correctly, since we also need to distinguish both of them from triangles.
• We might see a sequence of (xi, yi) pairs for which yi needs to be estimated in an instantaneous online fashion. This is commonly referred to as online learning.
• We might observe a collection X := {x1, . . . xm} and Y := {y1, . . . ym} of pairs (xi, yi) which are then used to estimate y for a (set of) so-far unseen X = x1, . . . , xm . This is commonly referred to as batch learning.
• We might be allowed to know X already at the time of constructing the model. This is commonly referred to as transduction.
• We might be allowed to choose X for the purpose of model building. This is known as active learning.
• We might not have full information about X, e.g. some of the coordinates of the xi might be missing, leading to the problem of estimation with missing variables.
• The sets X and X might come from diﬀerent data sources, leading to the problem of covariate shift correction.
• We might be given observations stemming from two problems at the same time with the side information that both problems are somehow related. This is known as co-training.
• Mistakes of estimation might be penalized diﬀerently depending on the type of error, e.g. when trying to distinguish diamonds from rocks a very asymmetric loss applies.
Multiclass Classiﬁcation is the logical extension of binary classiﬁcation. The main diﬀerence is that now y ∈ {1, . . . , n} may assume a range of diﬀerent values. For instance, we might want to classify a document according to the language it was written in (English, French, German, Spanish, Hindi, Japanese, Chinese, . . . ). See Figure 1.6 for an example. The main difference to before is that the cost of error may heavily depend on the type of

1.1 A Taste of Machine Learning

11

Fig. 1.7. Regression estimation. We are given a number of instances (indicated by black dots) and would like to ﬁnd some function f mapping the observations X to R such that f (x) is close to the observed values.
error we make. For instance, in the problem of assessing the risk of cancer, it makes a signiﬁcant diﬀerence whether we mis-classify an early stage of cancer as healthy (in which case the patient is likely to die) or as an advanced stage of cancer (in which case the patient is likely to be inconvenienced from overly aggressive treatment).
Structured Estimation goes beyond simple multiclass estimation by assuming that the labels y have some additional structure which can be used in the estimation process. For instance, y might be a path in an ontology, when attempting to classify webpages, y might be a permutation, when attempting to match objects, to perform collaborative ﬁltering, or to rank documents in a retrieval setting. Equally well, y might be an annotation of a text, when performing named entity recognition. Each of those problems has its own properties in terms of the set of y which we might consider admissible, or how to search this space. We will discuss a number of those problems in Chapter ??.
Regression is another prototypical application. Here the goal is to estimate a real-valued variable y ∈ R given a pattern x (see e.g. Figure 1.7). For instance, we might want to estimate the value of a stock the next day, the yield of a semiconductor fab given the current process, the iron content of ore given mass spectroscopy measurements, or the heart rate of an athlete, given accelerometer data. One of the key issues in which regression problems diﬀer from each other is the choice of a loss. For instance, when estimating stock values our loss for a put option will be decidedly one-sided. On the other hand, a hobby athlete might only care that our estimate of the heart rate matches the actual on average.
Novelty Detection is a rather ill-deﬁned problem. It describes the issue of determining “unusual” observations given a set of past measurements. Clearly, the choice of what is to be considered unusual is very subjective. A commonly accepted notion is that unusual events occur rarely. Hence a possible goal is to design a system which assigns to each observation a rating

12

1 Introduction

Fig. 1.8. Left: typical digits contained in the database of the US Postal Service. Right: unusual digits found by a novelty detection algorithm [SPST+01] (for a description of the algorithm see Section 7.4). The score below the digits indicates the degree of novelty. The numbers on the lower right indicate the class associated with the digit.
as to how novel it is. Readers familiar with density estimation might contend that the latter would be a reasonable solution. However, we neither need a score which sums up to 1 on the entire domain, nor do we care particularly much about novelty scores for typical observations. We will later see how this somewhat easier goal can be achieved directly. Figure 1.8 has an example of novelty detection when applied to an optical character recognition database.

1.2 Probability Theory
In order to deal with the instances of where machine learning can be used, we need to develop an adequate language which is able to describe the problems concisely. Below we begin with a fairly informal overview over probability theory. For more details and a very gentle and detailed discussion see the excellent book of [BT03].

1.2.1 Random Variables

Assume that we cast a dice and we would like to know our chances whether

we would see 1 rather than another digit. If the dice is fair all six outcomes

X = {1, . . . , 6} are equally likely to occur, hence we would see a 1 in roughly

1 out of 6 cases. Probability theory allows us to model uncertainty in the out-

come of such experiments. Formally we state that 1 occurs with probability

1 6

.

In many experiments, such as the roll of a dice, the outcomes are of a

numerical nature and we can handle them easily. In other cases, the outcomes

may not be numerical, e.g., if we toss a coin and observe heads or tails. In

these cases, it is useful to associate numerical values to the outcomes. This

is done via a random variable. For instance, we can let a random variable

1.2 Probability Theory

13

X take on a value +1 whenever the coin lands heads and a value of −1 otherwise. Our notational convention will be to use uppercase letters, e.g., X, Y etc to denote random variables and lower case letters, e.g., x, y etc to denote the values they take.

height

X
ξ(x) x
weight
Fig. 1.9. The random variable ξ maps from the set of outcomes of an experiment (denoted here by X) to real numbers. As an illustration here X consists of the patients a physician might encounter, and they are mapped via ξ to their weight and height.

1.2.2 Distributions
Perhaps the most important way to characterize a random variable is to associate probabilities with the values it can take. If the random variable is discrete, i.e., it takes on a ﬁnite number of values, then this assignment of probabilities is called a probability mass function or PMF for short. A PMF must be, by deﬁnition, non-negative and must sum to one. For instance, if the coin is fair, i.e., heads and tails are equally likely, then the random variable X described above takes on values of +1 and −1 with probability 0.5. This can be written as

P r(X = +1) = 0.5 and P r(X = −1) = 0.5.

(1.1)

When there is no danger of confusion we will use the slightly informal notation p(x) := P r(X = x).
In case of a continuous random variable the assignment of probabilities results in a probability density function or PDF for short. With some abuse of terminology, but keeping in line with convention, we will often use density or distribution instead of probability density function. As in the case of the PMF, a PDF must also be non-negative and integrate to one. Figure 1.10 shows two distributions: the uniform distribution

1
p(x) = b−a

if x ∈ [a, b]

0 otherwise,

(1.2)

14

1 Introduction

0.5

0.4

0.3

0.2

0.1

0.0

-4

-2

0

2

4

0.5

0.4

0.3

0.2

0.1

0.0

-4

-2

0

2

4

Fig. 1.10. Two common densities. Left: uniform distribution over the interval [−1, 1]. Right: Normal distribution with zero mean and unit variance.

and the Gaussian distribution (also called normal distribution)

1

(x − µ)2

p(x) = √ exp − 2πσ2

2σ2

.

(1.3)

Closely associated with a PDF is the indeﬁnite integral over p. It is commonly referred to as the cumulative distribution function (CDF).

Deﬁnition 1.1 (Cumulative Distribution Function) For a real valued random variable X with PDF p the associated Cumulative Distribution Function F is given by

x
F (x ) := Pr X ≤ x = dp(x).
−∞

(1.4)

The CDF F (x ) allows us to perform range queries on p eﬃciently. For instance, by integral calculus we obtain

b
Pr(a ≤ X ≤ b) = dp(x) = F (b) − F (a).
a

(1.5)

The values of x for which F (x ) assumes a speciﬁc value, such as 0.1 or 0.5 have a special name. They are called the quantiles of the distribution p.

Deﬁnition 1.2 (Quantiles) Let q ∈ (0, 1). Then the value of x for which Pr(X < x ) ≤ q and Pr(X > x ) ≤ 1 − q is the q-quantile of the distribution p. Moreover, the value x associated with q = 0.5 is called the median.

1.2 Probability Theory

15

p(x)

Fig. 1.11. Quantiles of a distribution correspond to the area under the integral of the density p(x) for which the integral takes on a pre-speciﬁed value. Illustrated are the 0.1, 0.5 and 0.9 quantiles respectively.

1.2.3 Mean and Variance
A common question to ask about a random variable is what its expected value might be. For instance, when measuring the voltage of a device, we might ask what its typical values might be. When deciding whether to administer a growth hormone to a child a doctor might ask what a sensible range of height should be. For those purposes we need to deﬁne expectations and related quantities of distributions.

Deﬁnition 1.3 (Mean) We deﬁne the mean of a random variable X as

E[X] := xdp(x)

(1.6)

More generally, if f : R → R is a function, then f (X) is also a random variable. Its mean is mean given by

E[f (X)] := f (x)dp(x).

(1.7)

Whenever X is a discrete random variable the integral in (1.6) can be replaced by a summation:

E[X] = xp(x).
x

(1.8)

For instance, in the case of a dice we have equal probabilities of 1/6 for all 6 possible outcomes. It is easy to see that this translates into a mean of (1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5.
The mean of a random variable is useful in assessing expected losses and beneﬁts. For instance, as a stock broker we might be interested in the expected value of our investment in a year’s time. In addition to that, however, we also might want to investigate the risk of our investment. That is, how likely it is that the value of the investment might deviate from its expectation since this might be more relevant for our decisions. This means that we

16

1 Introduction

need a variable to quantify the risk inherent in a random variable. One such measure is the variance of a random variable.

Deﬁnition 1.4 (Variance) We deﬁne the variance of a random variable X as

Var[X] := E (X − E[X])2 .

(1.9)

As before, if f : R → R is a function, then the variance of f (X) is given by

Var[f (X)] := E (f (X) − E[f (X)])2 .

(1.10)

The variance measures by how much on average f (X) deviates from its expected value. As we shall see in Section 2.1, an upper bound on the variance can be used to give guarantees on the probability that f (X) will be within
of its expected value. This is one of the reasons why the variance is often associated with the risk of a random variable. Note that often one discusses properties of a random variable in terms of its standard deviation, which is deﬁned as the square root of the variance.

1.2.4 Marginalization, Independence, Conditioning, and Bayes Rule
Given two random variables X and Y , one can write their joint density p(x, y). Given the joint density, one can recover p(x) by integrating out y. This operation is called marginalization:

p(x) = dp(x, y).
y

(1.11)

If Y is a discrete random variable, then we can replace the integration with a summation:

p(x) = p(x, y).
y

(1.12)

We say that X and Y are independent, i.e., the values that X takes does not depend on the values that Y takes whenever

p(x, y) = p(x)p(y).

(1.13)

Independence is useful when it comes to dealing with large numbers of random variables whose behavior we want to estimate jointly. For instance, whenever we perform repeated measurements of a quantity, such as when

1.2 Probability Theory

17

2.0

1.5

1.0

0.5

0.0

-0.-50.5

0.0

0.5

1.0

1.5

2.0

2.0

1.5

1.0

0.5

0.0

-0.-50.5

0.0

0.5

1.0

1.5

2.0

Fig. 1.12. Left: a sample from two dependent random variables. Knowing about ﬁrst coordinate allows us to improve our guess about the second coordinate. Right: a sample drawn from two independent random variables, obtained by randomly permuting the dependent sample.

measuring the voltage of a device, we will typically assume that the individual measurements are drawn from the same distribution and that they are independent of each other. That is, having measured the voltage a number of times will not aﬀect the value of the next measurement. We will call such random variables to be independently and identically distributed, or in short, iid random variables. See Figure 1.12 for an example of a pair of random variables drawn from dependent and independent distributions respectively.
Conversely, dependence can be vital in classiﬁcation and regression problems. For instance, the traﬃc lights at an intersection are dependent of each other. This allows a driver to perform the inference that when the lights are green in his direction there will be no traﬃc crossing his path, i.e. the other lights will indeed be red. Likewise, whenever we are given a picture x of a digit, we hope that there will be dependence between x and its label y.
Especially in the case of dependent random variables, we are interested in conditional probabilities, i.e., probability that X takes on a particular value given the value of Y . Clearly P r(X = rain|Y = cloudy) is higher than P r(X = rain|Y = sunny). In other words, knowledge about the value of Y signiﬁcantly inﬂuences the distribution of X. This is captured via conditional probabilities:

p(x, y)

p(x|y) :=

.

p(y)

(1.14)

Equation 1.14 leads to one of the key tools in statistical inference.

Theorem 1.5 (Bayes Rule) Denote by X and Y random variables then

18

1 Introduction

the following holds

p(x|y)p(y)

p(y|x) =

.

p(x)

(1.15)

This follows from the fact that p(x, y) = p(x|y)p(y) = p(y|x)p(x). The key consequence of (1.15) is that we may reverse the conditioning between a pair of random variables.

1.2.4.1 An Example
We illustrate our reasoning by means of a simple example — inference using an AIDS test. Assume that a patient would like to have such a test carried out on him. The physician recommends a test which is guaranteed to detect HIV-positive whenever a patient is infected. On the other hand, for healthy patients it has a 1% error rate. That is, with probability 0.01 it diagnoses a patient as HIV-positive even when he is, in fact, HIV-negative. Moreover, assume that 0.15% of the population is infected.
Now assume that the patient has the test carried out and the test returns ’HIV-negative’. In this case, logic implies that he is healthy, since the test has 100% detection rate. In the converse case things are not quite as straightforward. Denote by X and T the random variables associated with the health status of the patient and the outcome of the test respectively. We are interested in p(X = HIV+|T = HIV+). By Bayes rule we may write
p(T = HIV+|X = HIV+)p(X = HIV+) p(X = HIV+|T = HIV+) =
p(T = HIV+)

While we know all terms in the numerator, p(T = HIV+) itself is unknown. That said, it can be computed via

p(T = HIV+) =

p(T = HIV+, x)

x∈{HIV+,HIV-}

=

p(T = HIV+|x)p(x)

x∈{HIV+,HIV-}

= 1.0 · 0.0015 + 0.01 · 0.9985.

Substituting back into the conditional expression yields

1.0 · 0.0015

p(X = HIV+|T = HIV+) =

= 0.1306.

1.0 · 0.0015 + 0.01 · 0.9985

In other words, even though our test is quite reliable, there is such a low prior probability of having been infected with AIDS that there is not much evidence to accept the hypothesis even after this test.

1.2 Probability Theory

19

test 1

age

x

test 2

Fig. 1.13. A graphical description of our HIV testing scenario. Knowing the age of the patient inﬂuences our prior on whether the patient is HIV positive (the random variable X). The outcomes of the tests 1 and 2 are independent of each other given the status X. We observe the shaded random variables (age, test 1, test 2) and would like to infer the un-shaded random variable X. This is a special case of a graphical model which we will discuss in Chapter ??.

Let us now think how we could improve the diagnosis. One way is to obtain further information about the patient and to use this in the diagnosis. For instance, information about his age is quite useful. Suppose the patient is 35 years old. In this case we would want to compute p(X = HIV+|T = HIV+, A = 35) where the random variable A denotes the age. The corresponding expression yields:
p(T = HIV+|X = HIV+, A)p(X = HIV+|A) p(T = HIV+|A)
Here we simply conditioned all random variables on A in order to take additional information into account. We may assume that the test is independent of the age of the patient, i.e.

p(t|x, a) = p(t|x).

What remains therefore is p(X = HIV+|A). Recent US census data pegs this

number at approximately 0.9%. Plugging all data back into the conditional

expression

yields

1·0.009 1·0.009+0.01·0.991

=

0.48.

What

has

happened

here

is

that

by including additional observed random variables our estimate has become

more reliable. Combination of evidence is a powerful tool. In our case it

helped us make the classiﬁcation problem of whether the patient is HIV-

positive or not more reliable.

A second tool in our arsenal is the use of multiple measurements. After

the ﬁrst test the physician is likely to carry out a second test to conﬁrm the

diagnosis. We denote by T1 and T2 (and t1, t2 respectively) the two tests.

Obviously, what we want is that T2 will give us an “independent” second

opinion of the situation. In other words, we want to ensure that T2 does

not make the same mistakes as T1. For instance, it is probably a bad idea

to repeat T1 without changes, since it might perform the same diagnostic

20

1 Introduction

mistake as before. What we want is that the diagnosis of T2 is independent of that of T2 given the health status X of the patient. This is expressed as

p(t1, t2|x) = p(t1|x)p(t2|x).

(1.16)

See Figure 1.13 for a graphical illustration of the setting. Random variables satisfying the condition (1.16) are commonly referred to as conditionally independent. In shorthand we write T1, T2 ⊥⊥ X. For the sake of the argument we assume that the statistics for T2 are given by

p(t2|x)

x = HIV- x = HIV+

t2 = HIV- 0.95

0.01

t2 = HIV+ 0.05

0.99

Clearly this test is less reliable than the ﬁrst one. However, we may now

combine both estimates to obtain a very reliable estimate based on the

combination of both events. For instance, for t1 = t2 = HIV+ we have

1.0 · 0.99 · 0.009 p(X = HIV+|T1 = HIV+, T2 = HIV+) = 1.0 · 0.99 · 0.009 + 0.01 · 0.05 · 0.991 = 0.95.

In other words, by combining two tests we can now conﬁrm with very high conﬁdence that the patient is indeed diseased. What we have carried out is a combination of evidence. Strong experimental evidence of two positive tests eﬀectively overcame an initially very strong prior which suggested that the patient might be healthy.
Tests such as in the example we just discussed are fairly common. For instance, we might need to decide which manufacturing procedure is preferable, which choice of parameters will give better results in a regression estimator, or whether to administer a certain drug. Note that often our tests may not be conditionally independent and we would need to take this into account.

1.3 Basic Algorithms
We conclude our introduction to machine learning by discussing four simple algorithms, namely Naive Bayes, Nearest Neighbors, the Mean Classiﬁer, and the Perceptron, which can be used to solve a binary classiﬁcation problem such as that described in Figure 1.5. We will also introduce the K-means algorithm which can be employed when labeled data is not available. All these algorithms are readily usable and easily implemented from scratch in their most basic form.
For the sake of concreteness assume that we are interested in spam ﬁltering. That is, we are given a set of m e-mails xi, denoted by X := {x1, . . . , xm}

1.3 Basic Algorithms

21

From: "LucindaParkison497072" <LucindaParkison497072@hotmail.com> To: <kargr@earthlink.net> Subject: we think ACGU is our next winner Date: Mon, 25 Feb 2008 00:01:01 -0500 MIME-Version: 1.0 X-OriginalArrivalTime: 25 Feb 2008 05:01:01.0329 (UTC) FILETIME=[6A931810:01C8776B] Return-Path: lucindaparkison497072@hotmail.com
(ACGU) .045 UP 104.5%
I do think that (ACGU) at it’s current levels looks extremely attractive.
Asset Capital Group, Inc., (ACGU) announced that it is expanding the marketing of bio-remediation fluids and cleaning equipment. After its recent acquisition of interest in American Bio-Clean Corporation and an 80
News is expected to be released next week on this growing company and could drive the price even higher. Buy (ACGU) Monday at open. I believe those involved at this stage could enjoy a nice ride up.

Fig. 1.14. Example of a spam e-mail

x1: The quick brown fox jumped over the lazy dog. x2: The dog hunts a fox.

the quick brown fox jumped over lazy dog hunts a

x1 2 1

1

11

x2 1 0

0

10

1

1

10

0

0

0

11

1

Fig. 1.15. Vector space representation of strings.

and associated labels yi, denoted by Y := {y1, . . . , ym}. Here the labels satisfy yi ∈ {spam, ham}. The key assumption we make here is that the pairs (xi, yi) are drawn jointly from some distribution p(x, y) which represents the e-mail generating process for a user. Moreover, we assume that there is suﬃciently strong dependence between x and y that we will be able to estimate y given x and a set of labeled instances X, Y.
Before we do so we need to address the fact that e-mails such as Figure 1.14 are text, whereas the three algorithms we present will require data to be represented in a vectorial fashion. One way of converting text into a vector is by using the so-called bag of words representation [Mar61, Lew98]. In its simplest version it works as follows: Assume we have a list of all possible words occurring in X, that is a dictionary, then we are able to assign a unique number with each of those words (e.g. the position in the dictionary). Now we may simply count for each document xi the number of times a given word j is occurring. This is then used as the value of the j-th coordinate of xi. Figure 1.15 gives an example of such a representation. Once we have the latter it is easy to compute distances, similarities, and other statistics directly from the vectorial representation.

22

1 Introduction

1.3.1 Naive Bayes
In the example of the AIDS test we used the outcomes of the test to infer whether the patient is diseased. In the context of spam ﬁltering the actual text of the e-mail x corresponds to the test and the label y is equivalent to the diagnosis. Recall Bayes Rule (1.15). We could use the latter to infer

p(x|y)p(y)

p(y|x) =

.

p(x)

We may have a good estimate of p(y), that is, the probability of receiving a spam or ham mail. Denote by mham and mspam the number of ham and spam e-mails in X. In this case we can estimate

p(ham) ≈ mham and p(spam) ≈ mspam .

m

m

The key problem, however, is that we do not know p(x|y) or p(x). We may dispose of the requirement of knowing p(x) by settling for a likelihood ratio

p(spam|x) p(x|spam)p(spam)

L(x) :=

=

.

p(ham|x) p(x|ham)p(ham)

(1.17)

Whenever L(x) exceeds a given threshold c we decide that x is spam and consequently reject the e-mail. If c is large then our algorithm is conservative and classiﬁes an email as spam only if p(spam|x) p(ham|x). On the other hand, if c is small then the algorithm aggressively classiﬁes emails as spam.
The key obstacle is that we have no access to p(x|y). This is where we make our key approximation. Recall Figure 1.13. In order to model the distribution of the test outcomes T1 and T2 we made the assumption that they are conditionally independent of each other given the diagnosis. Analogously, we may now treat the occurrence of each word in a document as a separate test and combine the outcomes in a naive fashion by assuming that

# of words in x

p(x|y) =

p(wj |y),

j=1

(1.18)

where wj denotes the j-th word in document x. This amounts to the assumption that the probability of occurrence of a word in a document is independent of all other words given the category of the document. Even though this assumption does not hold in general – for instance, the word “York” is much more likely to after the word “New” – it suﬃces for our purposes (see Figure 1.16).
This assumption reduces the diﬃculty of knowing p(x|y) to that of estimating the probabilities of occurrence of individual words w. Estimates for

1.3 Basic Algorithms

23

y

word 1

word 2

word 3 ... word n

Fig. 1.16. Naive Bayes model. The occurrence of individual words is independent of each other, given the category of the text. For instance, the word Viagra is fairly frequent if y = spam but it is considerably less frequent if y = ham, except when considering the mailbox of a Pﬁzer sales representative.

p(w|y) can be obtained, for instance, by simply counting the frequency occurrence of the word within documents of a given class. That is, we estimate

m i=1

# of words in xi j=1

yi = spam and wij = w

p(w|spam) ≈

m i=1

# of j=1

words

in

xi

{yi

=

spam}

Here yi = spam and wij = w equals 1 if and only if xi is labeled as spam and w occurs as the j-th word in xi. The denominator is simply the total number of words in spam documents. Similarly one can compute p(w|ham). In principle we could perform the above summation whenever we see a new document x. This would be terribly ineﬃcient, since each such computation requires a full pass through X and Y. Instead, we can perform a single pass through X and Y and store the resulting statistics as a good estimate of the conditional probabilities. Algorithm 1.1 has details of an implementation. Note that we performed a number of optimizations: Firstly, the normalization by ms−p1am and m−ha1m respectively is independent of x, hence we incorporate it as a ﬁxed oﬀset. Secondly, since we are computing a product over a large number of factors the numbers might lead to numerical overﬂow or underﬂow. This can be addressed by summing over the logarithm of terms rather than computing products. Thirdly, we need to address the issue of estimating p(w|y) for words w which we might not have seen before. One way of dealing with this is to increment all counts by 1. This method is commonly referred to as Laplace smoothing. We will encounter a theoretical justiﬁcation for this heuristic in Section 2.3.
This simple algorithm is known to perform surprisingly well, and variants of it can be found in most modern spam ﬁlters. It amounts to what is commonly known as “Bayesian spam ﬁltering”. Obviously, we may apply it to problems other than document categorization, too.

24

1 Introduction

Algorithm 1.1 Naive Bayes
Train(X, Y) {reads documents X and labels Y}
Compute dictionary D of X with n words.
Compute m, mham and mspam. Initialize b := log c + log mham − log mspam to oﬀset the rejection threshold Initialize p ∈ R2×n with pij = 1, wspam = n, wham = n. {Count occurrence of each word} {Here xji denotes the number of times word j occurs in document xi} for i = 1 to m do
if yi = spam then for j = 1 to n do p0,j ← p0,j + xij wspam ← wspam + xij end for
else
for j = 1 to n do p1,j ← p1,j + xji wham ← wham + xij
end for
end if
end for
{Normalize counts to yield word probabilities}
for j = 1 to n do
p0,j ← p0,j /wspam p1,j ← p1,j /wham end for
Classify(x) {classiﬁes document x}
Initialize score threshold t = −b
for j = 1 to n do t ← t + xj(log p0,j − log p1,j)
end for
if t > 0 return spam else return ham

1.3.2 Nearest Neighbor Estimators
An even simpler estimator than Naive Bayes is nearest neighbors. In its most basic form it assigns the label of its nearest neighbor to an observation x (see Figure 1.17). Hence, all we need to implement it is a distance measure d(x, x ) between pairs of observations. Note that this distance need not even be symmetric. This means that nearest neighbor classiﬁers can be extremely

1.3 Basic Algorithms

25

Fig. 1.17. 1 nearest neighbor classiﬁer. Depending on whether the query point x is closest to the star, diamond or triangles, it uses one of the three labels for it.

Fig. 1.18. k-Nearest neighbor classiﬁers using Euclidean distances. Left: decision boundaries obtained from a 1-nearest neighbor classiﬁer. Middle: color-coded sets of where the number of red / blue points ranges between 7 and 0. Right: decision boundary determining where the blue or red dots are in the majority.
ﬂexible. For instance, we could use string edit distances to compare two documents or information theory based measures.
However, the problem with nearest neighbor classiﬁcation is that the estimates can be very noisy whenever the data itself is very noisy. For instance, if a spam email is erroneously labeled as nonspam then all emails which are similar to this email will share the same fate. See Figure 1.18 for an example. In this case it is beneﬁcial to pool together a number of neighbors, say the k-nearest neighbors of x and use a majority vote to decide the class membership of x. Algorithm 1.2 has a description of the algorithm. Note that nearest neighbor algorithms can yield excellent performance when used with a good distance measure. For instance, the technology underlying the Netﬂix progress prize [BK07] was essentially nearest neighbours based.
Note that it is trivial to extend the algorithm to regression. All we need to change in Algorithm 1.2 is to return the average of the values yi instead of their majority vote. Figure 1.19 has an example.
Note that the distance computation d(xi, x) for all observations can be-

26

1 Introduction

Algorithm 1.2 k-Nearest Neighbor Classiﬁcation Classify(X, Y, x) {reads documents X, labels Y and query x}
for i = 1 to m do Compute distance d(xi, x)
end for Compute set I containing indices for the k smallest distances d(xi, x). return majority label of {yi where i ∈ I}.

Fig. 1.19. k-Nearest neighbor regression estimator using Euclidean distances. Left: some points (x, y) drawn from a joint distribution. Middle: 1-nearest neighbour classiﬁer. Right: 7-nearest neighbour classiﬁer. Note that the regression estimate is much more smooth.

come extremely costly, in particular whenever the number of observations is large or whenever the observations xi live in a very high dimensional space.

Random projections are a technique that can alleviate the high computational cost of Nearest Neighbor classiﬁers. A celebrated lemma by Johnson and Lindenstrauss [DG03] asserts that a set of m points in high dimensional Euclidean space can be projected into a O(log m/ 2) dimensional Euclidean space such that the distance between any two points changes only by a factor of (1 ± ). Since Euclidean distances are preserved, running the Nearest Neighbor classiﬁer on this mapped data yields the same results but at a lower computational cost [GIM99].

The surprising fact is that the projection relies on a simple randomized

algorithm: to obtain a d-dimensional representation of n-dimensional ran-

dom observations we pick a matrix R ∈ Rd×n where each element is drawn

independently

from

a

normal

distribution

with

n−

1 2

variance

and

zero

mean.

Multiplying x with this projection matrix can be shown to achieve this prop-

erty with high probability. For details see [DG03].

1.3 Basic Algorithms

27

μ-

w

μ+

x

Fig. 1.20. A trivial classiﬁer. Classiﬁcation is carried out in accordance to which of
the two means µ− or µ+ is closer to the test point x. Note that the sets of positive and negative labels respectively form a half space.

1.3.3 A Simple Classiﬁer

We can use geometry to design another simple classiﬁcation algorithm [SS02] for our problem. For simplicity we assume that the observations x ∈ Rd, such as the bag-of-words representation of e-mails. We deﬁne the means µ+ and µ− to correspond to the classes y ∈ {±1} via

1

1

µ−

:=

m−

xi
yi=−1

and

µ+

:=

m+

xi.
yi=1

Here we used m− and m+ to denote the number of observations with label yi = −1 and yi = +1 respectively. An even simpler approach than using the nearest neighbor classiﬁer would be to use the class label which corresponds to the mean closest to a new query x, as described in Figure 1.20.
For Euclidean distances we have

µ− − x 2 = µ− 2 + x 2 − 2 µ−, x and µ+ − x 2 = µ+ 2 + x 2 − 2 µ+, x .

(1.19) (1.20)

Here ·, · denotes the standard dot product between vectors. Taking diﬀerences between the two distances yields

f (x) := µ+ − x 2 − µ− − x 2 = 2 µ− − µ+, x + µ− 2 − µ+ 2 . (1.21)

This is a linear function in x and its sign corresponds to the labels we estimate for x. Our algorithm sports an important property: The classiﬁcation rule can be expressed via dot products. This follows from

µ+ 2 = µ+, µ+ = m−+2

xi, xj and µ+, x = m+−1

xi, x .

yi=yj =1

yi=1

28

1 Introduction

X
x

H
φ(x)

Fig. 1.21. The feature map φ maps observations x from X into a feature space H. The map φ is a convenient way of encoding pre-processing steps systematically.

Analogous expressions can be computed for µ−. Consequently we may express the classiﬁcation rule (1.21) as

m
f (x) = αi xi, x + b
i=1

(1.22)

where b = m−−2 yi=yj=−1 xi, xj − m+−2 yi=yj=1 xi, xj and αi = yi/myi . This oﬀers a number of interesting extensions. Recall that when dealing
with documents we needed to perform pre-processing to map e-mails into a vector space. In general, we may pick arbitrary maps φ : X → H mapping the space of observations into a feature space H, as long as the latter is endowed with a dot product (see Figure 1.21). This means that instead of dealing with x, x we will be dealing with φ(x), φ(x ) .
As we will see in Chapter 6, whenever H is a so-called Reproducing Kernel Hilbert Space, the inner product can be abbreviated in the form of a kernel function k(x, x ) which satisﬁes

k(x, x ) := φ(x), φ(x ) .

(1.23)

This small modiﬁcation leads to a number of very powerful algorithm and

it is at the foundation of an area of research called kernel methods. We

will encounter a number of such algorithms for regression, classiﬁcation,

segmentation, and density estimation over the course of the book. Examples

of suitable k are the polynomial kernel k(x, x ) = x, x d for d ∈ N and the

Gaussian RBF kernel k(x, x ) = e−γ x−x

2
for γ > 0.

The upshot of (1.23) is that our basic algorithm can be kernelized. That

is, we may rewrite (1.21) as

m
f (x) = αik(xi, x) + b
i=1

(1.24)

where as before αi = yi/myi and the oﬀset b is computed analogously. As

1.3 Basic Algorithms

29

Algorithm 1.3 The Perceptron
Perceptron(X, Y) {reads stream of observations (xi, yi)} Initialize w = 0 and b = 0 while There exists some (xi, yi) with yi( w, xi + b) ≤ 0 do w ← w + yixi and b ← b + yi end while

Algorithm 1.4 The Kernel Perceptron
KernelPerceptron(X, Y) {reads stream of observations (xi, yi)} Initialize f = 0 while There exists some (xi, yi) with yif (xi) ≤ 0 do f ← f + yik(xi, ·) + yi end while

a consequence we have now moved from a fairly simple and pedestrian linear classiﬁer to one which yields a nonlinear function f (x) with a rather nontrivial decision boundary.

1.3.4 Perceptron
In the previous sections we assumed that our classiﬁer had access to a training set of spam and non-spam emails. In real life, such a set might be diﬃcult to obtain all at once. Instead, a user might want to have instant results whenever a new e-mail arrives and he would like the system to learn immediately from any corrections to mistakes the system makes.
To overcome both these diﬃculties one could envisage working with the following protocol: As emails arrive our algorithm classiﬁes them as spam or non-spam, and the user provides feedback as to whether the classiﬁcation is correct or incorrect. This feedback is then used to improve the performance of the classiﬁer over a period of time.
This intuition can be formalized as follows: Our classiﬁer maintains a parameter vector. At the t-th time instance it receives a data point xt, to which it assigns a label yˆt using its current parameter vector. The true label yt is then revealed, and used to update the parameter vector of the classiﬁer. Such algorithms are said to be online. We will now describe perhaps the simplest classiﬁer of this kind namely the Perceptron [Heb49, Ros58].
Let us assume that the data points xt ∈ Rd, and labels yt ∈ {±1}. As before we represent an email as a bag-of-words vector and we assign +1 to spam emails and −1 to non-spam emails. The Perceptron maintains a weight

30

w*

xt

wt

wt+1 w*

1 Introduction
xt

Fig. 1.22. The Perceptron without bias. Left: at time t we have a weight vector wt denoted by the dashed arrow with corresponding separating plane (also dashed). For reference we include the linear separator w∗ and its separating plane (both
denoted by a solid line). As a new observation xt arrives which happens to be mis-classiﬁed by the current weight vector wt we perform an update. Also note the margin between the point xt and the separating hyperplane deﬁned by w∗. Right: This leads to the weight vector wt+1 which is more aligned with w∗.

vector w ∈ Rd and classiﬁes xt according to the rule

yˆt := sign{ w, xt + b},

(1.25)

where w, xt denotes the usual Euclidean dot product and b is an oﬀset. Note the similarity of (1.25) to (1.21) of the simple classiﬁer. Just as the latter, the Perceptron is a linear classiﬁer which separates its domain Rd into two halfspaces, namely {x| w, x + b > 0} and its complement. If yˆt = yt then no updates are made. On the other hand, if yˆt = yt the weight vector is updated as

w ← w + ytxt and b ← b + yt.

(1.26)

Figure 1.22 shows an update step of the Perceptron algorithm. For simplicity we illustrate the case without bias, that is, where b = 0 and where it remains unchanged. A detailed description of the algorithm is given in Algorithm 1.3.
An important property of the algorithm is that it performs updates on w by multiples of the observations xi on which it makes a mistake. Hence we may express w as w = i∈Error yixi. Just as before, we can replace xi and x by φ(xi) and φ(x) to obtain a kernelized version of the Perceptron algorithm [FS99] (Algorithm 1.4).
If the dataset (X, Y) is linearly separable, then the Perceptron algorithm

1.3 Basic Algorithms

31

eventually converges and correctly classiﬁes all the points in X. The rate of convergence however depends on the margin. Roughly speaking, the margin quantiﬁes how linearly separable a dataset is, and hence how easy it is to solve a given classiﬁcation problem.

Deﬁnition 1.6 (Margin) Let w ∈ Rd be a weight vector and let b ∈ R be an oﬀset. The margin of an observation x ∈ Rd with associated label y is

γ(x, y) := y ( w, x + b) .

(1.27)

Moreover, the margin of an entire set of observations X with labels Y is

γ(X, Y) := min γ(xi, yi).
i

(1.28)

Geometrically speaking (see Figure 1.22) the margin measures the distance of x from the hyperplane deﬁned by {x| w, x + b = 0}. Larger the margin, the more well separated the data and hence easier it is to ﬁnd a hyperplane with correctly classiﬁes the dataset. The following theorem asserts that if there exists a linear classiﬁer which can classify a dataset with a large margin, then the Perceptron will also correctly classify the same dataset after making a small number of mistakes.

Theorem 1.7 (Novikoﬀ ’s theorem) Let (X, Y) be a dataset with at least

one example labeled +1 and one example labeled −1. Let R := maxt xt , and

assume that there exists (w∗, b∗) such that w∗ = 1 and γt := yt( w∗, xt +

b∗)

≥

γ

for

all

t.

Then,

the

Perceptron

will

make

at

most

(1+R2)(1+(b∗)2) γ2

mistakes.

This result is remarkable since it does not depend on the dimensionality of the problem. Instead, it only depends on the geometry of the setting, as quantiﬁed via the margin γ and the radius R of a ball enclosing the observations. Interestingly, a similar bound can be shown for Support Vector Machines [Vap95] which we will be discussing in Chapter 7. Proof We can safely ignore the iterations where no mistakes were made and hence no updates were carried out. Therefore, without loss of generality assume that the t-th update was made after seeing the t-th observation and let wt denote the weight vector after the update. Furthermore, for simplicity assume that the algorithm started with w0 = 0 and b0 = 0. By the update equation (1.26) we have
wt, w∗ + btb∗ = wt−1, w∗ + bt−1b∗ + yt( xt, w∗ + b∗) ≥ wt−1, w∗ + bt−1b∗ + γ.

32

1 Introduction

By induction it follows that wt, w∗ +btb∗ ≥ tγ. On the other hand we made an update because yt( xt, wt−1 + bt−1) < 0. By using ytyt = 1,
wt 2 + b2t = wt−1 2 + b2t−1 + yt2 xt 2 + 1 + 2yt( wt−1, xt + bt−1) ≤ wt−1 2 + bt2−1 + xt 2 + 1

Since xt 2 = R2 we can again apply induction to conclude that wt 2+b2t ≤ t R2 + 1 . Combining the upper and the lower bounds, using the Cauchy-
Schwartz inequality, and w∗ = 1 yields

tγ ≤ wt, w∗ + btb∗ =

≤ wt bt

w∗ b∗

wt bt

,

w∗ b∗

= wt 2 + b2t 1 + (b∗)2

≤ t(R2 + 1) 1 + (b∗)2.

Squaring both sides of the inequality and rearranging the terms yields an upper bound on the number of updates and hence the number of mistakes.

The Perceptron was the building block of research on Neural Networks [Hay98, Bis95]. The key insight was to combine large numbers of such networks, often in a cascading fashion, to larger objects and to fashion optimization algorithms which would lead to classiﬁers with desirable properties. In this book we will take a complementary route. Instead of increasing the number of nodes we will investigate what happens when increasing the complexity of the feature map φ and its associated kernel k. The advantage of doing so is that we will reap the beneﬁts from convex analysis and linear models, possibly at the expense of a slightly more costly function evaluation.

1.3.5 K-Means
All the algorithms we discussed so far are supervised, that is, they assume that labeled training data is available. In many applications this is too much to hope for; labeling may be expensive, error prone, or sometimes impossible. For instance, it is very easy to crawl and collect every page within the www.purdue.edu domain, but rather time consuming to assign a topic to each page based on its contents. In such cases, one has to resort to unsupervised learning. A prototypical unsupervised learning algorithm is K-means, which is clustering algorithm. Given X = {x1, . . . , xm} the goal of K-means is to partition it into k clusters such that each point in a cluster is similar to points from its own cluster than with points from some other cluster.

1.3 Basic Algorithms

33

Towards this end, deﬁne prototype vectors µ1, . . . , µk and an indicator vector rij which is 1 if, and only if, xi is assigned to cluster j. To cluster our dataset we will minimize the following distortion measure, which minimizes the distance of each point from the prototype vector:

1m J(r, µ) :=
2

k
rij xi − µj 2,

i=1 j=1

(1.29)

where r = {rij}, µ = {µj}, and · 2 denotes the usual Euclidean square norm.
Our goal is to ﬁnd r and µ, but since it is not easy to jointly minimize J with respect to both r and µ, we will adapt a two stage strategy:

Stage 1 Keep the µ ﬁxed and determine r. In this case, it is easy to see that the minimization decomposes into m independent problems. The solution for the i-th data point xi can be found by setting:

rij = 1 if j = argmin xi − µj 2,
j

(1.30)

and 0 otherwise.

Stage 2 Keep the r ﬁxed and determine µ. Since the r’s are ﬁxed, J is an

quadratic function of µ. It can be minimized by setting the derivative

with respect to µj to be 0:

m
rij(xi − µj) = 0 for all j.
i=1

(1.31)

Rearranging obtains

µj =

i rij xi . i rij

(1.32)

Since i rij counts the number of points assigned to cluster j, we are essentially setting µj to be the sample mean of the points assigned
to cluster j.

The algorithm stops when the cluster assignments do not change signiﬁcantly. Detailed pseudo-code can be found in Algorithm 1.5.
Two issues with K-Means are worth noting. First, it is sensitive to the choice of the initial cluster centers µ. A number of practical heuristics have been developed. For instance, one could randomly choose k points from the given dataset as cluster centers. Other methods try to pick k points from X which are farthest away from each other. Second, it makes a hard assignment of every point to a cluster center. Variants which we will encounter later in

34

Algorithm 1.5 K-Means

Cluster(X) {Cluster dataset X}

Initialize cluster centers µj for j = 1, . . . , k randomly repeat

for i = 1 to m do

Compute j = argminj=1,...,k d(xi, µj) Set rij = 1 and rij = 0 for all j = j end for

for j = 1 to k do

Compute µj =

i rij xi i rij

end for

until Cluster assignments rij are unchanged return {µ1, . . . , µk} and rij

1 Introduction

the book will relax this. Instead of letting rij ∈ {0, 1} these soft variants will replace it with the probability that a given xi belongs to cluster j.
The K-Means algorithm concludes our discussion of a set of basic machine learning methods for classiﬁcation and regression. They provide a useful starting point for an aspiring machine learning researcher. In this book we will see many more such algorithms as well as connections between these basic algorithms and their more advanced counterparts.

Problems Problem 1.1 (Eyewitness) Assume that an eyewitness is 90% certain that a given person committed a crime in a bar. Moreover, assume that there were 50 people in the restaurant at the time of the crime. What is the posterior probability of the person actually having committed the crime.
Problem 1.2 (DNA Test) Assume the police have a DNA library of 10 million records. Moreover, assume that the false recognition probability is below 0.00001% per record. Suppose a match is found after a database search for an individual. What are the chances that the identiﬁcation is correct? You can assume that the total population is 100 million people. Hint: compute the probability of no match occurring ﬁrst.
Problem 1.3 (Bomb Threat) Suppose that the probability that one of a thousand passengers on a plane has a bomb is 1 : 1, 000, 000. Assuming that the probability to have a bomb is evenly distributed among the passengers,

1.3 Basic Algorithms

35

the probability that two passengers have a bomb is roughly equal to 10−12. Therefore, one might decide to take a bomb on a plane to decrease chances that somebody else has a bomb. What is wrong with this argument?

Problem 1.4 (Monty-Hall Problem) Assume that in a TV show the candidate is given the choice between three doors. Behind two of the doors there is a pencil and behind one there is the grand prize, a car. The candidate chooses one door. After that, the showmaster opens another door behind which there is a pencil. Should the candidate switch doors after that? What is the probability of winning the car?

Problem 1.5 (Mean and Variance for Random Variables) Denote by Xi random variables. Prove that in this case

EX1,...XN

xi = EXi [xi] and VarX1,...XN

i

i

xi = VarXi [xi]

i

i

To show the second equality assume independence of the Xi.

Problem 1.6 (Two Dices) Assume you have a game which uses the maximum of two dices. Compute the probability of seeing any of the events {1, . . . , 6}. Hint: prove ﬁrst that the cumulative distribution function of the maximum of a pair of random variables is the square of the original cumulative distribution function.

Problem 1.7 (Matching Coins) Consider the following game: two players bring a coin each. the ﬁrst player bets that when tossing the coins both will match and the second one bets that they will not match. Show that even if one of the players were to bring a tainted coin, the game still would be fair. Show that it is in the interest of each player to bring a fair coin to the game. Hint: assume that the second player knows that the ﬁrst coin favors heads over tails.

Problem 1.8 (Randomized Maximization) How many observations do you need to draw from a distribution to ensure that the maximum over them is larger than 95% of all observations with at least 95% probability? Hint: generalize the result from Problem 1.6 to the maximum over n random variables.
Application: Assume we have 1000 computers performing MapReduce [DG08] and the Reducers have to wait until all 1000 Mappers are ﬁnished with their job. Compute the quantile of the typical time to completion.

36

1 Introduction

Problem 1.9 Prove that the Normal distribution (1.3) has mean µ and variance σ2. Hint: exploit the fact that p is symmetric around µ.

Problem 1.10 (Cauchy Distribution) Prove that for the density

1 p(x) = π(1 + x2)

(1.33)

mean and variance are undeﬁned. Hint: show that the integral diverges.

Problem 1.11 (Quantiles) Find a distribution for which the mean exceeds the median. Hint: the mean depends on the value of the high-quantile terms, whereas the median does not.

Problem 1.12 (Multicategory Naive Bayes) Prove that for multicategory Naive Bayes the optimal decision is given by

n

y∗(x) := argmax p(y) p([x]i|y)

y

i=1

(1.34)

where y ∈ Y is the class label of the observation x.

Problem 1.13 (Bayes Optimal Decisions) Denote by y∗(x) = argmaxy p(y|x) the label associated with the largest conditional class probability. Prove that for y∗(x) the probability of choosing the wrong label y is given by
l(x) := 1 − p(y∗(x)|x).
Moreover, show that y∗(x) is the label incurring the smallest misclassiﬁcation error.

Problem 1.14 (Nearest Neighbor Loss) Show that the expected loss incurred by the nearest neighbor classiﬁer does not exceed twice the loss of the Bayes optimal decision.

2 Density Estimation

2.1 Limit Theorems

Assume you are a gambler and go to a casino to play a game of dice. As

it happens, it is your unlucky day and among the 100 times you toss the

dice, you only see ’6’ eleven times. For a fair dice we know that each face

should

occur

with

equal

probability

1 6

.

Hence

the

expected

value

over

100

draws

is

100 6

≈

17,

which

is

considerably

more

than

the

eleven

times

that

we

observed. Before crying foul you decide that some mathematical analysis is

in order.

The probability of seeing a particular sequence of m trials out of which n

are

a

’6’

is

given

by

1 6

n

5 6

m−n.

Moreover,

there

are

m n

=

m! n!(m−n)!

diﬀerent

sequences of ’6’ and ’not 6’ with proportions n and m−n respectively. Hence

we may compute the probability of seeing a ’6’ only 11 or less via

11

11 100

Pr(X ≤ 11) = p(i) =

i

i=0

i=0

1 i 5 100−i ≈ 7.0%
66

(2.1)

After looking at this ﬁgure you decide that things are probably reasonable. And, in fact, they are consistent with the convergence behavior of a simulated dice in Figure 2.1. In computing (2.1) we have learned something useful: the expansion is a special case of a binomial series. The ﬁrst term

m=10 m=20 m=50 m=100 m=200 m=500

0.3

0.3

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.1

0.1

0.1

0.0

0.0

0.0

0.0

0.0

0.0

1234 56 1234 56 1234 56 1234 56 1234 56 1234 56

Fig. 2.1. Convergence of empirical means to expectations. From left to right: em-

pirical frequencies of occurrence obtained by casting a dice 10, 20, 50, 100, 200, and

500 times respectively. Note that after 20 throws we still have not observed a single

’6’, an event which occurs with only

5 6

20 ≈ 2.6% probability.

37

38

2 Density Estimation

counts the number of conﬁgurations in which we could observe i times ’6’ in a sequence of 100 dice throws. The second and third term are the probabilities of seeing one particular instance of such a sequence.
Note that in general we may not be as lucky, since we may have considerably less information about the setting we are studying. For instance, we might not know the actual probabilities for each face of the dice, which would be a likely assumption when gambling at a casino of questionable reputation. Often the outcomes of the system we are dealing with may be continuous valued random variables rather than binary ones, possibly even with unknown range. For instance, when trying to determine the average wage through a questionnaire we need to determine how many people we need to ask in order to obtain a certain level of conﬁdence.
To answer such questions we need to discuss limit theorems. They tell us by how much averages over a set of observations may deviate from the corresponding expectations and how many observations we need to draw to estimate a number of probabilities reliably. For completeness we will present proofs for some of the more fundamental theorems in Section 2.1.2. They are useful albeit non-essential for the understanding of the remainder of the book and may be omitted.

2.1.1 Fundamental Laws
The Law of Large Numbers developed by Bernoulli in 1713 is one of the fundamental building blocks of statistical analysis. It states that averages over a number of observations converge to their expectations given a suﬃciently large number of observations and given certain assumptions on the independence of these observations. It comes in two ﬂavors: the weak and the strong law.

Theorem 2.1 (Weak Law of Large Numbers) Denote by X1, . . . , Xm random variables drawn from p(x) with mean µ = EXi[xi] for all i. Moreover let

X¯m

:=

1 m

m

Xi

i=1

(2.2)

be the empirical average over the random variables Xi. Then for any > 0 the following holds

lim Pr
m→∞

X¯m − µ ≤

= 1.

(2.3)

2.1 Limit Theorems

39

6 5 4 3 2 1

101

102

103

Fig. 2.2. The mean of a number of casts of a dice. The horizontal straight line denotes the mean 3.5. The uneven solid line denotes the actual mean X¯n as a function of the number of draws, given as a semilogarithmic plot. The crosses denote the outcomes of the dice. Note how X¯n ever more closely approaches the mean 3.5 are we obtain an increasing number of observations.

This establishes that, indeed, for large enough sample sizes, the average will converge to the expectation. The strong law strengthens this as follows:

Theorem 2.2 (Strong Law of Large Numbers) Under the conditions of Theorem 2.1 we have Pr limm→∞ X¯m = µ = 1.

The strong law implies that almost surely (in a measure theoretic sense) X¯m

converges to µ, whereas the weak law only states that for every the random
variable X¯m will be within the interval [µ− , µ+ ]. Clearly the strong implies the weak law since the measure of the events X¯m = µ converges to 1, hence

any -ball around µ would capture this.

Both laws justify that we may take sample averages, e.g. over a number

of events such as the outcomes of a dice and use the latter to estimate their

means, their probabilities (here we treat the indicator variable of the event

as a {0; 1}-valued random variable), their variances or related quantities. We

postpone a proof until Section 2.1.2, since an eﬀective way of proving Theo-

rem 2.1 relies on the theory of characteristic functions which we will discuss

in the next section. For the moment, we only give a pictorial illustration in

Figure 2.2.

Once we established that the random variable X¯m = m−1

m i=1

Xi

con-

verges to its mean µ, a natural second question is to establish how quickly it

converges and what the properties of the limiting distribution of X¯m −µ are.

Note in Figure 2.2 that the initial deviation from the mean is large whereas

as we observe more data the empirical mean approaches the true one.

40

2 Density Estimation

6 5 4 3 2 1

101

102

103

Fig. 2.3. Five instantiations of a running average over outcomes of a toss of a dice. Note that all of them converge to the mean 3.5. Moreover note that they all are well contained within the upper and lower envelopes given by µ ± VarX [x]/m.

The central limit theorem answers this question exactly by addressing a slightly more general question, namely whether the sum over a number of independent random variables where each of them arises from a diﬀerent distribution might also have a well behaved limiting distribution. This is the case as long as the variance of each of the random variables is bounded. The limiting distribution of such a sum is Gaussian. This aﬃrms the pivotal role of the Gaussian distribution.

Theorem 2.3 (Central Limit Theorem) Denote by Xi independent random variables with means µi and standard deviation σi. Then

Zm :=

m

−

1 2

σi2

i=1

m
Xi − µi
i=1

(2.4)

converges to a Normal Distribution with zero mean and unit variance.

Note that just like the law of large numbers the central limit theorem (CLT) is an asymptotic result. That is, only in the limit of an inﬁnite number of observations will it become exact. That said, it often provides an excellent approximation even for ﬁnite numbers of observations, as illustrated in Figure 2.4. In fact, the central limit theorem and related limit theorems build the foundation of what is known as asymptotic statistics.

Example 2.1 (Dice) If we are interested in computing the mean of the values returned by a dice we may apply the CLT to the sum over m variables

2.1 Limit Theorems

41

which have all mean µ = 3.5 and variance (see Problem 2.1)

VarX [x] = EX [x2] − EX [x]2 = (1 + 4 + 9 + 16 + 25 + 36)/6 − 3.52 ≈ 2.92.

We now study the random variable Wm := m−1

m i=1

[Xi

−

3.5].

Since

each

of the terms in the sum has zero mean, also Wm’s mean vanishes. Moreover,

Wm is a multiple of Zm of (2.4). Hence we have that Wm converges to a

normal

distribution

with

zero

mean

and

standard

deviation

2.92m−

1 2

.

Consequently the average of m tosses of the dice yields a random vari-

able with mean 3.5 and it will approach a normal distribution with variance

m−

1 2

2.92.

In

other

words,

the

empirical

mean

converges

to

its

average

at

rate

O(m−

1 2

).

Figure

2.3

gives

an

illustration

of

the

quality

of

the

bounds

implied by the CLT.

One remarkable property of functions of random variables is that in many conditions convergence properties of the random variables are bestowed upon the functions, too. This is manifest in the following two results: a variant of Slutsky’s theorem and the so-called delta method. The former deals with limit behavior whereas the latter deals with an extension of the central limit theorem.

Theorem 2.4 (Slutsky’s Theorem) Denote by Xi, Yi sequences of random variables with Xi → X and Yi → c for c ∈ R in probability. Moreover, denote by g(x, y) a function which is continuous for all (x, c). In this case the random variable g(Xi, Yi) converges in probability to g(X, c).
For a proof see e.g. [Bil68]. Theorem 2.4 is often referred to as the continuous mapping theorem (Slutsky only proved the result for aﬃne functions). It means that for functions of random variables it is possible to pull the limiting procedure into the function. Such a device is useful when trying to prove asymptotic normality and in order to obtain characterizations of the limiting distribution.

Theorem 2.5 (Delta Method) Assume that Xn ∈ Rd is asymptotically normal with a−n 2(Xn − b) → N(0, Σ) for a2n → 0. Moreover, assume that g : Rd → Rl is a mapping which is continuously diﬀerentiable at b. In this
case the random variable g(Xn) converges

a−n 2 (g(Xn) − g(b)) → N(0, [∇xg(b)]Σ[∇xg(b)] ).

(2.5)

Proof Via a Taylor expansion we see that

an−2 [g(Xn) − g(b)] = [∇xg(ξn)] a−n 2(Xn − b)

(2.6)

42

2 Density Estimation

Here ξn lies on the line segment [b, Xn]. Since Xn → b we have that ξn → b, too. Since g is continuously diﬀerentiable at b we may apply Slutsky’s theorem to see that a−n 2 [g(Xn) − g(b)] → [∇xg(b)] a−n 2(Xn − b). As a consequence, the transformed random variable is asymptotically normal with covariance [∇xg(b)]Σ[∇xg(b)] .
We will use the delta method when it comes to investigating properties of maximum likelihood estimators in exponential families. There g will play the role of a mapping between expectations and the natural parametrization of a distribution.

2.1.2 The Characteristic Function
The Fourier transform plays a crucial role in many areas of mathematical analysis and engineering. This is equally true in statistics. For historic reasons its applications to distributions is called the characteristic function, which we will discuss in this section. At its foundations lie standard tools from functional analysis and signal processing [Rud73, Pap62]. We begin by recalling the basic properties:

Deﬁnition 2.6 (Fourier Transform) Denote by f : Rn → C a function deﬁned on a d-dimensional Euclidean space. Moreover, let x, ω ∈ Rn. Then the Fourier transform F and its inverse F −1 are given by

F

[f

](ω)

:=

(2π)−

d 2

F

−1[g](x)

:=

(2π)−

d 2

f (x) exp(−i ω, x )dx
Rn
g(ω) exp(i ω, x )dω.
Rn

(2.7) (2.8)

The key insight is that F −1 ◦ F = F ◦ F −1 = Id. In other words, F and

F −1 are inverses to each other for all functions which are L2 integrable on Rd, which includes probability distributions. One of the key advantages of

Fourier transforms is that derivatives and convolutions on f translate into

d
multiplications. That is F [f ◦ g] = (2π) 2 F [f ] · F [g]. The same rule applies

to

the

inverse

transform,

i.e.

F −1[f

◦

g]

=

(2π)

d 2

F

−1[f

]F

−1

[g].

The beneﬁt for statistical analysis is that often problems are more easily

expressed in the Fourier domain and it is easier to prove convergence results

there. These results then carry over to the original domain. We will be

exploiting this fact in the proof of the law of large numbers and the central

limit theorem. Note that the deﬁnition of Fourier transforms can be extended

to more general domains such as groups. See e.g. [BCR84] for further details.

2.1 Limit Theorems

43

We next introduce the notion of a characteristic function of a distribution.1

Deﬁnition 2.7 (Characteristic Function) Denote by p(x) a distribution
of a random variable X ∈ Rd. Then the characteristic function φX (ω) with ω ∈ Rd is given by

φX

(ω)

:=

(2π)

d 2

F

−1[p(x)]

=

exp(i ω, x )dp(x).

(2.9)

In other words, φX (ω) is the inverse Fourier transform applied to the probability measure p(x). Consequently φX (ω) uniquely characterizes p(x) and moreover, p(x) can be recovered from φX (ω) via the forward Fourier transform. One of the key utilities of characteristic functions is that they allow us to deal in easy ways with sums of random variables.

Theorem 2.8 (Sums of random variables and convolutions) Denote by X, Y ∈ R two independent random variables. Moreover, denote by Z := X + Y the sum of both random variables. Then the distribution over Z satisﬁes p(z) = p(x) ◦ p(y). Moreover, the characteristic function yields:

φZ (ω) = φX (ω)φY (ω).

(2.10)

Proof Z is given by Z = X + Y . Hence, for a given Z = z we have the freedom to choose X = x freely provided that Y = z − x. In terms of distributions this means that the joint distribution p(z, x) is given by

p(z, x) = p(Y = z − x)p(x)

and hence p(z) = p(Y = z − x)dp(x) = [p(x) ◦ p(y)](z).

The result for characteristic functions follows form the property of the Fourier transform.
For sums of several random variables the characteristic function is the product of the individual characteristic functions. This allows us to prove both the weak law of large numbers and the central limit theorem (see Figure 2.4 for an illustration) by proving convergence in the Fourier domain. Proof [Weak Law of Large Numbers] At the heart of our analysis lies a Taylor expansion of the exponential into
exp(iwx) = 1 + i w, x + o(|w|)
and hence φX (ω) = 1 + iwEX [x] + o(|w|).
1 In Chapter ?? we will discuss more general descriptions of distributions of which φX is a special case. In particular, we will replace the exponential exp(i ω, x ) by a kernel function k(x, x ).

44

2 Density Estimation

1.0

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0.0

-5 0 5

-5 0 5

-5 0 5

-5 0 5

-5 0 5

1.5

1.5

1.5

1.5

1.5

1.0

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

0.0

-1 0 1

-1 0 1

-1 0 1

-1 0 1

-1 0 1

Fig. 2.4. A working example of the central limit theorem. The top row contains distributions of sums of uniformly distributed random variables on the interval [0.5, 0.5]. From left to right we have sums of 1, 2, 4, 8 and 16 random var√iables. The bottom row contains the same distribution with the means rescaled by m, where m is the number of observations. Note how the distribution converges increasingly to the normal distribution.

Given m random variables Xi with mean EX [x] = µ this means that their

average

X¯m

:=

1 m

m i=1

Xi

has

the

characteristic

function

φX¯m (ω) =

1 + i wµ + o(m−1 |w|) m m

(2.11)

In the limit of m → ∞ this converges to exp(iwµ), the characteristic func-
tion of the constant distribution with mean µ. This proves the claim that in the large sample limit X¯m is essentially constant with mean µ.

Proof [Central Limit Theorem] We use the same idea as above to prove
the CLT. The main diﬀerence, though, is that we need to assume that the
second moments of the random variables Xi exist. To avoid clutter we only prove the case of constant mean EXi[xi] = µ and variance VarXi[xi] = σ2.

2.1 Limit Theorems

45

Let

Zm

:=

√1 mσ2

mi=1(Xi − µ). Our proof relies on showing convergence

of the characteristic function of Zm, i.e. φZm to that of a normally dis-

tributed random variable W with zero mean and unit variance. Expanding

the exponential to second order yields:

exp(iwx) = 1 + iwx − 1 w2x2 + o(|w|2) 2

and

hence

φX (ω)

=

1

+

iwEX [x]

−

1 2

w2VarX

[x]

+

o(|w|2)

Since the mean of Zm vanishes by centering (Xi − µ) and the variance per variable is m−1 we may write the characteristic function of Zm via

φZm (ω) =

1 − 1 w2 + o(m−1 |w|2) m 2m

As before, taking limits m → ∞ yields the exponential function. We have

that

limm→∞

φZm (ω)

=

exp(−

1 2

ω2)

which

is

the

characteristic

function

of

the normal distribution with zero mean and variance 1. Since the character-

istic function transform is injective this proves our claim.

Note that the characteristic function has a number of useful properties. For instance, it can also be used as moment generating function via the identity:

∇nωφX (0) = i−nEX [xn].

(2.12)

Its proof is left as an exercise. See Problem 2.2 for details. This connection also implies (subject to regularity conditions) that if we know the moments of a distribution we are able to reconstruct it directly since it allows us to reconstruct its characteristic function. This idea has been exploited in density estimation [Cra46] in the form of Edgeworth and Gram-Charlier expansions [Hal92].

2.1.3 Tail Bounds
In practice we never have access to an inﬁnite number of observations. Hence the central limit theorem does not apply but is just an approximation to the real situation. For instance, in the case of the dice, we might want to state worst case bounds for ﬁnite sums of random variables to determine by how much the empirical mean may deviate from its expectation. Those bounds will not only be useful for simple averages but to quantify the behavior of more sophisticated estimators based on a set of observations.
The bounds we discuss below diﬀer in the amount of knowledge they assume about the random variables in question. For instance, we might only

46

2 Density Estimation

know their mean. This leads to the Gauss-Markov inequality. If we know their mean and their variance we are able to state a stronger bound, the Chebyshev inequality. For an even stronger setting, when we know that each variable has bounded range, we will be able to state a Chernoﬀ bound. Those bounds are progressively more tight and also more diﬃcult to prove. We state them in order of technical sophistication.

Theorem 2.9 (Gauss-Markov) Denote by X ≥ 0 a random variable and

let µ be its mean. Then for any > 0 we have

µ Pr(X ≥ ) ≤ .

(2.13)

Proof We use the fact that for nonnegative random variables

Pr(X ≥ ) =

∞
dp(x) ≤

∞x dp(x) ≤

−1

∞

µ

xdp(x) = .

0

This means that for random variables with a small mean, the proportion of

samples with large value has to be small.

Consequently deviations from the mean are O( −1). However, note that this bound does not depend on the number of observations. A useful application of the Gauss-Markov inequality is Chebyshev’s inequality. It is a statement on the range of random variables using its variance.

Theorem 2.10 (Chebyshev) Denote by X a random variable with mean µ and variance σ2. Then the following holds for > 0:

σ2 Pr(|x − µ| ≥ ) ≤ 2 .

(2.14)

Proof Denote by Y := |X − µ|2 the random variable quantifying the deviation of X from its mean µ. By construction we know that EY [y] = σ2. Next let γ := 2. Applying Theorem 2.9 to Y and γ yields Pr(Y > γ) ≤ σ2/γ
which proves the claim.

Note the improvement to the Gauss-Markov inequality. Where before we had bounds whose conﬁdence improved with O( −1) we can now state O( −2)
bounds for deviations from the mean.

Example 2.2 (Chebyshev bound) Assume that X¯m := m−1

m i=1

Xi

is

the average over m random variables with mean µ and variance σ2. Hence

X¯m also has mean µ. Its variance is given by

m
VarX¯m [x¯m] = m−2VarXi [xi] = m−1σ2.
i=1

2.1 Limit Theorems

47

Applying Chebyshev’s inequality yields that the probability of a deviation

of

from

the

mean

µ

is

bounded

by

σ2 m2

.

For

ﬁxed

failure

probability

δ

=

Pr(|X¯m − µ| > ) we have

√ δ ≤ σ2m−1 −2 and equivalently ≤ σ/ mδ.

This bound is quite reasonable for large δ but it means that for high levels of conﬁdence we need a huge number of observations.

Much stronger results can be obtained if we are able to bound the range of the random variables. Using the latter, we reap an exponential improvement in the quality of the bounds in the form of the McDiarmid [McD89] inequality. We state the latter without proof:

Theorem 2.11 (McDiarmid) Denote by f : Xm → R a function on X and let Xi be independent random variables. In this case the following holds:

Pr (|f (x1, . . . , xm) − EX1,...,Xm[f (x1, . . . , xm)]| > ) ≤ 2 exp −2 2C−2 .

Here the constant C2 is given by C2 =

m i=1

c2i

where

f (x1, . . . , xi, . . . , xm) − f (x1, . . . , xi, . . . , xm) ≤ ci

for all x1, . . . , xm, xi and for all i.

This bound can be used for averages of a number of observations when they are computed according to some algorithm as long as the latter can be encoded in f . In particular, we have the following bound [Hoe63]:

Theorem 2.12 (Hoeﬀding) Denote by Xi iid random variables with bounded

range Xi ∈ [a, b] and mean µ. Let X¯m := m−1

m i=1

Xi

be

their

average.

Then the following bound holds:

Pr X¯m − µ >

2m 2 ≤ 2 exp − (b − a)2 .

(2.15)

Proof This is a corollary of Theorem 2.11. In X¯m each individual random variable has range [a/m, b/m] and we set f (X1, . . . , Xm) := X¯m. Straightforward algebra shows that C2 = m−2(b − a)2. Plugging this back into
McDiarmid’s theorem proves the claim.

Note that (2.15) is exponentially better than the previous bounds. With increasing sample size the conﬁdence level also increases exponentially.

Example 2.3 (Hoeﬀding bound) As in example 2.2 assume that Xi are iid random variables and let X¯m be their average. Moreover, assume that

48

2 Density Estimation

Xi ∈ [a, b] for all i. As before we want to obtain guarantees on the probability that |X¯m − µ| > . For a given level of conﬁdence 1 − δ we need to solve

δ ≤ 2 exp

−

2m 2 (b−a)2

(2.16)

for . Straightforward algebra shows that in this case needs to satisfy

≥ |b − a| [log 2 − log δ] /2m

(2.17)

In other words, while the conﬁdence level only enters logarithmically into the

inequality, the sample size m improves our conﬁdence only with

=

O(m−

1 2

).

That is, in order to improve our conﬁdence interval from = 0.1 to = 0.01

we need 100 times as many observations.

While this bound is tight (see Problem 2.5 for details), it is possible to obtain better bounds if we know additional information. In particular knowing a bound on the variance of a random variable in addition to knowing that it has bounded range would allow us to strengthen the statement considerably. The Bernstein inequality captures this connection. For details see [BBL05] or works on empirical process theory [vdVW96, SW86, Vap82].

2.1.4 An Example
It is probably easiest to illustrate the various bounds using a concrete example. In a semiconductor fab processors are produced on a wafer. A typical 300mm wafer holds about 400 chips. A large number of processing steps are required to produce a ﬁnished microprocessor and often it is impossible to assess the eﬀect of a design decision until the ﬁnished product has been produced.
Assume that the production manager wants to change some step from process ’A’ to some other process ’B’. The goal is to increase the yield of the process, that is, the number of chips of the 400 potential chips on the wafer which can be sold. Unfortunately this number is a random variable, i.e. the number of working chips per wafer can vary widely between diﬀerent wafers. Since process ’A’ has been running in the factory for a very long time we may assume that the yield is well known, say it is µA = 350 out of 400 processors on average. It is our goal to determine whether process ’B’ is better and what its yield may be. Obviously, since production runs are expensive we want to be able to determine this number as quickly as possible, i.e. using as few wafers as possible. The production manager is risk averse and wants to ensure that the new process is really better. Hence he requires a conﬁdence level of 95% before he will change the production.

2.1 Limit Theorems

49

A ﬁrst step is to formalize the problem. Since we know process ’A’ exactly we only need to concern ourselves with ’B’. We associate the random variable Xi with wafer i. A reasonable (and somewhat simplifying) assumption is to posit that all Xi are independent and identically distributed where all Xi have the mean µB. Obviously we do not know µB — otherwise there would be no reason for testing! We denote by X¯m the average of the yields of m wafers using process ’B’. What we are interested in is the accuracy for which the probability
δ = Pr(|X¯m − µB| > ) satisﬁes δ ≤ 0.05.

Let us now discuss how the various bounds behave. For the sake of the argument assume that µB − µA = 20, i.e. the new process produces on average 20 additional usable chips.

Chebyshev In order to apply the Chebyshev inequality we need to bound the variance of the random variables Xi. The worst possible variance would occur if Xi ∈ {0; 400} where both events occur with equal probability. In other words, with equal probability the wafer if fully usable or it is entirely broken. This amounts to σ2 = 0.5(200 − 0)2 + 0.5(200 − 400)2 = 40, 000. Since for Chebyshev bounds we have

δ ≤ σ2m−1 −2

(2.18)

we can solve for m = σ2/δ 2 = 40, 000/(0.05 · 400) = 20, 000. In other words, we would typically need 20,000 wafers to assess with reasonable conﬁdence whether process ’B’ is better than process ’A’. This is completely unrealistic.
Slightly better bounds can be obtained if we are able to make better assumptions on the variance. For instance, if we can be sure that the yield of process ’B’ is at least 300, then the largest possible variance is 0.25(300 − 0)2 + 0.75(300 − 400)2 = 30, 000, leading to a minimum of 15,000 wafers which is not much better.

Hoeﬀding Since the yields are in the interval {0, . . . , 400} we have an explicit bound on the range of observations. Recall the inequality (2.16) which bounds the failure probably δ = 0.05 by an exponential term. Solving this for m yields

m ≥ 0.5|b − a|2 −2 log(2/δ) ≈ 737.8

(2.19)

In other words, we need at lest 738 wafers to determine whether process ’B’ is better. While this is a signiﬁcant improvement of almost two orders of magnitude, it still seems wasteful and we would like to do better.

50

2 Density Estimation

Central Limit Theorem The central limit theorem is an approximation.
This means that our reasoning is not accurate any more. That said, for
large enough sample sizes, the approximation is good enough to use it for practical predictions. Assume for the moment that we knew the variance σ2 exactly. In this case we know that X¯m is approximately normal with mean µB and variance m−1σ2. We are interested in the interval [µ − , µ + ] which contains 95% of the probability mass of a normal distribution. That is, we
need to solve the integral

1

µ+

(x − µ)2

2πσ2 µ− exp − 2σ2

dx = 0.95

(2.20)

This can be solved eﬃciently using the cumulative distribution function of a normal distribution (see Problem 2.3 for more details). One can check that (2.20) is solved for = 2.96σ. In other words, an interval of ±2.96σ contains 95% of the probability mass of a normal distribution. The number of observations is therefore determined by

√

σ2

= 2.96σ/ m and hence m = 8.76 2

(2.21)

Again, our problem is that we do not know the variance of the distribution. Using the worst-case bound on the variance, i.e. σ2 = 40, 000 would lead to a requirement of at least m = 876 wafers for testing. However, while we do not know the variance, we may estimate it along with the mean and use the empirical estimate, possibly plus some small constant to ensure we do not underestimate the variance, instead of the upper bound.
Assuming that ﬂuctuations turn out to be in the order of 50 processors, i.e. σ2 = 2500, we are able to reduce our requirement to approximately 55 wafers. This is probably an acceptable number for a practical test.

Rates and Constants The astute reader will have noticed that all three conﬁdence bounds had scaling behavior m = O( −2). That is, in all cases the number of observations was a fairly ill behaved function of the amount of conﬁdence required. If we were just interested in convergence per se, a statement like that of the Chebyshev inequality would have been entirely suﬃcient. The various laws and bounds can often be used to obtain considerably better constants for statistical conﬁdence guarantees. For more complex estimators, such as methods to classify, rank, or annotate data, a reasoning such as the one above can become highly nontrivial. See e.g. [MYA94, Vap98] for further details.

2.2 Parzen Windows

51

2.2 Parzen Windows
2.2.1 Discrete Density Estimation
The convergence theorems discussed so far mean that we can use empirical observations for the purpose of density estimation. Recall the case of the Naive Bayes classiﬁer of Section 1.3.1. One of the key ingredients was the ability to use information about word counts for diﬀerent document classes to estimate the probability p(wj|y), where wj denoted the number of occurrences of word j in document x, given that it was labeled y. In the following we discuss an extremely simple and crude method for estimating probabilities. It relies on the fact that for random variables Xi drawn from distribution p(x) with discrete values Xi ∈ X we have

lim
m→∞

pˆX (x)

=

p(x)

m

where pˆX (x) := m−1 {xi = x} for all x ∈ X.

i=1

(2.22) (2.23)

Let us discuss a concrete case. We assume that we have 12 documents and would like to estimate the probability of occurrence of the word ’dog’ from it. As raw data we have:

Document ID

1 2 3 4 5 6 7 8 9 10 11 12

Occurrences of ‘dog’ 1 0 2 0 4 6 3 0 6 2 0 1

This means that the word ‘dog’ occurs the following number of times:

Occurrences of ‘dog’ 0 1 2 3 4 5 6

Number of documents 4 2 2 1 1 0 2

Something unusual is happening here: for some reason we never observed 5 instances of the word dog in our documents, only 4 and less, or alternatively 6 times. So what about 5 times? It is reasonable to assume that the corresponding value should not be 0 either. Maybe we did not sample enough. One possible strategy is to add pseudo-counts to the observations. This amounts to the following estimate:

m
pˆX (x) := (m + |X|)−1 1 + {xi = x} = p(x)
i=1

(2.24)

Clearly the limit for m → ∞ is still p(x). Hence, asymptotically we do not lose anything. This prescription is what we used in Algorithm 1.1 used a method called Laplace smoothing. Below we contrast the two methods:

52

2 Density Estimation

Occurrences of ‘dog’

0123

4

56

Number of documents 4 2 2 1

1

02

Frequency of occurrence 0.33 0.17 0.17 0.083 0.083 0 0.17

Laplace smoothing

0.26 0.16 0.16 0.11 0.11 0.05 0.16

The problem with this method is that as |X| increases we need increasingly more observations to obtain even a modicum of precision. On average, we will need at least one observation for every x ∈ X. This can be infeasible for large domains as the following example shows.

Example 2.4 (Curse of Dimensionality) Assume that X = {0, 1}d, i.e. x consists of binary bit vectors of dimensionality d. As d increases the size of X increases exponentially, requiring an exponential number of observations to perform density estimation. For instance, if we work with images, a 100 × 100 black and white picture would require in the order of 103010 observations to model such fairly low-resolution images accurately. This is clearly utterly infeasible — the number of particles in the known universe is in the order of 1080. Bellman [Bel61] was one of the ﬁrst to formalize this dilemma by coining the term ’curse of dimensionality’.

This example clearly shows that we need better tools to deal with highdimensional data. We will present one of such tools in the next section.

2.2.2 Smoothing Kernel

We now proceed to proper density estimation. Assume that we want to

estimate the distribution of weights of a population. Sample data from a

population might look as follows: X = {57, 88, 54, 84, 83, 59, 56, 43, 70, 63,

90, 98, 102, 97, 106, 99, 103, 112}. We could use this to perform a density

estimate by placing discrete components at the locations xi ∈ X with weight 1/|X| as what is done in Figure 2.5. There is no reason to believe that weights

are quantized in kilograms, or grams, or miligrams (or pounds and stones).

And even if it were, we would expect that similar weights would have similar

densities associated with it. Indeed, as the right diagram of Figure 2.5 shows,

the corresponding density is continuous.

The key question arising is how we may transform X into a realistic

estimate of the density p(x). Starting with a ’density estimate’ with only

discrete terms

1m

pˆ(x) = m

δ(x − xi)

i=1

(2.25)

2.2 Parzen Windows

53

we may choose to smooth it out by a smoothing kernel h(x) such that the
probability mass becomes somewhat more spread out. For a density estimate on X ⊆ Rd this is achieved by

1 pˆ(x) =
m

m
r−dh

x−xi r

.

i=1

(2.26)

This expansion is commonly known as the Parzen windows estimate. Note that obviously h must be chosen such that h(x) ≥ 0 for all x ∈ X and moreover that h(x)dx = 1 in order to ensure that (2.26) is a proper probability distribution. We now formally justify this smoothing. Let R be a small region such that

q = p(x) dx.
R
Out of the m samples drawn from p(x), the probability that k of them fall in region R is given by the binomial distribution
m qk(1 − q)m−k. k
The expected fraction of points falling inside the region can easily be computed from the expected value of the Binomial distribution: E[k/m] = q. Similarly, the variance can be computed as Var[k/m] = q(1 − q)/m. As m → ∞ the variance goes to 0 and hence the estimate peaks around the expectation. We can therefore set

k ≈ mq.

If we assume that R is so small that p(x) is constant over R, then

q ≈ p(x) · V,

where V is the volume of R. Rearranging we obtain

k p(x) ≈ .
mV

(2.27)

Let us now set R to be a cube with side length r, and deﬁne a function

h(u) =

1

if

|ui|

≤

1 2

0 otherwise.

Observe that h

x−xi r

is 1 if and only if xi lies inside a cube of size r centered

54

2 Density Estimation

around x. If we let

m
k= h

x − xi

,

r

i=1

then one can use (2.27) to estimate p via

1 pˆ(x) =

m
r−dh

x − xi

,

m

r

i=1

where rd is the volume of the hypercube of size r in d dimensions. By symmetry, we can interpret this equation as the sum over m cubes centered around m data points xn. If we replace the cube by any smooth kernel function h(·) this recovers (2.26).
There exists a large variety of diﬀerent kernels which can be used for the kernel density estimate. [Sil86] has a detailed description of the properties of a number of kernels. Popular choices are

h(x)

=

(2π

)−

1 2

e−

1 2

x2

Gaussian kernel

(2.28)

h(x)

=

1 2

e−|x|

h(x)

=

3 4

max(0, 1

−

x2)

h(x)

=

1 2

χ[−1,1](x)

h(x) = max(0, 1 − |x|)

Laplace kernel Epanechnikov kernel Uniform kernel Triangle kernel.

(2.29) (2.30) (2.31) (2.32)

Further kernels are the triweight and the quartic kernel which are basically powers of the Epanechnikov kernel. For practical purposes the Gaussian kernel (2.28) or the Epanechnikov kernel (2.30) are most suitable. In particular, the latter has the attractive property of compact support. This means that for any given density estimate at location x we will only need to evaluate terms h(xi − x) for which the distance xi − x is less than r. Such expansions are computationally much cheaper, in particular when we make use of fast nearest neighbor search algorithms [GIM99, IM98]. Figure 2.7 has some examples of kernels.

2.2.3 Parameter Estimation
So far we have not discussed the issue of parameter selection. It should be evident from Figure 2.6, though, that it is quite crucial to choose a good kernel width. Clearly, a kernel that is overly wide will oversmooth any ﬁne detail that there might be in the density. On the other hand, a very narrow kernel will not be very useful, since it will be able to make statements only about the locations where we actually observed data.

2.2 Parzen Windows
0.10

0.05

0.0040

50

60

70

80

90

100

110

55

0.05

0.04

0.03

0.02

0.01

0.0040

50

60

70

80

90

100

110

Fig. 2.5. Left: a naive density estimate given a sample of the weight of 18 persons. Right: the underlying weight distribution.

0.050

0.050

0.050

0.050

0.025

0.025

0.025

0.025

0.000

0.000

0.000

0.000

40 60 80 100 40 60 80 100 40 60 80 100 40 60 80 100

Fig. 2.6. Parzen windows density estimate associated with the 18 observations of the Figure above. From left to right: Gaussian kernel density estimate with kernel of width 0.3, 1, 3, and 10 respectively.

1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

0.0

0.0

0.0

0.0

-2 -1 0 1 2

-2 -1 0 1 2

-2 -1 0 1 2

-2 -1 0 1 2

Fig. 2.7. Some kernels for Parzen windows density estimation. From left to right: Gaussian kernel, Laplace kernel, Epanechikov kernel, and uniform density.

Moreover, there is the issue of choosing a suitable kernel function. The fact that a large variety of them exists might suggest that this is a crucial issue. In practice, this turns out not to be the case and instead, the choice of a suitable kernel width is much more vital for good estimates. In other words, size matters, shape is secondary.
The problem is that we do not know which kernel width is best for the data. If the problem is one-dimensional, we might hope to be able to eyeball the size of r. Obviously, in higher dimensions this approach fails. A second

56

2 Density Estimation

option would be to choose r such that the log-likelihood of the data is maximized. It is given by

m

m

m

log p(xi) = −m log m +

log

r−dh

xi−xj r

i=1

i=1 j=1

(2.33)

Remark 2.13 (Log-likelihood) We consider the logarithm of the likelihood for reasons of computational stability to prevent numerical underﬂow. While each term p(xi) might be within a suitable range, say 10−2, the product of 1000 of such terms will easily exceed the exponent of ﬂoating point representations on a computer. Summing over the logarithm, on the other hand, is perfectly feasible even for large numbers of observations.
Unfortunately computing the log-likelihood is equally infeasible: for decreasing r the only surviving terms in (2.33) are the functions h((xi − xi)/r) = h(0), since the arguments of all other kernel functions diverge. In other words, the log-likelihood is maximized when p(x) is peaked exactly at the locations where we observed the data. The graph on the left of Figure 2.6 shows what happens in such a situation.
What we just experienced is a case of overﬁtting where our model is too ﬂexible. This led to a situation where our model was able to explain the observed data “unreasonably well”, simply because we were able to adjust our parameters given the data. We will encounter this situation throughout the book. There exist a number of ways to address this problem.
Validation Set: We could use a subset of our set of observations as an estimate of the log-likelihood. That is, we could partition the observations into X := {x1, . . . , xn} and X := {xn+1, . . . , xm} and use the second part for a likelihood score according to (2.33). The second set is typically called a validation set.
n-fold Cross-validation: Taking this idea further, note that there is no particular reason why any given xi should belong to X or X respectively. In fact, we could use all splits of the observations into sets X and X to infer the quality of our estimate. While this is computationally infeasible, we could decide to split the observations into n equally sized subsets, say X1, . . . , Xn and use each of them as a validation set at a time while the remainder is used to generate a density estimate. Typically n is chosen to be 10, in which case this procedure is

2.2 Parzen Windows

57

referred to as 10-fold cross-validation. It is a computationally attractive procedure insofar as it does not require us to change the basic estimation algorithm. Nonetheless, computation can be costly. Leave-one-out Estimator: At the extreme end of cross-validation we could choose n = m. That is, we only remove a single observation at a time and use the remainder of the data for the estimate. Using the average over the likelihood scores provides us with an even more ﬁne-grained estimate. Denote by pi(x) the density estimate obtained by using X := {x1, . . . , xm} without xi. For a Parzen windows estimate this is given by

pi(xi) = (m − 1)−1

r−dh

xi−xj r

j=i

=

m m−1

p(xi) − r−dh(0)

.

(2.34)

Note that this is precisely the term r−dh(0) that is removed from the estimate. It is this term which led to divergent estimates for r → 0. This means that the leave-one-out log-likelihood estimate can be computed easily via

m

L(X)

=

m

log

m m−1

+

log p(xi) − r−dh(0) .

i=1

(2.35)

We then choose r such that L(X) is maximized. This strategy is very robust and whenever it can be implemented in a computationally eﬃcient manner, it is very reliable in performing model selection.

An alternative, probably more of theoretical interest, is to choose the scale r a priori based on the amount of data we have at our disposition. Intuitively, we need a scheme which ensures that r → 0 as the number of observations increases m → ∞. However, we need to ensure that this happens slowly enough that the number of observations within range r keeps on increasing in order to ensure good statistical performance. For details we refer the reader to [Sil86]. Chapter ?? discusses issues of model selection for estimators in general in considerably more detail.

2.2.4 Silverman’s Rule
Assume you are an aspiring demographer who wishes to estimate the population density of a country, say Australia. You might have access to a limited census which, for a random portion of the population determines where they live. As a consequence you will obtain a relatively high number of samples

58

2 Density Estimation

Fig. 2.8. Nonuniform density. Left: original density with samples drawn from the distribution. Middle: density estimate with a uniform kernel. Right: density estimate using Silverman’s adjustment.

of city dwellers, whereas the number of people living in the countryside is

likely to be very small.

If we attempt to perform density estimation using Parzen windows, we

will encounter an interesting dilemma: in regions of high density (i.e. the

cities) we will want to choose a narrow kernel width to allow us to model

the variations in population density accurately. Conversely, in the outback,

a very wide kernel is preferable, since the population there is very low.

Unfortunately, this information is exactly what a density estimator itself

could tell us. In other words we have a chicken and egg situation where

having a good density estimate seems to be necessary to come up with a

good density estimate.

Fortunately this situation can be addressed by realizing that we do not

actually need to know the density but rather a rough estimate of the latter.

This can be obtained by using information about the average distance of the

k nearest neighbors of a point. One of Silverman’s rules of thumb [Sil86] is

to choose ri as

c

ri = k

x − xi .

x∈kN N (xi)

(2.36)

Typically c is chosen to be 0.5 and k is small, e.g. k = 9 to ensure that the estimate is computationally eﬃcient. The density estimate is then given by

1 p(x) =
m

m

ri−dh

x−xi ri

.

i=1

(2.37)

Figure 2.8 shows an example of such a density estimate. It is clear that a locality dependent kernel width is better than choosing a uniformly constant kernel density estimate. However, note that this increases the computational complexity of performing a density estimate, since ﬁrst the k nearest neighbors need to be found before the density estimate can be carried out.

2.2 Parzen Windows

59

2.2.5 Watson-Nadaraya Estimator

Now that we are able to perform density estimation we may use it to perform classiﬁcation and regression. This leads us to an eﬀective method for nonparametric data analysis, the Watson-Nadaraya estimator [Wat64, Nad65].
The basic idea is very simple: assume that we have a binary classiﬁcation problem, i.e. we need to distinguish between two classes. Provided that we are able to compute density estimates p(x) given a set of observations X we could appeal to Bayes rule to obtain

p(x|y)p(y)

p(y|x) =

=

p(x)

my m

·

1 my

1

m

i:yi=y r−dh

xi−x r

m i=1

r−d

h

xi−x r

.

(2.38)

Here we only take the sum over all xi with label yi = y in the numerator. The advantage of this approach is that it is very cheap to design such an estimator. After all, we only need to compute sums. The downside, similar to that of the k-nearest neighbor classiﬁer is that it may require sums (or search) over a large number of observations. That is, evaluation of (2.38) is potentially an O(m) operation. Fast tree based representations can be used to accelerate this [BKL06, KM00], however their behavior depends signiﬁcantly on the dimensionality of the data. We will encounter computationally more attractive methods at a later stage.
For binary classiﬁcation (2.38) can be simpliﬁed considerably. Assume that y ∈ {±1}. For p(y = 1|x) > 0.5 we will choose that we should estimate y = 1 and in the converse case we would estimate y = −1. Taking the diﬀerence between twice the numerator and the denominator we can see that the function

f (x) =

i yih

xi−x r

ih

xi−x r

=

yi
i

h

xi−x r

ih

xi−x r

=:

yiwi(x)
i

(2.39)

can be used to achieve the same goal since f (x) > 0 ⇐⇒ p(y = 1|x) > 0.5.

Note that f (x) is a weighted combination of the labels yi associated with

weights wi(x) which depend on the proximity of x to an observation xi.

In other words, (2.39) is a smoothed-out version of the k-nearest neighbor

classiﬁer of Section 1.3.2. Instead of drawing a hard boundary at the k closest

observation we use a soft weighting scheme with weights wi(x) depending

on which observations are closest.

Note furthermore that the numerator of (2.39) is very similar to the simple

classiﬁer of Section 1.3.3. In fact, for kernels k(x, x ) such as the Gaussian

RBF kernel, which are also kernels in the sense of a Parzen windows den-

sity estimate, i.e. k(x, x ) = r−dh

x−x r

the two terms are identical. This

60

2 Density Estimation

Fig. 2.9. Watson Nadaraya estimate. Left: a binary classiﬁer. The optimal solution would be a straight line since both classes were drawn from a normal distribution with the same variance. Right: a regression estimator. The data was generated from a sinusoid with additive noise. The regression tracks the sinusoid reasonably well.
means that the Watson Nadaraya estimator provides us with an alternative explanation as to why (1.24) leads to a usable classiﬁer.
In the same fashion as the Watson Nadaraya classiﬁer extends the knearest neighbor classiﬁer we also may construct a Watson Nadaraya regression estimator by replacing the binary labels yi by real-valued values yi ∈ R to obtain the regression estimator i yiwi(x). Figure 2.9 has an example of the workings of both a regression estimator and a classiﬁer. They are easy to use and they work well for moderately dimensional data.

2.3 Exponential Families
Distributions from the exponential family are some of the most versatile tools for statistical inference. Gaussians, Poisson, Gamma and Wishart distributions all form part of the exponential family. They play a key role in dealing with graphical models, classiﬁcation, regression and conditional random ﬁelds which we will encounter in later parts of this book. Some of the reasons for their popularity are that they lead to convex optimization problems and that they allow us to describe probability distributions by linear models.

2.3.1 Basics Densities from the exponential family are deﬁned by
p(x; θ) := p0(x) exp ( φ(x), θ − g(θ)) .

(2.40)

2.3 Exponential Families

61

Here p0(x) is a density on X and is often called the base measure, φ(x) is a map from x to the suﬃcient statistics φ(x). θ is commonly referred to as the natural parameter. It lives in the space dual to φ(x). Moreover, g(θ) is a normalization constant which ensures that p(x) is properly normalized. g is often referred to as the log-partition function. The name stems from physics where Z = eg(θ) denotes the number of states of a physical ensemble. g can be computed as follows:

g(θ) = log exp ( φ(x), θ ) dx.
X

(2.41)

Example 2.5 (Binary Model) Assume that X = {0; 1} and that φ(x) =

x. In this case we have g(θ) = log e0 + eθ = log 1 + eθ . It follows that

p(x = 0; θ) =

1 1+eθ

and

p(x

=

1; θ)

=

eθ 1+eθ

.

In other words,

by choosing

diﬀerent values of θ one can recover diﬀerent Bernoulli distributions.

One of the convenient properties of exponential families is that the logpartition function g can be used to generate moments of the distribution itself simply by taking derivatives.

Theorem 2.14 (Log partition function) The function g(θ) is convex. Moreover, the distribution p(x; θ) satisﬁes

∇θg(θ) = Ex [φ(x)] and ∇θ2g(θ) = Varx [φ(x)] .

(2.42)

Proof Note that ∇2θg(θ) = Varx [φ(x)] implies that g is convex, since the covariance matrix is positive semideﬁnite. To show (2.42) we expand

∇θg(θ) =

X φ(x) exp φ(x), θ dx = X exp φ(x), θ

φ(x)p(x; θ)dx = Ex [φ(x)] . (2.43)

Next we take the second derivative to obtain

∇θ2g(θ) = φ(x) [φ(x) − ∇θg(θ)] p(x; θ)dx
X
= Ex φ(x)φ(x) − Ex [φ(x)] Ex [φ(x)]

(2.44) (2.45)

which proves the claim. For the ﬁrst equality we used (2.43). For the second line we used the deﬁnition of the variance.
One may show that higher derivatives ∇nθ g(θ) generate higher order cumulants of φ(x) under p(x; θ). This is why g is often also referred as the cumulant-generating function. Note that in general, computation of g(θ)

62

2 Density Estimation

is nontrivial since it involves solving a highdimensional integral. For many

cases, in fact, the computation is NP hard, for instance when X is the do-

main of permutations [FJ95]. Throughout the book we will discuss a number

of approximation techniques which can be applied in such a case.

Let us brieﬂy illustrate (2.43) using the binary model of Example 2.5.

We

have

that

∇θ

=

eθ 1+eθ

and

∇θ2

=

eθ (1+eθ

)2

.

This

is

exactly

what

we

would

have obtained from direct computation of the mean p(x = 1; θ) and variance

p(x = 1; θ) − p(x = 1; θ)2 subject to the distribution p(x; θ).

2.3.2 Examples
A large number of densities are members of the exponential family. Note, however, that in statistics it is not common to express them in the dot product formulation for historic reasons and for reasons of notational compactness. We discuss a number of common densities below and show why they can be written in terms of an exponential family. A detailed description of the most commonly occurring types are given in a table.

Gaussian Let x, µ ∈ Rd and let Σ ∈ Rd×d where Σ 0, that is, Σ is a positive deﬁnite matrix. In this case the normal distribution can be expressed via

p(x)

=

(2π)−

d 2

|Σ|−

1 2

exp

1 − (x − µ)

Σ−1(x − µ)

2

(2.46)

= exp x

Σ−1µ + tr

1 − xx

Σ−1 − c(µ, Σ)

2

where c(µ, Σ) =

1 2

µ

Σ−1µ

+

d 2

log

2π

+

1 2

log

|Σ|.

By

combining

the

terms

in

x

into

φ(x)

:=

(x,

−

1 2

xx

) we obtain the suﬃcient statistics

of x. The corresponding linear coeﬃcients (Σ−1µ, Σ−1) constitute the

natural parameter θ. All that remains to be done to express p(x) in

terms of (2.40) is to rewrite g(θ) in terms of c(µ, Σ). The summary

table on the following page contains details.

Multinomial Another popular distribution is one over k discrete events.

In this case X = {1, . . . , k} and we have in completely generic terms p(x) = πx where πx ≥ 0 and x πx = 1. Now denote by ex ∈ Rk the x-th unit vector of the canonical basis, that is ex, ex = 1 if x = x
and 0 otherwise. In this case we may rewrite p(x) via

p(x) = πx = exp ( ex, log π )

(2.47)

where log π = (log π1, . . . , log πk). In other words, we have succeeded

2.3 Exponential Families

63

in rewriting the distribution as a member of the exponential family where φ(x) = ex and where θ = log π. Note that in this deﬁnition θ is restricted to a k −1 dimensional manifold (the k dimensional probability simplex). If we relax those constraints we need to ensure that p(x) remains normalized. Details are given in the summary table.
Poisson This distribution is often used to model distributions over discrete events. For instance, the number of raindrops which fall on a given surface area in a given amount of time, the number of stars in a given volume of space, or the number of Prussian soldiers killed by horse-kicks in the Prussian cavalry all follow this distribution. It is given by

e−λλx 1 p(x) = x! = x! exp (x log λ − λ) where x ∈ N0 .

(2.48)

By deﬁning φ(x) = x we obtain an exponential families model. Note

that

things

are

a

bit

less

trivial

here

since

1 x!

is

the

nonuniform

counting measure on N0. The case of the uniform measure which

leads to the exponential distribution is discussed in Problem 2.16.

The reason why many discrete processes follow the Poisson distri-

bution is that it can be seen as the limit over the average of a large

number of Bernoulli draws: denote by z ∈ {0, 1} a random variable

with p(z = 1) = α. Moreover, denote by zn the sum over n draws

from this random variable. In this case zn follows the multinomial

distribution with p(zn = k) =

n k

αk(1

−

α)n−k .

Now

assume

that

we let n → ∞ such that the expected value of zn remains constant.

That

is,

we

rescale

α

=

λ n

.

In

this

case

we

have

n! λk

λ n−k

p(zn = k) = (n − k)!k! nk

1− n

λk

λn

n!

λk

= 1−

k!

n

nk(n − k)!

1− n

(2.49)

For n → ∞ the second term converges to e−λ. The third term converges to 1, since we have a product of only 2k terms, each of which converge to 1. Using the exponential families notation we may check that E[x] = λ and that moreover also Var[x] = λ.
Beta This is a distribution on the unit interval X = [0, 1] which is very versatile when it comes to modelling unimodal and bimodal distri-

64

2 Density Estimation

0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.000

5

10

15

20

25

30

3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.00.0

0.2

0.4

0.6

0.8

1.0

Fig. 2.10. Left: Poisson distributions with λ = {1, 3, 10}. Right: Beta distributions with a = 2 and b ∈ {1, 2, 3, 5, 7}. Note how with increasing b the distribution becomes more peaked close to the origin.

butions. It is given by

p(x) = xa−1(1 − x)b−1 Γ(a + b) . Γ(a)Γ(b)

(2.50)

Taking logarithms we see that this, too, is an exponential families distribution, since p(x) = exp((a − 1) log x + (b − 1) log(1 − x) + log Γ(a + b) − log Γ(a) − log Γ(b)).

Figure 2.10 has a graphical description of the Poisson distribution and the Beta distribution. For a more comprehensive list of exponential family distributions see the table below and [Fel71, FT94, MN83]. In principle any map φ(x), domain X with underlying measure µ are suitable, as long as the log-partition function g(θ) can be computed eﬃciently.

Theorem 2.15 (Convex feasible domain) The domain of deﬁnition Θ of g(θ) is convex.
Proof By construction g is convex and diﬀerentiable everywhere. Hence the below-sets for all values c with {x|g(x) ≤ c} exist. Consequently the domain of deﬁnition is convex.
Having a convex function is very valuable when it comes to parameter inference since convex minimization problems have unique minimum values and global minima. We will discuss this notion in more detail when designing maximum likelihood estimators.

2.3 Exponential Families

Multinomial Exponential Poisson
Laplace

{1..N } N+0 N0+
[0, ∞)

Gaussian

R

Rn

Inverse Normal [0, ∞)

Beta

[0, 1]

Gamma Wishart

[0, ∞) Cn

Dirichlet

Sn

Inverse χ2

R+

Logarithmic N

Conjugate

Θ

Counting
Counting
1 x!
Lebesgue

Lebesgue

Lebesgue

x−

3 2

1

x(1−x)

1

x

|X

|−

n+1 2

(

n i=1

xi)−1

e−

1 2x

1 x

Lebesgue

ex x x

x

x,

−

1 2

x2

x,

−

1 2

xx

−x,

−

1 x

(log x, log (1 − x))

(log x, −x)

log

|x|,

−

1 2

x

(log x1, . . . , log xn) − log x x
(θ, −g(θ))

log

N i=1

eθi

− log 1 − eθ

eθ

log θ

1 2

log

2π

−

1 2

log

θ2

+

1 2

θ12 θ2

n 2
1 2

log log

2π − √12 log |θ2| π − 2 θ1θ2 −

+

1 2

θ1

θ2−1θ1

1 2

log

θ2

log

Γ(θ1)Γ(θ2) Γ(θ1+θ2)

log Γ(θ1) − θ1 log θ2

−θ1 log |θ2| + θ1n log 2

+

n i=1

log

Γ

θ1

+

1−i 2

n i=1

log

Γ(θi)

−

log

Γ

(

n i=1

θi

)

(θ − 1) log 2 + log(θ − 1)

log(− log(1 − eθ))

generic

RN (−∞, 0) R (−∞, 0) R ×(0, ∞) Rn ×Cn (0, ∞)2 R2 (0, ∞)2 R ×Cn
(R+)n (0, ∞) (−∞, 0)

Sn denotes the probability simplex in n dimensions. Cn is the cone of positive semideﬁnite matrices in Rn×n.

65

66

2 Density Estimation

2.4 Estimation
In many statistical problems the challenge is to estimate parameters of interest. For instance, in the context of exponential families, we may want to estimate a parameter θˆ such that it is close to the “true” parameter θ∗ in the distribution. While the problem is fully general, we will describe the relevant steps in obtaining estimates for the special case of the exponential family. This is done for two reasons — ﬁrstly, exponential families are an important special case and we will encounter slightly more complex variants on the reasoning in later chapters of the book. Secondly, they are of a suﬃciently simple form that we are able to show a range of diﬀerent techniques. In more advanced applications only a small subset of those methods may be practically feasible. Hence exponential families provide us with a working example based on which we can compare the consequences of a number of diﬀerent techniques.

2.4.1 Maximum Likelihood Estimation
Whenever we have a distribution p(x; θ) parametrized by some parameter θ we may use data to ﬁnd a value of θ which maximizes the likelihood that the data would have been generated by a distribution with this choice of parameter.
For instance, assume that we observe a set of temperature measurements X = {x1, . . . , xm}. In this case, we could try ﬁnding a normal distribution such that the likelihood p(X; θ) of the data under the assumption of a normal distribution is maximized. Note that this does not imply in any way that the temperature measurements are actually drawn from a normal distribution. Instead, it means that we are attempting to ﬁnd the Gaussian which ﬁts the data in the best fashion.
While this distinction may appear subtle, it is critical: we do not assume that our model accurately reﬂects reality. Instead, we simply try doing the best possible job at modeling the data given a speciﬁed model class. Later we will encounter alternative approaches at estimation, namely Bayesian methods, which make the assumption that our model ought to be able to describe the data accurately.

Deﬁnition 2.16 (Maximum Likelihood Estimator) For a model p(·; θ) parametrized by θ and observations X the maximum likelihood estimator (MLE) is

θˆML[X] := argmax p(X; θ).
θ

(2.51)

2.4 Estimation

67

In the context of exponential families this leads to the following procedure: given m observations drawn iid from some distribution, we can express the joint likelihood as

m

m

p(X; θ) = p(xi; θ) = exp ( φ(xi), θ − g(θ))

i=1

i=1

= exp (m ( µ[X], θ − g(θ)))

1m

where µ[X] := m

φ(xi).

i=1

(2.52) (2.53) (2.54)

Here µ[X] is the empirical average of the map φ(x). Maximization of p(X; θ) is equivalent to minimizing the negative log-likelihood − log p(X; θ). The latter is a common practical choice since for independently drawn data, the product of probabilities decomposes into the sum of the logarithms of individual likelihoods. This leads to the following objective function to be minimized

− log p(X; θ) = m [g(θ) − θ, µ[X] ]

(2.55)

Since g(θ) is convex and θ, µ[X] is linear in θ, it follows that minimization of (2.55) is a convex optimization problem. Using Theorem 2.14 and the ﬁrst order optimality condition ∇θg(θ) = µ[X] for (2.55) implies that
θ = [∇θg]−1 (µ[X]) or equivalently Ex∼p(x;θ)[φ(x)] = ∇θg(θ) = µ[X]. (2.56)

Put another way, the above conditions state that we aim to ﬁnd the distribution p(x; θ) which has the same expected value of φ(x) as what we observed empirically via µ[X]. Under very mild technical conditions a solution to (2.56) exists.
In general, (2.56) cannot be solved analytically. In certain special cases, though, this is easily possible. We discuss two such choices in the following: Multinomial and Poisson distributions.

Example 2.6 (Poisson Distribution) For the Poisson distribution1 where

p(x; θ)

=

1 x!

exp(θx − eθ)

it

follows

that

g(θ)

=

eθ

and

φ(x)

=

x.

This

allows

1 Often the Poisson distribution is speciﬁed using λ := log θ as its rate parameter. In this case we have p(x; λ) = λxe−λ/x! as its parametrization. The advantage of the natural parametrization
using θ is that we can directly take advantage of the properties of the log-partition function as
generating the cumulants of x.

68

2 Density Estimation

us to solve (2.56) in closed form using

∇θ g (θ)

=

eθ

=

1 m

m

xi and hence θ = log

m

xi − log m.

i=1

i=1

(2.57)

Example 2.7 (Multinomial Distribution) For the multinomial distri-

bution the log-partition function is given by g(θ) = log

N i=1

eθi

,

hence

we

have that

∇ig(θ) =

eθi

1m

N j=1

eθj

=

m {xj
j=1

= i} .

(2.58)

It is easy to check that (2.58) is satisﬁed for eθi =

m j=1

{xj

=

i}.

In

other

words, the MLE for a discrete distribution simply given by the empirical

frequencies of occurrence.

The multinomial setting also exhibits two rather important aspects of ex-

ponential families: ﬁrstly, choosing θi = c + log

m i=1

{xj

=

i}

for

any

c

∈

R

will lead to an equivalent distribution. This is the case since the suﬃcient

statistic φ(x) is not minimal. In our context this means that the coordinates

of φ(x) are linearly dependent — for any x we have that j[φ(x)]j = 1, hence we could eliminate one dimension. This is precisely the additional

degree of freedom which is reﬂected in the scaling freedom in θ.

Secondly, for data where some events do not occur at all, the expression

log

m j=1

{xj

=

i}

= log 0 is ill deﬁned. This is due to the fact that this

particular set of counts occurs on the boundary of the convex set within

which the natural parameters θ are well deﬁned. We will see how diﬀerent

types of priors can alleviate the issue.

Using the MLE is not without problems. As we saw in Figure 2.1, conver-

gence can be slow, since we are not using any side information. The latter

can provide us with problems which are both numerically better conditioned

and which show better convergence, provided that our assumptions are ac-

curate. Before discussing a Bayesian approach to estimation, let us discuss

basic statistical properties of the estimator.

2.4.2 Bias, Variance and Consistency When designing any estimator θˆ(X) we would like to obtain a number of desirable properties: in general it should not be biased towards a particular solution unless we have good reason to believe that this solution should be preferred. Instead, we would like the estimator to recover, at least on

2.4 Estimation

69

average, the “correct” parameter, should it exist. This can be formalized in the notion of an unbiased estimator.
Secondly, we would like that, even if no correct parameter can be found, e.g. when we are trying to ﬁt a Gaussian distribution to data which is not normally distributed, that we will converge to the best possible parameter choice as we obtain more data. This is what is understood by consistency.
Finally, we would like the estimator to achieve low bias and near-optimal estimates as quickly as possible. The latter is measured by the eﬃciency of an estimator. In this context we will encounter the Cram´er-Rao bound which controls the best possible rate at which an estimator can achieve this goal. Figure 2.11 gives a pictorial description.

Fig. 2.11. Left: unbiased estimator; the estimates, denoted by circles have as mean the true parameter, as denoted by a star. Middle: consistent estimator. While the true model is not within the class we consider (as denoted by the ellipsoid), the estimates converge to the white star which is the best model within the class that approximates the true model, denoted by the solid star. Right: diﬀerent estimators have diﬀerent regions of uncertainty, as made explicit by the ellipses around the true parameter (solid star).
Deﬁnition 2.17 (Unbiased Estimator) An estimator θˆ[X] is unbiased if for all θ where X ∼ p(X; θ) we have EX[θˆ[X]] = θ.
In other words, in expectation the parameter estimate matches the true parameter. Note that this only makes sense if a true parameter actually exists. For instance, if the data is Poisson distributed and we attempt modeling it by a Gaussian we will obviously not obtain unbiased estimates.
For ﬁnite sample sizes MLE is often biased. For instance, for the normal distribution the variance estimates carry bias O(m−1). See problem 2.19 for details. In general, under fairly mild conditions, MLE is asymptotically unbiased [DGL96]. We prove this for exponential families. For more general settings the proof depends on the dimensionality and smoothness of the family of densities that we have at our disposition.

70

2 Density Estimation

Theorem 2.18 (MLE for Exponential Families) Assume that X is an m-sample drawn iid from p(x; θ). The estimate θˆ[X] = g−1(µ[X]) is asymp-
totically normal with

m−

1 2

[θˆ[X]

−

θ]

→

N(0,

∇θ2g(θ)

−1).

(2.59)

In other words, the estimate θˆ[X] is asymptotically normal, it converges to

the true parameter θ, and moreover, the variance at the correct parameter

is given by the inverse of the covariance matrix of the data, as given by the

second derivative of the log-partition function ∇θ2g(θ).

Proof Denote by µ = ∇θg(θ) the true mean. Moreover, note that ∇θ2g(θ) is

the covariance of the data drawn from p(x; θ). By the central limit theorem

(Theorem

2.3)

we

have

that

n−

1 2

[µ[X]

−

µ]

→

N(0,

∇2θ g (θ)).

Now note that θˆ[X] = [∇θg]−1 (µ[X]). Therefore, by the delta method

(Theorem 2.5) we know that θˆ[X] is also asymptotically normal. Moreover,

by the inverse function theorem the Jacobian of g−1 satisﬁes ∇µ [∇θg]−1 (µ) =

∇2θg(θ) −1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.

Now that we established the asymptotic properties of the MLE for exponen-

tial families it is only natural to ask how much variation one may expect in θˆ[X] when performing estimation. The Cramer-Rao bound governs this.

Theorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from p(X; θ) and let θˆ[X] be an asymptotically unbiased estimator. Denote by I the Fisher information matrix and by B the variance of θˆ[X] where

I := Cov [∇θ log p(X; θ)] and B := Var θˆ[X] .

(2.60)

In this case det IB ≥ 1 for all estimators θˆ[X].

Proof We prove the claim for the scalar case. The extension to matrices is straightforward. Using the Cauchy-Schwarz inequality we have

Cov2 ∇θ log p(X; θ), θˆ[X] ≤ Var [∇θ log p(X; θ)] Var θˆ[X] = IB. (2.61)

Note that at the true parameter the expected log-likelihood score vanishes

EX[∇θ log p(X; θ)] = ∇θ p(X; θ)dX = ∇θ1 = 0.

(2.62)

2.4 Estimation

71

Hence we may simplify the covariance formula by dropping the means via

Cov ∇θ log p(X; θ), θˆ[X] = EX ∇θ log p(X; θ)θˆ[X]

= p(X; θ)θˆ(X)∇θ log p(X; θ)dθ

= ∇θ p(X; θ)θˆ(X)dX = ∇θθ = 1.

Here the last equality follows since we may interchange integration by X and the derivative with respect to θ.

The Cram´er-Rao theorem implies that there is a limit to how well we may estimate a parameter given ﬁnite amounts of data. It is also a yardstick by which we may measure how eﬃciently an estimator uses data. Formally, we deﬁne the eﬃciency as the quotient between actual performance and the Cram´er-Rao bound via

e := 1/det IB.

(2.63)

The closer e is to 1, the lower the variance of the corresponding estimator θˆ(X). Theorem 2.18 implies that for exponential families MLE is asymptot-
ically eﬃcient. It turns out to be generally true.

Theorem 2.20 (Eﬃciency of MLE [Cra46, GW92, Ber85]) The maximum likelihood estimator is asymptotically eﬃcient (e = 1).
So far we only discussed the behavior of θˆ[X] whenever there exists a true θ generating p(θ; X). If this is not true, we need to settle for less: how well θˆ[X] approaches the best possible choice of within the given model class. Such behavior is referred to as consistency. Note that it is not possible to deﬁne consistency per se. For instance, we may ask whether θˆ converges to the optimal parameter θ∗, or whether p(x; θˆ) converges to the optimal density p(x; θ∗), and with respect to which norm. Under fairly general conditions this turns out to be true for ﬁnite-dimensional parameters and smoothly parametrized densities. See [DGL96, vdG00] for proofs and further details.

2.4.3 A Bayesian Approach
The analysis of the Maximum Likelihood method might suggest that inference is a solved problem. After all, in the limit, MLE is unbiased and it exhibits as small variance as possible. Empirical results using a ﬁnite amount of data, as present in Figure 2.1 indicate otherwise.
While not making any assumptions can lead to interesting and general

72

2 Density Estimation

theorems, it ignores the fact that in practice we almost always have some

idea about what to expect of our solution. It would be foolish to ignore such

additional information. For instance, when trying to determine the voltage

of a battery, it is reasonable to expect a measurement in the order of 1.5V

or less. Consequently such prior knowledge should be incorporated into the

estimation process. In fact, the use of side information to guide estimation

turns out to be the tool to building estimators which work well in high

dimensions.

Recall

Bayes’

rule

(1.15)

which

states

that

p(θ|x)

=

p(x|θ)p(θ) p(x)

.

In

our

con-

text this means that if we are interested in the posterior probability of θ

assuming a particular value, we may obtain this using the likelihood (often

referred to as evidence) of x having been generated by θ via p(x|θ) and our

prior belief p(θ) that θ might be chosen in the distribution generating x.

Observe the subtle but important diﬀerence to MLE: instead of treating θ

as a parameter of a density model, we treat θ as an unobserved random

variable which we may attempt to infer given the observations X.

This can be done for a number of diﬀerent purposes: we might want to

infer the most likely value of the parameter given the posterior distribution

p(θ|X). This is achieved by

θˆMAP(X) := argmax p(θ|X) = argmin − log p(X|θ) − log p(θ). (2.64)

θ

θ

The second equality follows since p(X) does not depend on θ. This estimator is also referred to as the Maximum a Posteriori, or MAP estimator. It diﬀers from the maximum likelihood estimator by adding the negative log-prior to the optimization problem. For this reason it is sometimes also referred to as Penalized MLE. Eﬀectively we are penalizing unlikely choices θ via − log p(θ).
Note that using θˆMAP(X) as the parameter of choice is not quite accurate. After all, we can only infer a distribution over θ and in general there is no guarantee that the posterior is indeed concentrated around its mode. A more accurate treatment is to use the distribution p(θ|X) directly via

p(x|X) = p(x|θ)p(θ|X)dθ.

(2.65)

In other words, we integrate out the unknown parameter θ and obtain the density estimate directly. As we will see, it is generally impossible to solve (2.65) exactly, an important exception being conjugate priors. In the other cases one may resort to sampling from the posterior distribution to approximate the integral.
While it is possible to design a wide variety of prior distributions, this book

2.4 Estimation

73

focuses on two important families: norm-constrained prior and conjugate priors. We will encounter them throughout, the former sometimes in the guise of regularization and Gaussian Processes, the latter in the context of exchangeable models such as the Dirichlet Process.
Norm-constrained priors take on the form

p(θ) ∝ exp(−λ

θ − θ0

d p

)

for

p,

d

≥

1

and

λ

>

0.

(2.66)

That is, they restrict the deviation of the parameter value θ from some guess θ0. The intuition is that extreme values of θ are much less likely than more moderate choices of θ which will lead to more smooth and even distributions p(x|θ).
A popular choice is the Gaussian prior which we obtain for p = d = 1 and λ = 1/2σ2. Typically one sets θ0 = 0 in this case. Note that in (2.66) we did not spell out the normalization of p(θ) — in the context of MAP estimation this is not needed since it simply becomes a constant oﬀset in the optimization problem (2.64). We have

θˆMAP[X] = argmin m [g(θ) −

θ, µ[X] ] + λ

θ − θ0

d p

θ

(2.67)

For d, p ≥ 1 and λ ≥ 0 the resulting optimization problem is convex and it has a unique solution. Moreover, very eﬃcient algorithms exist to solve this problem. We will discuss this in detail in Chapter 3. Figure 2.12 shows the regions of equal prior probability for a range of diﬀerent norm-constrained priors.
As can be seen from the diagram, the choice of the norm can have profound consequences on the solution. That said, as we will show in Chapter ??, the estimate θˆMAP is well concentrated and converges to the optimal solution under fairly general conditions.
An alternative to norm-constrained priors are conjugate priors. They are designed such that the posterior p(θ|X) has the same functional form as the prior p(θ). In exponential families such priors are deﬁned via

p(θ|n, ν) = exp ( nν, θ − ng(θ) − h(ν, n)) where

(2.68)

h(ν, n) = log exp ( nν, θ − ng(θ)) dθ.

(2.69)

Note that p(θ|n, ν) itself is a member of the exponential family with the feature map φ(θ) = (θ, −g(θ)). Hence h(ν, n) is convex in (nν, n). Moreover, the posterior distribution has the form
p(θ|X) ∝ p(X|θ)p(θ|n, ν) ∝ exp ( mµ[X] + nν, θ − (m + n)g(θ)) . (2.70)

74

2 Density Estimation

Fig. 2.12. From left to right: regions of equal prior probability in R2 for priors using the 1, 2 and ∞ norm. Note that only the 2 norm is invariant with regard to the coordinate system. As we shall see later, the 1 norm prior leads to solutions where only a small number of coordinates is nonzero.

That is, the posterior distribution has the same form as a conjugate prior

with

parameters

mµ[X]+nν m+n

and

m + n.

In

other

words,

n

acts

like

a

phantom

sample size and ν is the corresponding mean parameter. Such an interpreta-

tion is reasonable given our desire to design a prior which, when combined

with the likelihood remains in the same model class: we treat prior knowl-

edge as having observed virtual data beforehand which is then added to the

actual set of observations. In this sense data and prior become completely

equivalent — we obtain our knowledge either from actual observations or

from virtual observations which describe our belief into how the data gen-

eration process is supposed to behave.

Eq. (2.70) has the added beneﬁt of allowing us to provide an exact nor-

malized version of the posterior. Using (2.68) we obtain that

p(θ|X) = exp

mµ[X] + nν, θ − (m + n)g(θ) − h

mµ[X]+nν m+n

,

m

+

n

.

The main remaining challenge is to compute the normalization h for a range of important conjugate distributions. The table on the following page provides details. Besides attractive algebraic properties, conjugate priors also have a second advantage — the integral (2.65) can be solved exactly:

p(x|X) = exp ( φ(x), θ − g(θ)) ×

exp

mµ[X] + nν, θ

− (m + n)g(θ) − h

mµ[X]+nν m+n

,

m

+

n

dθ

Combining terms one may check that the integrand amounts to the normal-

2.4 Estimation

75

ization in the conjugate distribution, albeit φ(x) added. This yields

p(x|X) = exp

h

mµ[X]+nν+φ(x) m+n+1

,

m

+

n

+

1

−h

mµ[X]+nν m+n

,

m

+

n

Such an expansion is very useful whenever we would like to draw x from p(x|X) without the need to obtain an instantiation of the latent variable θ. We provide explicit expansions in appendix 2. [GS04] use the fact that θ can be integrated out to obtain what is called a collapsed Gibbs sampler for topic models [BNJ03].

2.4.4 An Example

Assume we would like to build a language model based on available documents. For instance, a linguist might be interested in estimating the frequency of words in Shakespeare’s collected works, or one might want to compare the change with respect to a collection of webpages. While models describing documents by treating them as bags of words which all have been obtained independently of each other are exceedingly simple, they are valuable for quick-and-dirty content ﬁltering and categorization, e.g. a spam ﬁlter on a mail server or a content ﬁlter for webpages.
Hence we model a document d as a multinomial distribution: denote by wi for i ∈ {1, . . . , md} the words in d. Moreover, denote by p(w|θ) the probability of occurrence of word w, then under the assumption that the words are independently drawn, we have

md
p(d|θ) = p(wi|θ).
i=1

(2.71)

It is our goal to ﬁnd parameters θ such that p(d|θ) is accurate. For a given collection D of documents denote by mw the number of counts for word w in the entire collection. Moreover, denote by m the total number of words in the entire collection. In this case we have

p(D|θ) = p(di|θ) = p(w|θ)mw .

i

w

(2.72)

Finding suitable parameters θ given D proceeds as follows: In a maximum

likelihood model we set

p(w|θ) = mw . m

(2.73)

In other words, we use the empirical frequency of occurrence as our best

guess and the suﬃcient statistic of D is φ(w) = ew, where ew denotes the unit

vector

which

is

nonzero

only

for

the

“coordinate”

w.

Hence

µ[D]w

=

mw m

.

76

2 Density Estimation

We know that the conjugate prior of the multinomial model is a Dirichlet

model. It follows from (2.70) that the posterior mode is obtained by replacing

µ[D]

by

mµ[D]+nν m+n

.

Denote

by

nw

:=

νw

·n

the

pseudo-counts

arising

from

the conjugate prior with parameters (ν, n). In this case we will estimate the

probability of the word w as

p(w|θ) = mw + nw = mw + nνw .

m+n

m+n

(2.74)

In other words, we add the pseudo counts nw to the actual word counts mw. This is particularly useful when the document we are dealing with is brief, that is, whenever we have little data: it is quite unreasonable to infer from a webpage of approximately 1000 words that words not occurring in this page have zero probability. This is exactly what is mitigated by means of the conjugate prior (ν, n).
Finally, let us consider norm-constrained priors of the form (2.66). In this case, the integral required for

p(D) = p(D|θ)p(θ)dθ

∝

exp

−λ

θ − θ0

d p

+

m

µ[D], θ

− mg(θ)

dθ

is intractable and we need to resort to an approximation. A popular choice is to replace the integral by p(D|θ∗) where θ∗ maximizes the integrand. This is precisely the MAP approximation of (2.64). Hence, in order to perform estimation we need to solve

minimize g(θ) − µ[D], θ
θ

λ +
m

θ − θ0

d p

.

(2.75)

A very simple strategy for minimizing (2.75) is gradient descent. That is for
a given value of θ we compute the gradient of the objective function and take
a ﬁxed step towards its minimum. For simplicity assume that d = p = 2 and λ = 1/2σ2, that is, we assume that θ is normally distributed with variance σ2 and mean θ0. The gradient is given by

1 ∇θ [− log p(D, θ)] = Ex∼p(x|θ)[φ(x)] − µ[D] + mσ2 [θ − θ0]

(2.76)

In other words, it depends on the discrepancy between the mean of φ(x) with respect to our current model and the empirical average µ[X], and the diﬀerence between θ and the prior mean θ0.
Unfortunately, convergence of the procedure θ ← θ − η∇θ [. . .] is usually very slow, even if we adjust the steplength η eﬃciently. The reason is that the gradient need not point towards the minimum as the space is most likely

2.5 Sampling

77

distorted. A better strategy is to use Newton’s method (see Chapter 3 for a detailed discussion and a convergence proof). It relies on a second order Taylor approximation

1 − log p(D, θ + δ) ≈ − log p(D, θ) + δ, G + δ Hδ
2

(2.77)

where G and H are the ﬁrst and second derivatives of − log p(D, θ) with respect to θ. The quadratic expression can be minimized with respect to δ by choosing δ = −H−1G and we can fashion an update algorithm from this by letting θ ← θ −H−1G. One may show (see Chapter 3) that Algorithm 2.1 is quadratically convergent. Note that the prior on θ ensures that H is well conditioned even in the case where the variance of φ(x) is not. In practice this means that the prior ensures fast convergence of the optimization algorithm.

Algorithm 2.1 Newton method for MAP estimation

NewtonMAP(D)

Initialize θ = θ0

while not converged do

Compute

G

=

Ex∼p(x|θ)[φ(x)]

−

µ[D]

+

1 mσ2

[θ

−

θ0]

Compute

H

=

Varx∼p(x|θ)[φ(x)]

+

1 mσ2

1

Update θ ← θ − H−1G

end while

return θ

2.5 Sampling
So far we considered the problem of estimating the underlying probability density, given a set of samples drawn from that density. Now let us turn to the converse problem, that is, how to generate random variables given the underlying probability density. In other words, we want to design a random variable generator. This is useful for a number of reasons:
We may encounter probability distributions where optimization over suitable model parameters is essentially impossible and where it is equally impossible to obtain a closed form expression of the distribution. In these cases it may still be possible to perform sampling to draw examples of the kind of data we expect to see from the model. Chapter ?? discusses a number of graphical models where this problem arises.
Secondly, assume that we are interested in testing the performance of a network router under diﬀerent load conditions. Instead of introducing the under-development router in a live network and wreaking havoc, one could

78

2 Density Estimation

estimate the probability density of the network traﬃc under various load conditions and build a model. The behavior of the network can then be simulated by using a probabilistic model. This involves drawing random variables from an estimated probability distribution.
Carrying on, suppose that we generate data packets by sampling and see an anomalous behavior in your router. In order to reproduce and debug this problem one needs access to the same set of random packets which caused the problem in the ﬁrst place. In other words, it is often convenient if our random variable generator is reproducible; At ﬁrst blush this seems like a contradiction. After all, our random number generator is supposed to generate random variables. This is less of a contradiction if we consider how random numbers are generated in a computer — given a particular initialization (which typically depends on the state of the system, e.g. time, disk size, bios checksum, etc.) the random number algorithm produces a sequence of numbers which, for all practical purposes, can be treated as iid. A simple method is the linear congruential generator [PTVF94]

xi+1 = (axi + b) mod c.

The performance of these iterations depends signiﬁcantly on the choice of the constants a, b, c. For instance, the GNU C compiler uses a = 1103515245, b = 12345 and c = 232. In general b and c need to be relatively prime and a − 1 needs to be divisible by all prime factors of c and by 4. It is very much advisable not to attempt implementing such generators on one’s own unless it is absolutely necessary.
Useful desiderata for a pseudo random number generator (PRNG) are that for practical purposes it is statistically indistinguishable from a sequence of iid data. That is, when applying a number of statistical tests, we will accept the null-hypothesis that the random variables are iid. See Chapter ?? for a detailed discussion of statistical testing procedures for random variables. In the following we assume that we have access to a uniform RNG U [0, 1] which draws random numbers uniformly from the range [0, 1].

2.5.1 Inverse Transformation We now consider the scenario where we would like to draw from some distinctively non-uniform distribution. Whenever the latter is relatively simple this can be achieved by applying an inverse transform:
Theorem 2.21 For z ∼ p(z) with z ∈ Z and an injective transformation φ : Z → X with inverse transform φ−1 on φ(Z) it follows that the random

2.5 Sampling
Discrete Probability Distribution

79
Cumulative Density Function

1
0.3 0.8

0.2

0.6

0.1

0

1

2

3

4

5

0.4

0.2

0

1

2

3

4

5

6

Fig. 2.13. Left: discrete probability distribution over 5 possible outcomes. Right: associated cumulative distribution function. When sampling, we draw x uniformly at random from U [0, 1] and compute the inverse of F .

variable x := φ(z) is drawn from ∇xφ−1(x) · p(φ−1(x)). Here ∇xφ−1(x) denotes the determinant of the Jacobian of φ−1.
This follows immediately by applying a variable transformation for a measure, i.e. we change dp(z) to dp(φ−1(x)) ∇xφ−1(x) . Such a conversion strategy is particularly useful for univariate distributions.

Corollary 2.22 Denote by p(x) a distribution on R with cumulative distri-

bution function F (x ) =

x −∞

dp(x).

Then

the

transformation

x

=

φ(z)

=

F −1(z) converts samples z ∼ U [0, 1] to samples drawn from p(x).

We now apply this strategy to a number of univariate distributions. One of the most common cases is sampling from a discrete distribution.

Example 2.8 (Discrete Distribution) In the case of a discrete distribution over {1, . . . , k} the cumulative distribution function is a step-function with steps at {1, . . . , k} where the height of each step is given by the corresponding probability of the event.
The implementation works as follows: denote by p ∈ [0, 1]k the vector of probabilities and denote by f ∈ [0, 1]k with fi = fi−1 + pi and f1 = p1 the steps of the cumulative distribution function. Then for a random variable z drawn from U [0, 1] we obtain x = φ(z) := argmini {fi ≥ z}. See Figure 2.13 for an example of a distribution over 5 events.

80
Exponential Distribution 1 0.8 0.6 0.4 0.2

2 Density Estimation
Cumulative Distribution Function 1 0.8 0.6 0.4 0.2

0

0

0

2

4

6

8

10 0

2

4

6

8

10

Fig. 2.14. Left: Exponential distribution with λ = 1. Right: associated cumulative distribution function. When sampling, we draw x uniformly at random from U [0, 1] and compute the inverse.

Example 2.9 (Exponential Distribution) The density of a Exponentialdistributed random variable is given by

p(x|λ) = λ exp(−λx) if λ > 0 and x ≥ 0.

(2.78)

This allows us to compute its cdf as

F (x|λ) = 1 − exp(−λx)if λ > 0 for x ≥ 0.

(2.79)

Therefore to generate a Exponential random variable we draw z ∼ U [0, 1] and solve x = φ(z) = F −1(z|λ) = −λ−1 log(1 − z). Since z and 1 − z are drawn from U [0, 1] we can simplify this to x = −λ−1 log z.

We could apply the same reasoning to the normal distribution in order to draw Gaussian random variables. Unfortunately, the cumulative distribution function of the Gaussian is not available in closed form and we would need resort to rather nontrivial numerical techniques. It turns out that there exists a much more elegant algorithm which has its roots in Gauss’ proof of the normalization constant of the Normal distribution. This technique is known as the Box-Mu¨ller transform.

Example 2.10 (Box-Mu¨ller Transform) Denote by X, Y independent Gaussian random variables with zero mean and unit variance. We have

1 p(x, y) = √

e−

1 2

x2

1 √

e−

1 2

y2

=

1

e−

1 2

(x2

+y2

)

2π

2π

2π

(2.80)

2.5 Sampling

81

0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00 4 3 2 1 0 1 2 3 4 5

Fig. 2.15. Red: true density of the standard normal distribution (red line) is contrasted with the histogram of 20,000 random variables generated by the Box-Mu¨ller transform.

The key observation is that the joint distribution p(x, y) is radially symmetric, i.e. it only depends on the radius r2 = x2 + y2. Hence we may perform
a variable substitution in polar coordinates via the map φ where

x = r cos θ and y = r sin θ hence (x, y) = φ−1(r, θ).

(2.81)

This allows us to express the density in terms of (r, θ) via

p(r, θ) = p(φ−1(r, θ))

∇r,θφ−1(r, θ)

=

1

e−

1 2

r2

2π

cos θ −r sin θ

sin θ r cos θ

=

r

e−

1 2

r2

.

2π

The fact that p(r, θ) is constant in θ means that we can easily sample θ ∈

[0, 2π] by drawing a random variable, say zθ from U [0, 1] and rescaling it with

2π. To obtain a sampler for r we need to compute the cumulative distribution

function

for

p(r)

=

re−

1 2

r2

:

F (r ) =

r

re−

1 2

r2

dr

=

1

−

e−

1 2

r

2

and

hence

r = F −1(z) =

0

−2 log(1 − z). (2.82)

Observing that z ∼ U [0, 1] implies that 1 − z ∼ U [0, 1] yields the following sampler: draw zθ, zr ∼ U [0, 1] and compute x and y by
x = −2 log zr cos 2πzθ and y = −2 log zr sin 2πzθ.
Note that the Box-Mu¨ller transform yields two independent Gaussian random variables. See Figure 2.15 for an example of the sampler.

82

2 Density Estimation

Example 2.11 (Uniform distribution on the disc) A similar strategy can be employed when sampling from the unit disc. In this case the closedform expression of the distribution is simply given by

1
p(x, y) = π

if x2 + y2 ≤ 1

0 otherwise

(2.83)

Using the variable transform (2.81) yields

r
p(r, θ) = p(φ−1(r, θ)) ∇r,θφ−1(r, θ) = π

if r ≤ 1

0 otherwise

(2.84)

Integrating out θ yields p(r) = 2r for r ∈ [0, 1] with corresponding CDF

F (r)

=

r2

for

r∈ √

[0, 1].

Hence

our

sampler √

draws

zr, zθ

∼

U [0, 1]

and

then

computes x = zr cos 2πzθ and y = zr sin 2πzθ.

2.5.2 Rejection Sampler
All the methods for random variable generation that we looked at so far require intimate knowledge about the pdf of the distribution. We now describe a general purpose method, which can be used to generate samples from an arbitrary distribution. Let us begin with sampling from a set:
Example 2.12 (Rejection Sampler) Denote by X ⊆ X a set and let p be a density on X. Then a sampler for drawing from pX (x) ∝ p(x) for x ∈ X and pX (x) = 0 for x ∈ X, that is, pX (x) = p(x|x ∈ X) is obtained by the procedure:
repeat draw x ∼ p(x)
until x ∈ X return x That is, the algorithm keeps on drawing from p until the random variable is contained in X. The probability that this occurs is clearly p(X). Hence the larger p(X) the higher the eﬃciency of the sampler. See Figure 2.16.

Example 2.13 (Uniform distribution on a disc) The procedure works

trivially as follows: draw x, y ∼ U [0, 1]. Accept if (2x − 1)2 + (2y − 1)2 ≤ 1

and

return

sample

(2x − 1, 2y − 1).

This

sampler

has

eﬃciency

4 π

since

this

is the surface ratio between the unit square and the unit ball.

Note that this time we did not need to carry out any sophisticated measure

2.5 Sampling

83

Fig. 2.16. Rejection sampler. Left: samples drawn from the uniform distribution on [0, 1]2. Middle: the samples drawn from the uniform distribution on the unit disc
are all the points in the grey shaded area. Right: the same procedure allows us to
sample uniformly from arbitrary sets.

2.5

2.0

1.5

1.0

0.5

0.0 0.0

0.2

0.4

0.6

0.8

1.0

3.0

2.5

2.0

1.5

1.0

0.5

0.00.0

0.2

0.4

0.6

0.8

1.0

Fig. 2.17. Accept reject sampling for the Beta(2, 5) distribution. Left: Samples are generated uniformly from the blue rectangle (shaded area). Only those samples which fall under the red curve of the Beta(2, 5) distribution (darkly shaded area) are accepted. Right: The true density of the Beta(2, 5) distribution (red line) is contrasted with the histogram of 10,000 samples drawn by the rejection sampler.

transform. This mathematical convenience came at the expense of a slightly less eﬃcient sampler — about 21% of all samples are rejected.
The same reasoning that we used to obtain a hard accept/reject procedure can be used for a considerably more sophisticated rejection sampler. The basic idea is that if, for a given distribution p we can ﬁnd another distribution q which, after rescaling, becomes an upper envelope on p, we can use q to sample from and reject depending on the ratio between q and p.

Theorem 2.23 (Rejection Sampler) Denote by p and q distributions on X and let c be a constant such that such that cq(x) ≥ p(x) for all x ∈ X.

84

2 Density Estimation

Then the algorithm below draws from p with acceptance probability c−1.

repeat

draw x ∼ q(x) and t ∼ U [0, 1]

until

ct

≤

p(x) q(x)

return x

Proof Denote by Z the event that the sample drawn from q is accepted. Then by Bayes rule the probability Pr(x|Z) can be written as follows

Pr(x|Z) =

Pr(Z|x) Pr(x) Pr(Z )

=

p(x) cq(x)

·

q(x)

c−1

= p(x)

(2.85)

Here we used that Pr(Z) = Pr(Z|x)q(x)dx = c−1p(x)dx = c−1.

Note that the algorithm of Example 2.12 is a special case of such a rejection

sampler

—

we

majorize

pX

by

the

uniform

distribution

rescaled

by

1 p(X

)

.

Example 2.14 (Beta distribution) Recall that the Beta(a, b) distribution, as a member of the Exponential Family with suﬃcient statistics (log x, log(1− x)), is given by

p(x|a, b) = Γ(a + b) xa−1(1 − x)b−1, Γ(a)Γ(b)

(2.86)

For given (a, b) one can verify (problem 2.25) that

a−1

M := argmax p(x|a, b) =

.

x

a+b−2

(2.87)

provided a > 1. Hence, if we use as proposal distribution the uniform distri-

bution U [0, 1] with scaling factor c = p(M |a, b) we may apply Theorem 2.23.

As illustrated in Figure 2.17, to generate a sample from Beta(a, b) we ﬁrst

generate a pair (x, t), uniformly at random from the shaded rectangle. A

sample is retained if ct ≤ p(x|a, b), and rejected otherwise. The acceptance

rate

of

this

sampler

is

1 c

.

Example 2.15 (Normal distribution) We may use the Laplace distribution to generate samples from the Normal distribution. That is, we use

q(x|λ) = λ e−λ|x| 2

(2.88)

as the proposal distribution. For a normal distribution p = N(0, 1) with zero

2.5 Sampling

85

mean and unit variance it turns out that choosing λ = 1 yields the most eﬃcient sampling scheme (see Problem 2.27) with

2e

p(x) ≤

q(x|λ = 1)

π

As illustrated in Figure 2.18, we ﬁrst generate x ∼ q(x|λ = 1) using the

inverse transform method (see Example 2.9 and Problem 2.21) and t ∼

U [0, 1]. If t ≤ 2e/πp(x) we accept x, otherwise we reject it. The eﬃciency

of this scheme is

π 2e

.

2e π

g(x|0,

1)

0.6

p(x)

0.4

0.2

0

−4

−2

0

2

4

Fig. 2.18. Rejection sampling for the Normal distribution (red curve). Samples are
generated uniformly from the Laplace distribution rescaled by 2e/π. Only those samples which fall under the red curve of the standard normal distribution (darkly shaded area) are accepted.

While rejection sampling is fairly eﬃcient in low dimensions its eﬃciency is unsatisfactory in high dimensions. This leads us to an instance of the curse of dimensionality [Bel61]: the pdf of a d-dimensional Gaussian random variable centered at 0 with variance σ2 1 is given by

p(x|σ2)

=

(2π)−

d 2

σ−de−

1 2σ2

x2

Now suppose that we want to draw from p(x|σ2) by sampling from another Gaussian q with slightly larger variance ρ2 > σ2. In this case the ratio
between both distributions is maximized at 0 and it yields

q(0|σ2) ρ d c = p(0|ρ2) = σ

86

2 Density Estimation

If

suppose

ρ σ

=

1.01,

and

d

=

1000,

we

ﬁnd

that

c

≈

20960.

In

other

words,

we need to generate approximately 21,000 samples on the average from q to

draw a single sample from p. We will discuss a more sophisticated sampling

algorithms, namely Gibbs Sampling, in Section ??. It allows us to draw from

rather nontrivial distributions as long as the distributions in small subsets

of random variables are simple enough to be tackled directly.

Problems
Problem 2.1 (Bias Variance Decomposition {1}) Prove that the variance VarX [x] of a random variable can be written as EX [x2] − EX [x]2.

Problem 2.2 (Moment Generating Function {2}) Prove that the characteristic function can be used to generate moments as given in (2.12). Hint: use the Taylor expansion of the exponential and apply the diﬀerential operator before the expectation.

Problem 2.3 (Cumulative Error Function {2})
x
erf(x) = 2/π e−x2dx.
0

(2.89)

Problem 2.4 (Weak Law of Large Numbers {2}) In analogy to the proof
of the central limit theorem prove the weak law of large numbers. Hint: use a ﬁrst order Taylor expansion of eiωt = 1 + iωt + o(t) to compute an approx-
imation of the characteristic function. Next compute the limit m → ∞ for
φX¯m. Finally, apply the inverse Fourier transform to associate the constant distribution at the mean µ with it.

Problem 2.5 (Rates and conﬁdence bounds {3}) Show that the rate of hoeﬀding is tight — get bound from central limit theorem and compare to the hoeﬀding rate.

Problem 2.6 Why can’t we just use each chip on the wafer as a random variable? Give a counterexample. Give bounds if we actually were allowed to do this.

Problem 2.7 (Union Bound) Work on many bounds at the same time. We only have logarithmic penalty.

Problem 2.8 (Randomized Rounding {4}) Solve the linear system of equations Ax = b for integral x.

2.5 Sampling

87

Problem 2.9 (Randomized Projections {3}) Prove that the randomized projections converge.

Problem 2.10 (The Count-Min Sketch {5}) Prove the projection trick

Problem 2.11 (Parzen windows with triangle kernels {1}) Suppose you are given the following data: X = {2, 3, 3, 5, 5}. Plot the estimated density using a kernel density estimator with the following kernel:
0.5 − 0.25 ∗ |u| if |u| ≤ 2 k(u) =
0 otherwise.

Problem 2.12 Gaussian process link with Gaussian prior on natural parameters

Problem 2.13 Optimization for Gaussian regularization

Problem 2.14 Conjugate prior (student-t and wishart).

Problem 2.15 (Multivariate Gaussian {1}) Prove that Σ 0 is a necessary and suﬃcient condition for the normal distribution to be well deﬁned.

Problem 2.16 (Discrete Exponential Distribution {2}) φ(x) = x and uniform measure.

Problem 2.17 Exponential random graphs.

Problem 2.18 (Maximum Entropy Distribution) Show that exponential families arise as the solution of the maximum entropy estimation problem.

Problem 2.19 (Maximum Likelihood Estimates for Normal Distributions) Derive the maximum likelihood estimates for a normal distribution, that is, show that they result in

1 µˆ =
m

m

xi

and

σˆ2

=

1 m

m
(xi − µˆ)2

i=1

i=1

(2.90)

using the exponential families parametrization. Next show that while the

mean

estimate

µˆ

is

unbiased,

the

variance

estimate

has

a

slight

bias

of

O(

1 m

).

To see this, take the expectation with respect to σˆ2.

88

2 Density Estimation

Problem 2.20 (cdf of Logistic random variable {1}) Show that the cdf of the Logistic random variable (??) is given by (??).

Problem 2.21 (Double-exponential (Laplace) distribution {1}) Use the inverse-transform method to generate a sample from the double-exponential (Laplace) distribution (2.88).

Problem 2.22 (Normal random variables in polar coordinates {1})
If X1 and X2 are standard normal random variables and let (R, θ) denote the polar coordinates of the pair (X1, X2). Show that R2 ∼ χ22 and θ ∼ Unif[0, 2π].

Problem 2.23 (Monotonically increasing mappings {1}) A mapping T : R → R is one-to-one if, and only if, T is monotonically increasing, that is, x > y implies that T (x) > T (y).
Problem 2.24 (Monotonically increasing multi-maps {2}) Let T : Rn → Rn be one-to-one. If X ∼ pX (x), then show that the distribution pY (y) of Y = T (X) can be obtained via (??).

Problem 2.25 (Argmax of the Beta(a, b) distribution {1}) Show that the mode of the Beta(a, b) distribution is given by (2.87).

Problem 2.26 (Accept reject sampling for the unit disk {2}) Give at least TWO diﬀerent accept-reject based sampling schemes to generate samples uniformly at random from the unit disk. Compute their eﬃciency.

Problem 2.27 (Optimizing Laplace for Standard Normal {1}) Optimize the ratio p(x)/g(x|µ, σ), with respect to µ and σ, where p(x) is the standard normal distribution (??), and g(x|µ, σ) is the Laplace distribution (2.88).

Problem 2.28 (Normal Random Variable Generation {2}) The aim of this problem is to write code to generate standard normal random variables (??) by using diﬀerent methods. To do this generate U ∼ Unif[0, 1] and apply

(i) the Box-Muller transformation outlined in Section ??. (ii) use the following approximation to the inverse CDF

Φ−1(α)

≈

t

−

1

a0 + a1t + b1t + b2t2

,

(2.91)

2.5 Sampling

89

where t2 = log(α−2) and

a0 = 2.30753, a1 = 0.27061, b1 = 0.99229, b2 = 0.04481

(iii) use the method outlined in example 2.15.

Plot a histogram of the samples you generated to conﬁrm that they are normally distributed. Compare these diﬀerent methods in terms of the time needed to generate 1000 random variables.

Problem 2.29 (Non-standard Normal random variables {2}) Describe a scheme based on the Box-Muller transform to generate d dimensional normal random variables p(x|0, I). How can this be used to generate arbitrary normal random variables p(x|µ, Σ).

Problem 2.30 (Uniform samples from a disk {2}) Show how the ideas

described in Section ?? can be generalized to draw samples uniformly at ran-

dom

from

an

axis

parallel

ellipse:

{(x, y)

:

x12 a2

+

x22 b2

≤

1}.

3 Optimization

Optimization plays an increasingly important role in machine learning. For instance, many machine learning algorithms minimize a regularized risk functional:

min J(f ) := λΩ(f ) + Remp(f ),
f

(3.1)

with the empirical risk

1m Remp(f ) := m l(f (xi), yi).
i=1

(3.2)

Here xi are the training instances and yi are the corresponding labels. l the loss function measures the discrepancy between y and the predictions f (xi). Finding the optimal f involves solving an optimization problem.
This chapter provides a self-contained overview of some basic concepts and tools from optimization, especially geared towards solving machine learning problems. In terms of concepts, we will cover topics related to convexity, duality, and Lagrange multipliers. In terms of tools, we will cover a variety of optimization algorithms including gradient descent, stochastic gradient descent, Newton’s method, and Quasi-Newton methods. We will also look at some specialized algorithms tailored towards solving Linear Programming and Quadratic Programming problems which often arise in machine learning problems.

3.1 Preliminaries Minimizing an arbitrary function is, in general, very diﬃcult, but if the objective function to be minimized is convex then things become considerably simpler. As we will see shortly, the key advantage of dealing with convex functions is that a local optima is also the global optima. Therefore, well developed tools exist to ﬁnd the global minima of a convex function. Consequently, many machine learning algorithms are now formulated in terms of convex optimization problems. We brieﬂy review the concept of convex sets and functions in this section.
91

92

3 Optimization

3.1.1 Convex Sets Deﬁnition 3.1 (Convex Set) A subset C of Rn is said to be convex if (1 − λ)x + λy ∈ C whenever x ∈ C, y ∈ C and 0 < λ < 1.

Intuitively, what this means is that the line joining any two points x and y from the set C lies inside C (see Figure 3.1). It is easy to see (Exercise 3.1) that intersections of convex sets are also convex.

Fig. 3.1. The convex set (left) contains the line joining any two points that belong to the set. A non-convex set (right) does not satisfy this property.
A vector sum i λixi is called a convex combination if λi ≥ 0 and i λi = 1. Convex combinations are helpful in deﬁning a convex hull:
Deﬁnition 3.2 (Convex Hull) The convex hull, conv(X), of a ﬁnite subset X = {x1, . . . , xn} of Rn consists of all convex combinations of x1, . . . , xn.

3.1.2 Convex Functions Let f be a real valued function deﬁned on a set X ⊂ Rn. The set

{(x, µ) : x ∈ X, µ ∈ R, µ ≥ f (x)}

(3.3)

is called the epigraph of f . The function f is deﬁned to be a convex function if its epigraph is a convex set in Rn+1. An equivalent, and more commonly used, deﬁnition (Exercise 3.5) is as follows (see Figure 3.2 for geometric
intuition):

Deﬁnition 3.3 (Convex Function) A function f deﬁned on a set X is called convex if, for any x, x ∈ X and any 0 < λ < 1 such that λx + (1 − λ)x ∈ X, we have

f (λx + (1 − λ)x ) ≤ λf (x) + (1 − λ)f (x ).

(3.4)

